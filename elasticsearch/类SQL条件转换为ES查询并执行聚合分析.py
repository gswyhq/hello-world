import traceback
from typing import Dict, List, Any, Union
import json
import sqlparse
from sqlparse.sql import Comparison, Parenthesis, IdentifierList, Identifier, Token
from sqlparse.tokens import Whitespace, Keyword, Punctuation, String, Number

import requests
from typing import Dict, Any

from typing import Dict, Any, List

import requests
from typing import Dict, Any
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import Optional

false = False
true = True
null = None

def query_elasticsearch(query: Dict[str, Any], es_url: str = "http://localhost:9200", index_name: str = "test_users") -> Dict[str, Any]:
    """
    ä½¿ç”¨ requests å‘ Elasticsearch å‘é€æŸ¥è¯¢è¯·æ±‚ã€‚

    å‚æ•°:
        query (dict): ç¬¦åˆ Elasticsearch DSL çš„æŸ¥è¯¢è¯­å¥ã€‚
        es_url (str): Elasticsearch æœåŠ¡åœ°å€ï¼Œé»˜è®¤ä¸º http://localhost:9200ã€‚
        index_name (str): è¦æŸ¥è¯¢çš„ç´¢å¼•åç§°ã€‚

    è¿”å›:
        dict: Elasticsearch è¿”å›çš„åŸå§‹ JSON å“åº”ã€‚

    å¼‚å¸¸:
        requests.exceptions.RequestException: ç½‘ç»œæˆ–è¯·æ±‚é”™è¯¯ã€‚
        ValueError: è¿”å›é 2xx çŠ¶æ€ç ã€‚
    """
    headers = {
        "Content-Type": "application/json; charset=utf-8"
    }

    url = f"{es_url}/{index_name}/_search"

    try:
        response = requests.post(url, json=query, headers=headers)
        # response.raise_for_status()  # å¦‚æœçŠ¶æ€ç ä¸æ˜¯ 2xxï¼ŒæŠ›å‡ºå¼‚å¸¸
        return response.json()
    except Exception as e:
        print(f"è¯·æ±‚ Elasticsearch å¤±è´¥: {e}")
        raise


# ========================
# å­—æ®µç±»å‹å¸¸é‡ï¼ˆä¾¿äºç»´æŠ¤ï¼‰
# ========================
FIELD_TYPE_KEYWORD = "keyword"
FIELD_TYPE_TEXT = "text"
FIELD_TYPE_DATE = "date"
FIELD_TYPE_INTEGER = "integer"
FIELD_TYPE_LONG = "long"
FIELD_TYPE_FLOAT = "float"
FIELD_TYPE_DOUBLE = "double"

# å¯èšåˆçš„ç±»å‹ï¼ˆæ”¯æŒ terms èšåˆï¼‰
AGGREGATABLE_TYPES = {
    FIELD_TYPE_KEYWORD,
    FIELD_TYPE_INTEGER,
    FIELD_TYPE_LONG,
    FIELD_TYPE_FLOAT,
    FIELD_TYPE_DOUBLE,
    FIELD_TYPE_DATE,
}

DEFAULT_FIELD_TYPES = {'active': 'integer',
     'age': 'integer',
     'code': 'keyword',
     'created_at': 'date',
     'created_date': 'date',
     'department': 'keyword',
     'email': 'keyword',
     'login_time': 'date',
     'name': 'keyword',
     'register_time': 'date',
     'salary': 'float',
     'status': 'keyword',
     'updated_at': 'date',
     'x': 'integer',
     'y': 'integer',
     'z': 'integer',
     'ä¼šå‘˜ç­‰çº§': 'keyword',
     'åˆ›å»ºæ—¶é—´': 'date',
     'å­¦å†': 'keyword',
     'æ€§åˆ«': 'keyword',
     'æœ€åç™»å½•æ—¶é—´': 'date',
     'æ ‡ç­¾': 'keyword',
     'æ´»è·ƒçŠ¶æ€': 'keyword',
     'æ¶ˆè´¹é‡‘é¢': 'float',
     'ç™»å½•æ¬¡æ•°': 'integer',
     'ç±è´¯': 'keyword',
     'èŒä¸š': 'keyword',
     'è®¾å¤‡ç±»å‹': 'keyword',
     'è®¿é—®æ¸ é“': 'keyword',
     'é‚®ç®±': 'keyword'
                       }


# éœ€è¦ .keyword å­å­—æ®µæ‰èƒ½ç²¾ç¡®åŒ¹é…/èšåˆçš„ç±»å‹
REQUIRES_KEYWORD_SUBFIELD = {FIELD_TYPE_TEXT}


class ESQueryGenerator:
    """Elasticsearch æŸ¥è¯¢ç”Ÿæˆå™¨ï¼ˆå…¼å®¹ ES 5.4+ï¼Œæ”¯æŒæ˜¾å¼å­—æ®µç±»å‹ï¼‰"""

    def __init__(self):
        pass

    def _get_field_for_term_query(self, field: str, field_type: str) -> str:
        """
        æ ¹æ®å­—æ®µç±»å‹è¿”å›ç”¨äº term/terms/wildcard æŸ¥è¯¢çš„å®é™…å­—æ®µå
        """
        if field_type in REQUIRES_KEYWORD_SUBFIELD:
            return f"{field}.keyword"
        else:
            return field  # keyword / date / numeric ç›´æ¥ä½¿ç”¨åŸå­—æ®µ

    def _get_field_for_agg(self, field: str, field_type: str) -> str:
        """
        æ ¹æ®å­—æ®µç±»å‹è¿”å›ç”¨äº terms èšåˆçš„å®é™…å­—æ®µå
        æ³¨æ„ï¼štext å­—æ®µå¿…é¡»ç”¨ .keywordï¼›å…¶ä»–ç±»å‹ç›´æ¥ç”¨åŸå­—æ®µ
        """
        if field_type in REQUIRES_KEYWORD_SUBFIELD:
            return f"{field}.keyword"
        elif field_type in AGGREGATABLE_TYPES:
            return field
        else:
            raise ValueError(f"å­—æ®µ '{field}' ç±»å‹ä¸º '{field_type}'ï¼Œä¸æ”¯æŒ terms èšåˆ")

    def _build_condition(
            self,
            condition: Dict[str, Any],
            field_types=DEFAULT_FIELD_TYPES,
    ) -> Dict[str, Any]:
        """
        é€’å½’æ„å»ºæŸ¥è¯¢æ¡ä»¶ï¼Œä¾èµ– field_types åˆ¤æ–­å­—æ®µçœŸå®ç±»å‹
        """
        condition_type = condition["type"]
        field = condition.get("field", "")

        # if field and field not in field_types:
        #     raise ValueError(f"å­—æ®µ '{field}' æœªåœ¨ field_types ä¸­å®šä¹‰ç±»å‹")

        field_type = field_types.get(field, FIELD_TYPE_KEYWORD)  # é»˜è®¤ fallback

        if condition_type == "and":
            return {
                "bool": {
                    "must": [self._build_condition(c, field_types) for c in condition["conditions"]]
                }
            }
        elif condition_type == "or":
            return {
                "bool": {
                    "should": [self._build_condition(c, field_types) for c in condition["conditions"]],
                    "minimum_should_match": 1
                }
            }
        elif condition_type == "not":
            return {
                "bool": {
                    "must_not": [self._build_condition(condition["condition"], field_types)]
                }
            }
        elif condition_type == "equal":
            actual_field = self._get_field_for_term_query(field, field_type)
            return {"term": {actual_field: condition["value"]}}
        elif condition_type == "not_equal":
            actual_field = self._get_field_for_term_query(field, field_type)
            return {"bool": {"must_not": {"term": {actual_field: condition["value"]}}}}
        elif condition_type in ("greater", "less", "greater_equal", "less_equal"):
            # range æŸ¥è¯¢é€‚ç”¨äº date / numericï¼Œä¸é€‚ç”¨äº text/keywordï¼ˆé™¤éæ˜¯ numeric keywordï¼‰
            op_map = {
                "greater": "gt",
                "less": "lt",
                "greater_equal": "gte",
                "less_equal": "lte"
            }
            return {"range": {field: {op_map[condition_type]: condition["value"]}}}
        elif condition_type == "in":
            actual_field = self._get_field_for_term_query(field, field_type)
            return {"terms": {actual_field: condition["values"]}}
        elif condition_type == "not_in":
            actual_field = self._get_field_for_term_query(field, field_type)
            return {"bool": {"must_not": {"terms": {actual_field: condition["values"]}}}}
        elif condition_type == "contains":
            # æ³¨æ„ï¼šwildcard åªé€‚ç”¨äº keyword æˆ– text.keywordï¼Œä¸èƒ½ç”¨äº date/numeric
            if field_type not in (FIELD_TYPE_KEYWORD, FIELD_TYPE_TEXT):
                raise ValueError(f"å­—æ®µ '{field}' ç±»å‹ä¸º '{field_type}'ï¼Œä¸æ”¯æŒ contains æ“ä½œ")
            actual_field = self._get_field_for_term_query(field, field_type)
            return {"wildcard": {actual_field: f"*{condition['value']}*"}}
        elif condition_type == "not_contains":
            if field_type not in (FIELD_TYPE_KEYWORD, FIELD_TYPE_TEXT):
                raise ValueError(f"å­—æ®µ '{field}' ç±»å‹ä¸º '{field_type}'ï¼Œä¸æ”¯æŒ not_contains æ“ä½œ")
            actual_field = self._get_field_for_term_query(field, field_type)
            return {"bool": {"must_not": {"wildcard": {actual_field: f"*{condition['value']}*"}}}}
        elif condition_type.lower() == "like":
            if field_type not in (FIELD_TYPE_KEYWORD, FIELD_TYPE_TEXT):
                raise ValueError(f"å­—æ®µ '{field}' ç±»å‹ä¸º '{field_type}'ï¼Œä¸æ”¯æŒ LIKE æ“ä½œ")
            pattern = condition["pattern"].replace('%', '*').replace('_', '?')
            actual_field = self._get_field_for_term_query(field, field_type)
            return {"wildcard": {actual_field: pattern}}
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¡ä»¶ç±»å‹: {condition_type}")

    def generate_query(
            self,
            filter_conditions: Dict[str, Any],
            analysis_fields: List[str],
            field_types: Dict[str, str] = DEFAULT_FIELD_TYPES,
            aggs_size: int = 100
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆå®Œæ•´çš„ Elasticsearch æŸ¥è¯¢è¯­å¥

        Args:
            filter_conditions: è¿‡æ»¤æ¡ä»¶æ ‘
            analysis_fields: éœ€è¦èšåˆåˆ†æçš„å­—æ®µåˆ—è¡¨
            field_types: å­—æ®µå -> ç±»å‹ çš„æ˜ å°„ï¼Œä¾‹å¦‚ {"å¹´é¾„": "integer", "åŸå¸‚": "keyword"}
            aggs_size: èšåˆè¿”å›çš„æ¡¶æ•°é‡
        """
        query = {
            "size": 0,
            # "track_total_hits": False,
            "query": {"bool": {}},
            "aggs": {}
        }

        # æ„å»ºæŸ¥è¯¢æ¡ä»¶
        built_cond = self._build_condition(filter_conditions, field_types)
        query["query"]["bool"] = built_cond.get("bool", {})

        # æ„å»ºèšåˆ
        for field in analysis_fields:
            # if field not in field_types:
            #     raise ValueError(f"èšåˆå­—æ®µ '{field}' æœªåœ¨ field_types ä¸­å®šä¹‰ç±»å‹")
            field_type = field_types.get(field, FIELD_TYPE_KEYWORD)
            agg_field = self._get_field_for_agg(field, field_type)
            query["aggs"][f"{field}_åˆ†å¸ƒ"] = {
                "terms": {
                    "field": agg_field,
                    "size": aggs_size
                }
            }

        return query

generator = ESQueryGenerator()


# ========================
# SQL WHERE å­å¥è§£æå™¨ï¼ˆå¢å¼ºç‰ˆï¼‰
# ========================

def _clean_value(val: str):
    val = val.strip()
    if (val.startswith("'") and val.endswith("'")) or (val.startswith('"') and val.endswith('"')):
        return val[1:-1]
    try:
        if '.' in val:
            return float(val)
        else:
            return int(val)
    except ValueError:
        return val


def _is_keyword(token, keywords):
    if not isinstance(token, Token):
        return False
    return token.ttype is Keyword and token.value.upper() in (k.upper() for k in keywords)


def _extract_in_values(paren_token: Parenthesis):
    values = []
    inner_tokens = [t for t in paren_token.tokens if not t.is_whitespace and str(t).strip() not in '()']
    if not inner_tokens:
        return values
    for t in inner_tokens:
        if isinstance(t, IdentifierList):
            for sub in t.get_identifiers():
                values.append(_clean_value(str(sub)))
        elif isinstance(t, Identifier) or t.ttype in [String.Single, Number.Integer, Number.Float]:
            values.append(_clean_value(str(t)))
        elif str(t).strip() not in [',']:
            values.append(_clean_value(str(t)))
    return values


def _parse_comparison(comp: Comparison):
    tokens = [t for t in comp.tokens if not t.is_whitespace]
    if len(tokens) < 3:
        raise ValueError(f"æ— æ•ˆ Comparison: {comp}")

    # === ä¿®å¤æ ¸å¿ƒï¼šç›´æ¥é€šè¿‡å­—ç¬¦ä¸²å€¼è¯†åˆ« LIKEï¼Œä¸ä¾èµ– token ç±»å‹ ===
    if len(tokens) == 3:
        token_str = str(tokens[1]).strip().upper()
        if token_str == "LIKE":
            field = "".join(str(t).strip() for t in tokens[0])
            pattern = _clean_value(str(tokens[2]))
            return {
                "type": "like",
                "field": field,
                "pattern": pattern
            }

    left = str(tokens[0]).strip()

    # IN
    if len(tokens) >= 3 and _is_keyword(tokens[1], ["IN"]) and isinstance(tokens[2], Parenthesis):
        return {
            "type": "in",
            "field": left,
            "values": _extract_in_values(tokens[2])
        }

    # NOT IN
    if (len(tokens) >= 4 and
        _is_keyword(tokens[1], ["NOT"]) and
        _is_keyword(tokens[2], ["IN"]) and
        isinstance(tokens[3], Parenthesis)):
        return {
            "type": "not",
            "condition": {
                "type": "in",
                "field": left,
                "values": _extract_in_values(tokens[3])
            }
        }

    # LIKEï¼ˆå¤‡ç”¨è·¯å¾„ï¼Œä»¥é˜²ä¸Šé¢æœªå‘½ä¸­ï¼‰
    if len(tokens) == 3 and _is_keyword(tokens[1], ["LIKE"]):
        return {
            "type": "like",
            "field": left,
            "pattern": _clean_value(str(tokens[2]))
        }

    # äºŒå…ƒæ“ä½œç¬¦
    if len(tokens) == 3:
        op_raw = str(tokens[1]).strip().upper()
        op_map = {
            "=": "equal",
            "!=": "not_equal",
            "<>": "not_equal",
            ">": "greater",
            ">=": "greater_equal",
            "<": "less",
            "<=": "less_equal"
        }
        if op_raw in op_map:
            return {
                "type": op_map[op_raw],
                "field": left,
                "value": _clean_value(str(tokens[2]))
            }

    raise ValueError(f"æ— æ³•è§£æ Comparison: {comp}")


def _parse_atomic_condition_from_tokens(tokens):
    tokens = [t for t in tokens if not t.is_whitespace]
    if len(tokens) == 3:
        a, op, b = tokens
        if isinstance(a, (Identifier, Token)) and _is_keyword(op, ["IN"]) and isinstance(b, Parenthesis):
            return {
                "type": "in",
                "field": str(a).strip(),
                "values": _extract_in_values(b)
            }
        # æ–°å¢ LIKE æ”¯æŒ
        if isinstance(a, (Identifier, Token)) and _is_keyword(op, ["LIKE"]):
            return {
                "type": "like",
                "field": str(a).strip(),
                "pattern": _clean_value(str(b))
            }
    if len(tokens) == 4:
        a, not_kw, in_kw, b = tokens
        if (isinstance(a, (Identifier, Token)) and
            _is_keyword(not_kw, ["NOT"]) and
            _is_keyword(in_kw, ["IN"]) and
            isinstance(b, Parenthesis)):
            return {
                "type": "not",
                "condition": {
                    "type": "in",
                    "field": str(a).strip(),
                    "values": _extract_in_values(b)
                }
            }
    if len(tokens) == 3:
        a, op, b = tokens
        if isinstance(a, (Identifier, Token)) and isinstance(b, (Token, Identifier)):
            op_str = str(op).strip().upper()
            op_map = {
                "=": "equal",
                "!=": "not_equal",
                "<>": "not_equal",
                ">": "greater",
                ">=": "greater_equal",
                "<": "less",
                "<=": "less_equal"
            }
            if op_str in op_map:
                return {
                    "type": op_map[op_str],
                    "field": str(a).strip(),
                    "value": _clean_value(str(b))
                }
    return None


def _parse_expression(tokens):
    tokens = [t for t in tokens if not t.is_whitespace]
    if not tokens:
        return None

    if len(tokens) == 1 and isinstance(tokens[0], Comparison):
        return _parse_comparison(tokens[0])

    if len(tokens) == 1 and isinstance(tokens[0], Parenthesis):
        return _parse_expression(tokens[0].tokens[1:-1])

    if _is_keyword(tokens[0], ["NOT"]):
        rest = tokens[1:]
        if len(rest) == 1 and isinstance(rest[0], Parenthesis):
            inner = _parse_expression(rest[0].tokens[1:-1])
        else:
            inner = _parse_expression(rest)
        return {"type": "not", "condition": inner}

    level = 0
    candidates = []
    for idx, token in enumerate(tokens):
        if token.match(Punctuation, '('):
            level += 1
        elif token.match(Punctuation, ')'):
            level -= 1
        elif level == 0 and _is_keyword(token, ["AND", "OR"]):
            candidates.append((idx, token.value.lower()))

    if candidates:
        op_type = candidates[0][1]
        conditions = []
        start = 0
        for idx, _ in candidates:
            cond = _parse_expression(tokens[start:idx])
            if cond:
                conditions.append(cond)
            start = idx + 1
        cond = _parse_expression(tokens[start:])
        if cond:
            conditions.append(cond)
        if len(conditions) == 1:
            return conditions[0]
        return {"type": op_type, "conditions": conditions}

    atomic = _parse_atomic_condition_from_tokens(tokens)
    if atomic:
        return atomic

    raw = "".join(str(t) for t in tokens)
    try:
        parsed = sqlparse.parse(raw)[0]
        if len(parsed.tokens) == 1 and isinstance(parsed.tokens[0], Comparison):
            return _parse_comparison(parsed.tokens[0])
    except Exception:
        pass

    raise ValueError(f"æ— æ³•è§£æè¡¨è¾¾å¼: {' '.join(str(t) for t in tokens)}")


def sql_to_filter_conditions(where_clause: str):
    where_clause = where_clause.strip()
    if not where_clause:
        raise ValueError("WHERE å­å¥ä¸èƒ½ä¸ºç©º")

    parsed = sqlparse.parse(where_clause)
    if not parsed:
        raise ValueError("SQL è§£æå¤±è´¥")

    inner_cond = _parse_expression(parsed[0].tokens)

    if inner_cond is None:
        raise ValueError("è§£æç»“æœä¸ºç©º")

    if isinstance(inner_cond, dict) and inner_cond.get("type") in ("and", "or"):
        return inner_cond

    return {"type": "and", "conditions": [inner_cond]}

# ========================
# æµ‹è¯•ç”¨ä¾‹
# ========================

def test_cases2():
    """æµ‹è¯• SQL è§£æ + ES æŸ¥è¯¢ç”Ÿæˆç«¯åˆ°ç«¯æµç¨‹"""
    test_cases = [
        " age > 18",
        " (age > 18)",
        " age > 18 AND salary < 5000",
        " name = 'Alice' OR name = 'Bob'",
        " age > 18 AND (department = 'IT' OR department = 'HR')",
        " (age > 18 AND salary > 5000) OR (age <= 18 AND salary > 3000)",
        " salary not in ( 3,4,5,6)",
        " (age > 18 AND name = 'Alice') OR ((salary >= 5000 or salary < -500 ) AND department IN ('IT', 'HR'))",
        " (status = 'A' OR status = 'B') AND (active = 1 OR last_login > '2023-01-01')",
        " (x = 1) OR y = 2 OR z != 3",
        " (x = 1) OR ((y = 2) OR (z != 3))",
        " NOT active = 0 AND age > 20",
        "(age > 18) AND (name = 'Alice' OR salary in ( 5000 , 10000))",
        "age > 18 AND (name = 'Alice' OR (salary >= 5000 AND salary < 10000))",
        " age > 18 ",
        " name LIKE 'A%'",
        " email LIKE '%@qq.com'",
        " code LIKE 'A_B'",  # _ åŒ¹é…å•å­—ç¬¦
        " created_at > '2023-01-01'",
        " login_time <= '2024-12-31T23:59:59'",
        " status = 'active' AND created_date >= '2023-01-01'",
        " (name LIKE 'John%') OR (email LIKE '%gmail.com')",
        " NOT updated_at < '2020-01-01'",
        " register_time IN ('2023-01-01', '2023-01-02')",
        " last_login LIKE '2023%' AND age > 25"
    ]

    for i, sql in enumerate(test_cases, 1):
        print(f"\n--- æµ‹è¯•ç”¨ä¾‹ {i} ---")
        print("SQL:", sql)
        try:
            cond = sql_to_filter_conditions(sql)
            print(json.dumps(cond, indent=2, ensure_ascii=False))
            analysis_fields1 = ["age", "å©šå§»çŠ¶æ€", "å­¦å†"]
            query1 = generator.generate_query(cond, analysis_fields1)
            print("ç”Ÿæˆçš„ ES æŸ¥è¯¢:")
            print(json.dumps(query1, indent=2, ensure_ascii=False))

            rets = query_elasticsearch(query1)
            print(f"esæŸ¥è¯¢ç»“æœï¼š{json.dumps(rets, indent=2, ensure_ascii=False)}")
            print("\n" + "=" * 50 + "\n")
        except Exception as e:
            print("âŒ é”™è¯¯:", str(e))
            print(f"é”™è¯¯è¯¦æƒ…ï¼š{traceback.format_exc()}")


def test_cases():
    """æ‰‹åŠ¨æ„é€ æ¡ä»¶æ ‘çš„æµ‹è¯•ç”¨ä¾‹"""
    # æµ‹è¯•ç”¨ä¾‹1: ç®€å•æ¡ä»¶ç»„åˆ
    filter_cond_list = [{
        "type": "and",
        "conditions": [
            {"type": "equal", "field": "ç±è´¯", "value": "å¹¿ä¸œ"},
            {"type": "equal", "field": "æ€§åˆ«", "value": "å¥³"},
            {"type": "greater", "field": "age", "value": 18},
            {"type": "less", "field": "age", "value": 60}
        ]
    }, {
        "type": "and",
        "conditions": [
            {
                "type": "or",
                "conditions": [
                    {"type": "equal", "field": "ç±è´¯", "value": "å¹¿ä¸œ"},
                    {"type": "equal", "field": "ç±è´¯", "value": "å¹¿è¥¿"}
                ]
            },
            {
                "type": "not",
                "condition": {
                    "type": "in",
                    "field": "å­¦å†",
                    "values": ["åšå£«", "ç¡•å£«"]
                }
            },
            {
                "type": "and",
                "conditions": [
                    {"type": "greater", "field": "æ¶ˆè´¹é‡‘é¢", "value": 1000},
                    {"type": "contains", "field": "é‚®ç®±", "value": "@qq.com"}
                ]
            }
        ]
    },{
        "type": "and",
        "conditions": [
            {"type": "equal", "field": "æ´»è·ƒçŠ¶æ€", "value": "æ˜¯"},
            {
                "type": "not",
                "condition": {
                    "type": "or",
                    "conditions": [
                        {"type": "less", "field": "ç™»å½•æ¬¡æ•°", "value": 5},
                        {"type": "contains", "field": "æ ‡ç­¾", "value": "é»‘åå•"}
                    ]
                }
            }
        ]
    }, {
        "type": "and",
        "conditions": [
            # ç±è´¯ in (å¹¿ä¸œ, å¹¿è¥¿)
            {
                "type": "in",
                "field": "ç±è´¯",
                "values": ["å¹¿ä¸œ", "å¹¿è¥¿"]
            },
            # (æ€§åˆ«=å¥³) or (èŒä¸š!=æ•™å¸ˆ)
            {
                "type": "or",
                "conditions": [
                    {"type": "equal", "field": "æ€§åˆ«", "value": "å¥³"},
                    {"type": "not_equal", "field": "èŒä¸š", "value": "æ•™å¸ˆ"}
                ]
            }
        ]
    },
        {
            "type": "and",
            "conditions": [
                {"type": "like", "field": "é‚®ç®±", "pattern": "%@example.com"},
                {"type": "greater_equal", "field": "åˆ›å»ºæ—¶é—´", "value": "2023-01-01"},
                {"type": "less", "field": "æœ€åç™»å½•æ—¶é—´", "value": "2025-01-01"}
            ]
        }
    ]
    analysis_fields4 = ["è®¾å¤‡ç±»å‹", "è®¿é—®æ¸ é“", "ä¼šå‘˜ç­‰çº§"]
    for cond in filter_cond_list:
        print('*'*80)
        print(f"æ¡ä»¶ï¼š{cond}")
        query4 = generator.generate_query(cond, analysis_fields4)
        print(json.dumps(query4, indent=2, ensure_ascii=False))


# if __name__ == "__main__":
#     test_cases()
#     test_cases2()


# ========================
# FastAPI æ¨¡å‹ä¸è·¯ç”±
# ========================
app = FastAPI(title="SQL to Elasticsearch Query API", version="1.0")

class QueryRequest(BaseModel):
    sql_where: str
    analysis_fields: List[str]
    index_name: Optional[str] = "test_users"
    es_url: Optional[str] = "http://localhost:9200"

@app.post("/query")
async def handle_query(request: QueryRequest):
    try:
        # è§£æ SQL WHERE
        filter_conditions = sql_to_filter_conditions(request.sql_where)

        # ç”Ÿæˆ ES æŸ¥è¯¢
        es_query = generator.generate_query(
            filter_conditions=filter_conditions,
            analysis_fields=request.analysis_fields,
            field_types=DEFAULT_FIELD_TYPES
        )

        # æŸ¥è¯¢ ES
        es_result = query_elasticsearch(es_query, es_url=request.es_url, index_name=request.index_name)

        return {
            "success": True,
            "es_query": es_query,
            "es_result": es_result
        }

    except Exception as e:
        error_msg = str(e)
        traceback.print_exc()
        raise HTTPException(status_code=500, detail={
            "success": False,
            "error": error_msg,
            "traceback": traceback.format_exc()
        })

# ========================
# ç›´æ¥è¿è¡Œå…¥å£ï¼ˆå…³é”®ï¼ï¼‰
# ========================
if __name__ == "__main__":
    import uvicorn
    print("ğŸš€ æ­£åœ¨å¯åŠ¨ FastAPI åº”ç”¨...")
    print("è®¿é—®æ–‡æ¡£: http://127.0.0.1:8000/docs")
    uvicorn.run(app, host="127.0.0.1", port=8000)

# æµ‹è¯•è¯·æ±‚ç¤ºä¾‹ï¼š
'''
curl -XPOST http://localhost:8000/query -H "Content-Type:application/json; charset=utf-8" -d '
{
  "sql_where": "age > 18 AND name LIKE 'A'",
  "analysis_fields": ["èŒä¸š", "status", "èŒä¸š", "ç±è´¯"]
}'



docker run -d \
  --name es543 \
  -p 9200:9200 \
  -p 9300:9300 \
  -e "discovery.type=single-node" \
  -e "xpack.security.enabled=false" \
  -e "bootstrap.memory_lock=true" \
  -e "ES_JAVA_OPTS=-Xms512m -Xmx512m" \
  docker.elastic.co/elasticsearch/elasticsearch:5.4.3


curl -XPUT "http://localhost:9200/test_users" -H "Content-Type: application/json" -d'
{
  "settings": {
    "number_of_shards": 1,
    "number_of_replicas": 0
  },
  "mappings": {
    "user": {
      "properties": {
        "age": { "type": "integer" },
        "salary": { "type": "float" },
        "name": { "type": "text", "fields": { "keyword": { "type": "keyword" } } },
        "email": { "type": "text", "fields": { "keyword": { "type": "keyword" } } },
        "code": { "type": "keyword" },
        "department": { "type": "keyword" },
        "status": { "type": "keyword" },
        "active": { "type": "integer" },
        "x": { "type": "integer" },
        "y": { "type": "integer" },
        "z": { "type": "integer" },
        "ç±è´¯": { "type": "keyword" },
        "æ€§åˆ«": { "type": "keyword" },
        "å­¦å†": { "type": "keyword" },
        "æ¶ˆè´¹é‡‘é¢": { "type": "float" },
        "é‚®ç®±": { "type": "text", "fields": { "keyword": { "type": "keyword" } } },
        "æ´»è·ƒçŠ¶æ€": { "type": "keyword" },
        "ç™»å½•æ¬¡æ•°": { "type": "integer" },
        "æ ‡ç­¾": { "type": "text", "fields": { "keyword": { "type": "keyword" } } },
        "èŒä¸š": { "type": "keyword" },
        "è®¾å¤‡ç±»å‹": { "type": "keyword" },
        "è®¿é—®æ¸ é“": { "type": "keyword" },
        "ä¼šå‘˜ç­‰çº§": { "type": "keyword" },
        "created_at": { "type": "date", "format": "yyyy-MM-dd||yyyy-MM-dd HH:mm:ss||epoch_millis" },
        "login_time": { "type": "date", "format": "yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis" },
        "created_date": { "type": "date", "format": "yyyy-MM-dd" },
        "updated_at": { "type": "date", "format": "yyyy-MM-dd" },
        "register_time": { "type": "date", "format": "yyyy-MM-dd" },
        "æœ€åç™»å½•æ—¶é—´": { "type": "date", "format": "yyyy-MM-dd" },
        "åˆ›å»ºæ—¶é—´": { "type": "date", "format": "yyyy-MM-dd" }
      }
    }
  }
}'

curl -XPOST "http://localhost:9200/test_users/user/_bulk?pretty" -H "Content-Type: application/json" --data-binary '
{"index":{}}
{"age":25,"salary":4500.0,"name":"Alice","email":"alice@qq.com","code":"A1B","department":"IT","status":"active","active":1,"x":1,"y":2,"z":4,"ç±è´¯":"å¹¿ä¸œ","æ€§åˆ«":"å¥³","å­¦å†":"æœ¬ç§‘","æ¶ˆè´¹é‡‘é¢":1200.5,"é‚®ç®±":"alice@example.com","æ´»è·ƒçŠ¶æ€":"æ˜¯","ç™»å½•æ¬¡æ•°":10,"æ ‡ç­¾":"VIP","èŒä¸š":"å·¥ç¨‹å¸ˆ","è®¾å¤‡ç±»å‹":"æ‰‹æœº","è®¿é—®æ¸ é“":"APP","ä¼šå‘˜ç­‰çº§":"é»„é‡‘","created_at":"2023-05-10","login_time":"2024-10-01 10:30:00","created_date":"2023-01-15","updated_at":"2023-06-01","register_time":"2023-01-01","æœ€åç™»å½•æ—¶é—´":"2024-10-01","åˆ›å»ºæ—¶é—´":"2023-01-10"}
{"index":{}}
{"age":30,"salary":8000.0,"name":"Bob","email":"bob@gmail.com","code":"A2B","department":"HR","status":"A","active":1,"x":0,"y":2,"z":3,"ç±è´¯":"å¹¿è¥¿","æ€§åˆ«":"ç”·","å­¦å†":"ç¡•å£«","æ¶ˆè´¹é‡‘é¢":3000.0,"é‚®ç®±":"bob@gmail.com","æ´»è·ƒçŠ¶æ€":"æ˜¯","ç™»å½•æ¬¡æ•°":20,"æ ‡ç­¾":"æ™®é€š","èŒä¸š":"æ•™å¸ˆ","è®¾å¤‡ç±»å‹":"PC","è®¿é—®æ¸ é“":"Web","ä¼šå‘˜ç­‰çº§":"ç™½é“¶","created_at":"2022-12-01","login_time":"2024-11-01 09:00:00","created_date":"2022-12-01","updated_at":"2024-01-01","register_time":"2023-01-02","æœ€åç™»å½•æ—¶é—´":"2024-11-01","åˆ›å»ºæ—¶é—´":"2022-12-01"}
{"index":{}}
{"age":17,"salary":3500.0,"name":"Charlie","email":"charlie@qq.com","code":"C3D","department":"IT","status":"B","active":0,"x":1,"y":1,"z":3,"ç±è´¯":"æ¹–å—","æ€§åˆ«":"ç”·","å­¦å†":"åšå£«","æ¶ˆè´¹é‡‘é¢":800.0,"é‚®ç®±":"charlie@qq.com","æ´»è·ƒçŠ¶æ€":"å¦","ç™»å½•æ¬¡æ•°":3,"æ ‡ç­¾":"é»‘åå•","èŒä¸š":"å­¦ç”Ÿ","è®¾å¤‡ç±»å‹":"å¹³æ¿","è®¿é—®æ¸ é“":"H5","ä¼šå‘˜ç­‰çº§":"æ™®é€š","created_at":"2024-01-01","login_time":"2024-02-01","created_date":"2024-01-01","updated_at":"2020-01-01","register_time":"2024-01-01","æœ€åç™»å½•æ—¶é—´":"2024-02-01","åˆ›å»ºæ—¶é—´":"2024-01-01"}
{"index":{}}
{"age":40,"salary":-1000.0,"name":"David","email":"david@example.com","code":"A_B","department":"Finance","status":"inactive","active":1,"x":1,"y":2,"z":5,"ç±è´¯":"å¹¿ä¸œ","æ€§åˆ«":"ç”·","å­¦å†":"å¤§ä¸“","æ¶ˆè´¹é‡‘é¢":5000.0,"é‚®ç®±":"david@example.com","æ´»è·ƒçŠ¶æ€":"æ˜¯","ç™»å½•æ¬¡æ•°":15,"æ ‡ç­¾":"VIP","èŒä¸š":"ä¼šè®¡","è®¾å¤‡ç±»å‹":"æ‰‹æœº","è®¿é—®æ¸ é“":"APP","ä¼šå‘˜ç­‰çº§":"é’»çŸ³","created_at":"2023-03-01","login_time":"2024-12-31 23:59:59","created_date":"2023-03-01","updated_at":"2023-04-01","register_time":"2023-03-01","æœ€åç™»å½•æ—¶é—´":"2024-12-31","åˆ›å»ºæ—¶é—´":"2023-03-01"}
{"index":{}}
{"age":22,"salary":6000.0,"name":"Eva","email":"eva@163.com","code":"E5F","department":"IT","status":"active","active":1,"x":0,"y":0,"z":0,"ç±è´¯":"å¹¿è¥¿","æ€§åˆ«":"å¥³","å­¦å†":"æœ¬ç§‘","æ¶ˆè´¹é‡‘é¢":2000.0,"é‚®ç®±":"eva@163.com","æ´»è·ƒçŠ¶æ€":"æ˜¯","ç™»å½•æ¬¡æ•°":8,"æ ‡ç­¾":"æ™®é€š","èŒä¸š":"è®¾è®¡å¸ˆ","è®¾å¤‡ç±»å‹":"PC","è®¿é—®æ¸ é“":"Web","ä¼šå‘˜ç­‰çº§":"é»„é‡‘","created_at":"2023-07-01","login_time":"2024-09-01","created_date":"2023-07-01","updated_at":"2023-08-01","register_time":"2023-07-01","æœ€åç™»å½•æ—¶é—´":"2024-09-01","åˆ›å»ºæ—¶é—´":"2023-07-01"}
'

curl -XPOST "http://localhost:9200/test_users/user/_bulk?pretty" -H "Content-Type: application/json" --data-binary '
{"index":{}}
{"age":19,"salary":5000.0,"name":"Frank","email":"frank@example.com","code":"F7G","department":"IT","status":"active","active":1,"x":1,"y":2,"z":3,"ç±è´¯":"å¹¿ä¸œ","æ€§åˆ«":"ç”·","å­¦å†":"å¤§ä¸“","æ¶ˆè´¹é‡‘é¢":2500.0,"é‚®ç®±":"frank@example.com","æ´»è·ƒçŠ¶æ€":"æ˜¯","ç™»å½•æ¬¡æ•°":12,"æ ‡ç­¾":"æ–°ç”¨æˆ·,å­¦ç”Ÿ","èŒä¸š":"å®ä¹ ç”Ÿ","è®¾å¤‡ç±»å‹":"æ‰‹æœº","è®¿é—®æ¸ é“":"APP","ä¼šå‘˜ç­‰çº§":"æ™®é€š","created_at":"2023-01-01","login_time":"2024-05-10 14:20:00","created_date":"2023-01-01","updated_at":"2023-02-01","register_time":"2023-01-01","æœ€åç™»å½•æ—¶é—´":"2024-05-10","åˆ›å»ºæ—¶é—´":"2023-01-01"}
{"index":{}}
{"age":60,"salary":10000.0,"name":"Grace","email":"grace@gmail.com","code":"G8H","department":"HR","status":"B","active":1,"x":0,"y":0,"z":4,"ç±è´¯":"å¹¿è¥¿","æ€§åˆ«":"å¥³","å­¦å†":"åšå£«","æ¶ˆè´¹é‡‘é¢":8000.0,"é‚®ç®±":"grace@gmail.com","æ´»è·ƒçŠ¶æ€":"æ˜¯","ç™»å½•æ¬¡æ•°":30,"æ ‡ç­¾":"é«˜ç®¡","èŒä¸š":"æ€»ç›‘","è®¾å¤‡ç±»å‹":"PC","è®¿é—®æ¸ é“":"Web","ä¼šå‘˜ç­‰çº§":"é’»çŸ³","created_at":"2022-11-15","login_time":"2024-12-30 18:00:00","created_date":"2022-11-15","updated_at":"2024-01-10","register_time":"2022-11-15","æœ€åç™»å½•æ—¶é—´":"2024-12-30","åˆ›å»ºæ—¶é—´":"2022-11-15"}
{"index":{}}
{"age":16,"salary":2000.0,"name":"Henry","email":"henry@qq.com","code":"H9I","department":"Support","status":"inactive","active":0,"x":1,"y":1,"z":1,"ç±è´¯":"æ¹–å—","æ€§åˆ«":"ç”·","å­¦å†":"é«˜ä¸­","æ¶ˆè´¹é‡‘é¢":300.0,"é‚®ç®±":"henry@qq.com","æ´»è·ƒçŠ¶æ€":"å¦","ç™»å½•æ¬¡æ•°":2,"æ ‡ç­¾":"é»‘åå•,æœªæˆå¹´","èŒä¸š":"å­¦ç”Ÿ","è®¾å¤‡ç±»å‹":"å¹³æ¿","è®¿é—®æ¸ é“":"H5","ä¼šå‘˜ç­‰çº§":"æ™®é€š","created_at":"2024-03-01","login_time":"2024-04-01","created_date":"2024-03-01","updated_at":"2020-01-01","register_time":"2024-03-01","æœ€åç™»å½•æ—¶é—´":"2024-04-01","åˆ›å»ºæ—¶é—´":"2024-03-01"}
{"index":{}}
{"age":28,"salary":-500.0,"name":"Ivy","email":"ivy@163.com","code":"I_J","department":"IT","status":"A","active":1,"x":1,"y":2,"z":6,"ç±è´¯":"å¹¿ä¸œ","æ€§åˆ«":"å¥³","å­¦å†":"æœ¬ç§‘","æ¶ˆè´¹é‡‘é¢":4000.0,"é‚®ç®±":"ivy@163.com","æ´»è·ƒçŠ¶æ€":"æ˜¯","ç™»å½•æ¬¡æ•°":25,"æ ‡ç­¾":"VIP","èŒä¸š":"ç¨‹åºå‘˜","è®¾å¤‡ç±»å‹":"Mac","è®¿é—®æ¸ é“":"APP","ä¼šå‘˜ç­‰çº§":"é»„é‡‘","created_at":"2023-08-20","login_time":"2024-11-15 09:45:00","created_date":"2023-08-20","updated_at":"2023-09-01","register_time":"2023-08-20","æœ€åç™»å½•æ—¶é—´":"2024-11-15","åˆ›å»ºæ—¶é—´":"2023-08-20"}
{"index":{}}
{"age":35,"salary":3000.0,"name":"Jack","email":"jack@example.com","code":"J1K","department":"Finance","status":"active","active":1,"x":0,"y":2,"z":3,"ç±è´¯":"å››å·","æ€§åˆ«":"ç”·","å­¦å†":"ç¡•å£«","æ¶ˆè´¹é‡‘é¢":6000.0,"é‚®ç®±":"jack@example.com","æ´»è·ƒçŠ¶æ€":"æ˜¯","ç™»å½•æ¬¡æ•°":18,"æ ‡ç­¾":"æ™®é€š","èŒä¸š":"æ•™å¸ˆ","è®¾å¤‡ç±»å‹":"PC","è®¿é—®æ¸ é“":"Web","ä¼šå‘˜ç­‰çº§":"ç™½é“¶","created_at":"2023-02-14","login_time":"2024-10-20","created_date":"2023-02-14","updated_at":"2023-03-01","register_time":"2023-02-14","æœ€åç™»å½•æ—¶é—´":"2024-10-20","åˆ›å»ºæ—¶é—´":"2023-02-14"}
{"index":{}}
{"age":21,"salary":7000.0,"name":"Kate","email":"kate@gmail.com","code":"K2L","department":"IT","status":"B","active":1,"x":1,"y":0,"z":5,"ç±è´¯":"å¹¿è¥¿","æ€§åˆ«":"å¥³","å­¦å†":"æœ¬ç§‘","æ¶ˆè´¹é‡‘é¢":3500.0,"é‚®ç®±":"kate@gmail.com","æ´»è·ƒçŠ¶æ€":"æ˜¯","ç™»å½•æ¬¡æ•°":7,"æ ‡ç­¾":"å­¦ç”Ÿ","èŒä¸š":"ç ”ç©¶ç”Ÿ","è®¾å¤‡ç±»å‹":"æ‰‹æœº","è®¿é—®æ¸ é“":"APP","ä¼šå‘˜ç­‰çº§":"æ™®é€š","created_at":"2023-09-01","login_time":"2024-08-01","created_date":"2023-09-01","updated_at":"2023-10-01","register_time":"2023-09-01","æœ€åç™»å½•æ—¶é—´":"2024-08-01","åˆ›å»ºæ—¶é—´":"2023-09-01"}
{"index":{}}
{"age":45,"salary":12000.0,"name":"Leo","email":"leo@outlook.com","code":"L3M","department":"Executive","status":"active","active":1,"x":1,"y":1,"z":2,"ç±è´¯":"åŒ—äº¬","æ€§åˆ«":"ç”·","å­¦å†":"åšå£«","æ¶ˆè´¹é‡‘é¢":15000.0,"é‚®ç®±":"leo@outlook.com","æ´»è·ƒçŠ¶æ€":"æ˜¯","ç™»å½•æ¬¡æ•°":40,"æ ‡ç­¾":"é«˜ç®¡,VIP","èŒä¸š":"CTO","è®¾å¤‡ç±»å‹":"PC","è®¿é—®æ¸ é“":"Web","ä¼šå‘˜ç­‰çº§":"é’»çŸ³","created_at":"2022-01-01","login_time":"2024-12-31 23:59:59","created_date":"2022-01-01","updated_at":"2023-12-01","register_time":"2022-01-01","æœ€åç™»å½•æ—¶é—´":"2024-12-31","åˆ›å»ºæ—¶é—´":"2022-01-01"}
'


curl -XPOST "http://localhost:9200/test_users/user/_search?pretty" -H "Content-Type: application/json" -d'
{
  "size": 10,
  "query": {
    "bool": {
      "must": [
        { "range": { "age": { "gt": 18 } } },
        {
          "bool": {
            "should": [
              { "term": { "name.keyword": "Alice" } },
              { "terms": { "salary": [5000, 10000] } }
            ],
            "minimum_should_match": 1
          }
        }
      ]
    }
  }
}'

'''
