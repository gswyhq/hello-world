
load一个bert微调后的模型时
model = load_model('/home/gswyhq/hello-world/bert/model.h5')
报错：
ImportError: cannot import name 'TokenEmbedding'
问题分析
当我们自定义loss或者layer时，如果直接进行训练后模型文件加载，将会出现Value error 或layer 不存在等问题。
解决方案：
在Keras中，如果存在自定义layer或者loss，需要在load_model()中以字典形式指定layer或loss。如：

from keras_bert.layers.embedding import TokenEmbedding
from keras_pos_embd.pos_embd import PositionEmbedding
from keras_layer_normalization.layer_normalization import LayerNormalization
from keras_multi_head.multi_head_attention import MultiHeadAttention
from keras_position_wise_feed_forward.feed_forward import FeedForward
from keras_bert.bert import gelu_tensorflow

model = load_model('/home/gswyhq/hello-world/bert/model.h5', custom_objects={'TokenEmbedding':TokenEmbedding,
                                                                             'PositionEmbedding': PositionEmbedding,
                                                                             'LayerNormalization': LayerNormalization,
                                                                             'MultiHeadAttention': MultiHeadAttention,
                                                                             'FeedForward': FeedForward,
                                                                             'gelu_tensorflow': gelu_tensorflow})

ValueError: Unknown layer: Attention
#这里需要注意两点，
#1、是'Attention',这里对应了model.summary()打印的attention_1层的type
#2、AttentionLayer是class Attention实例化出来的对象

# 梯度消失与梯度爆炸、Loss为Nan的原因
1.梯度爆炸。梯度变得非常大，使得学习过程难以继续。
2.不当的损失函数。
3.不当的输入。if np.isnan(data).any(): continue
4.池化层中步长比卷积核的尺寸大。

在导入_obtain_input_shape时
from keras.applications.imagenet_utils import _obtain_input_shape

出现错误如下：
ImportError: cannot import name '_obtain_input_shape'
原因是在keras 2.2.2中，keras.applications.imagenet_utils模块不再有_obtain_input_shape方法。解决方法：
将导入语句修改如下
from keras_applications.imagenet_utils import _obtain_input_shape

keras可视化时报错：
  File "/usr/local/lib/python3.5/dist-packages/pydot.py", line 136, in call_graphviz
    **kwargs
  File "/usr/lib/python3.5/subprocess.py", line 947, in __init__
    restore_signals, start_new_session)
  File "/usr/lib/python3.5/subprocess.py", line 1551, in _execute_child
    raise child_exception_type(errno_num, err_msg)
FileNotFoundError: [Errno 2] No such file or directory: 'dot'

解决方法：
# apt-get install graphviz
# dot -V
dot - graphviz version 2.38.0 (20140413.2041)

利用model.fit_generator训练，并且可视化训练过程时，报错：
  File "/usr/local/lib/python3.5/dist-packages/keras/callbacks.py", line 912, in on_epoch_end
    raise ValueError("If printing histograms, validation_data must be "
ValueError: If printing histograms, validation_data must be provided, and cannot be a generator.

解决方法：
直接把histogram_freq设置为0即可，histogram_freq，按照何等频率（epoch）来计算直方图，0为不计算
tbCallBack = TensorBoard(log_dir=TENSORBOARD_PATH, histogram_freq=0,  
                  write_graph=True, write_images=True)

Keras Reshape层报错：
TypeError: Layer reshape_1 does not support masking, but was passed an input_mask: Tensor("WordEmbedding/NotEqual:0", shape=(?, 100), dtype=bool)
解决方案，添加一层：
x = Lambda(lambda x: x, output_shape=lambda s:s)(x)

在使用GPU训练时报错：
tensorflow.python.framework.errors_impl.InternalError: Blas GEMM launch failed : a.shape=(6368, 125), b.shape=(125, 125), m=6368, n=125, k=125

原因及解决方案：
这是调用GPU时，显存分配遇到了问题，所以发生错误。
比较保险的方式是在模型训练之前为tensorflow或者keras分配显存空间，tensorflow就用如下语句创建session：
gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)  
sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))  
而keras就在引入keras时进行参数设置：
import tensorflow as tf
from keras.backend.tensorflow_backend import set_session
config = tf.ConfigProto()
config.gpu_options.allocator_type = 'BFC' #A "Best-fit with coalescing" algorithm, simplified from a version of dlmalloc.
config.gpu_options.per_process_gpu_memory_fraction = 0.3
config.gpu_options.allow_growth = True
set_session(tf.Session(config=config)) 
如果使用ipython notebook，做完上述设置后可能出现GPU sync failed，重启一下就应该没问题了。

构建模型时报错：
TypeError: Layer reshape_1 does not support masking, but was passed an input_mask: Tensor("concatenate_1/All:0", shape=(?, ?), dtype=bool)
解决方案：
添加一个lambda层`dense = Lambda(lambda x: x, output_shape=lambda s: s)(dense)`使网络结构由
_________________________________________________________________
bidirectional_1 (Bidirection (None, None, 200)         240800
_________________________________________________________________
attention_1 (Attention)      (None, None, 128)         76800
_________________________________________________________________

变为：
_________________________________________________________________
bidirectional_1 (Bidirection (None, None, 200)         240800
_________________________________________________________________
lambda_1 (Lambda)            (None, None, 200)         0
_________________________________________________________________
attention_1 (Attention)      (None, None, 128)         76800
_________________________________________________________________


其他类似操作有：
添加一个lambda层`output = Lambda(lambda x: x[:, 0])(output)`使网络结构由
__________________________________________________________________________________________________
attention_1 (Attention)         (None, None, 512)    1572864     Encoder-1-FeedForward-Norm[23][0]
__________________________________________________________________________________________________
dense_7 (Dense)                 (None, 73)           37449       lambda_1[0][0]
__________________________________________________________________________________________________
变为：
__________________________________________________________________________________________________
attention_1 (Attention)         (None, None, 512)    1572864     Encoder-1-FeedForward-Norm[23][0]
__________________________________________________________________________________________________
lambda_1 (Lambda)               (None, 512)          0           attention_1[0][0]
__________________________________________________________________________________________________
dense_7 (Dense)                 (None, 73)           37449       lambda_1[0][0]
__________________________________________________________________________________________________

当一个包含keras模型的python脚本执行时，如果没有任何多线程/多处理功能，它将可以正常工作。但是，一旦使用了多线程并且keras模型需要在新线程中执行，就会引起问题。
tornado启动服务，单线程启动加载模型没有问题；但改成多线程出现下面的问题：
ValueError: Tensor Tensor("ner_out/cond/Merge:0", shape=(?, ?, 125), dtype=float32) is not an element of this graph.
解决方案：
初始化加载模型后，先执行一次model._make_predict_function()操作，之后的调用就不会出问题了
model = load_model(filepath=model_path)
model._make_predict_function()

或者多线程报错：
ValueError: Tensor("training/Adam/Const:0", shape=(), dtype=float32) must be from the same graph as Tensor("sub:0", shape=(), dtype=float32).
解决方案：
加载模型时候：
    self.model = load_model(model_path)
    self.model._make_predict_function()
    self.graph = tf.get_default_graph()

预测的时候：
    with self.graph.as_default():
        labels = self.model.predict(data)

训练的时候：
    with self.graph.as_default():
        hist = self.model.fit(x, y)

问题： keras 并发load_model报错,即同时加载两个不同的模型报错, 两个不同的模型放在同一个py文件中在不同函数中加载运行；
单独加载模型预测没有问题，两个函数都执行时，就会报错，即使在第一次加载预测完成后手动清理内存也没有用 gc.collect()；
ValueError: Weights must be of equal size to apply a reshape operation. Layer conv1d_4's weights have shape (3, 100, 128) and size 38400. The weights for loading have shape (5, 128, 128) and size 81920. 
解决方案1：
在第一个模型加载预测完成后，第二个模型加载模型文件前（不仅仅是model.load_weights前面, 而是构建模型之前运行，若仅仅是model.load_weights前则前面构建模型的变量等也会被清空，导致model.load_weights失败）添加下面的语句：
keras.backend.clear_session()
错误示例：
    model = Model(sequence_input, preds)
    keras.backend.clear_session()
    model.load_weights('./models/model_conv_2.h5', by_name=True, reshape=True)
正确示例：
    keras.backend.clear_session()
    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')
    ...
    preds = Dense(2, activation='softmax')(l_dense)
    model = Model(sequence_input, preds)
    model.load_weights('./models/model_conv_2.h5', by_name=True, reshape=True)
解决方案2：
    graph = tf.Graph()  # Tensorflow graph
    with graph.as_default():
        session = tf.Session()  # Tensorflow session
        with session.as_default():
            sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')
            ...
            preds = Dense(2, activation='softmax')(l_dense)

            model = Model(sequence_input, preds)
            model.load_weights('./models/model_conv_2.h5', by_name=True, reshape=True)

            sequences = tokenizer.texts_to_sequences([text])

            X = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)
            ret = model.predict(X)
            print(ret)

keras==2.2.4版本，使用tensorflow作后端，Attention GRU network出现以下错误：
    y_permute_dim = [y_permute_dim.pop(-2)] + y_permute_dim
IndexError: pop index out of range
解决方案， 更改AttLayer层的call函数：
def call(self, x, mask=None):
    eij = K.tanh(K.dot(x, self.W))

    ai = K.exp(eij)
    weights = ai/K.sum(ai, axis=1).dimshuffle(0,'x')

    weighted_input = x*weights.dimshuffle(0,1,'x')
    return weighted_input.sum(axis=1)
改为：
def call(self, x, mask=None):
    eij = K.tanh(K.squeeze(K.dot(x, K.expand_dims(self.W)), axis=-1))

    ai = K.exp(eij)
    weights = ai/K.expand_dims(K.sum(ai, axis=1),1)

    weighted_input = x*K.expand_dims(weights,2)
    return K.sum(weighted_input, axis=1)

在使用Attention GRU network时，网络结构不对，报错：
ValueError: Error when checking target: expected dense_1 to have shape (1000, 2) but got array with shape (2)
解决方案， 更改AttLayer层的get_output_shape_for函数,重命名为compute_output_shape：
def compute_output_shape(self, input_shape):
    return (input_shape[0], input_shape[-1])

有时候使用keras报错：
AttributeError: module 'keras.backend.tensorflow_backend' has no attribute '_is_tf_1'
解决方法, 卸载之前的重新安装即可：
~$ pip3 uninstall keras
Uninstalling Keras-2.3.1:
  /home/gswyhq/.local/lib/python3.6/site-packages/Keras-2.3.1.dist-info/INSTALLER
  /home/gswyhq/.local/lib/python3.6/site-packages/Keras-2.3.1.dist-info/LICENSE
  /home/gswyhq/.local/lib/python3.6/site-packages/Keras-2.3.1.dist-info/METADATA
...
$ sudo pip3 uninstall keras
Uninstalling Keras-2.2.4:
  /usr/local/lib/python3.6/dist-packages/Keras-2.2.4.dist-info/INSTALLER
  /usr/local/lib/python3.6/dist-packages/Keras-2.2.4.dist-info/METADATA
  /usr/local/lib/python3.6/dist-packages/Keras-2.2.4.dist-info/RECORD
  /usr/local/lib/python3.6/dist-packages/Keras-2.2.4.dist-info/WHEEL
...
$ pip3 install Keras==2.2.4 -i http://pypi.douban.com/simple --trusted-host=pypi.douban.com

有时候报错：
TypeError: Tensors in list passed to 'values' of 'ConcatV2' Op have types [bool, float32] that don't all match.
主要是因为keras版本不对(2.3.1版本有问题，切换回2.2.4版本就可以了):
import tensorflow as tf
print(tf.VERSION)
print(tf.keras.__version__)
~$ pip3 install Keras==2.2.4 -i http://pypi.douban.com/simple --trusted-host=pypi.douban.com


错误，使用Merge 时会报下面的错误：
    attention_mul = merge([inputs, attention_probs], output_shape=32, name='attention_mul', mode='mul')
TypeError: 'module' object is not callable

问题分析：
Merge 不能与 sequential model一起使用，在sequential model中，只能有一个输入一个输出；
上面问题的解决，可以修改成如下
from keras.layers import Input, Dense, merge, concatenate, multiply
attention_mul = multiply([inputs, attention_probs], name='attention_mul')
这里是使用multiply，其他有时候可能是add, concatenate等。

另外，Keras2.2.4中Merge功能已经删除；
若实现的是concat的功能，则可使用以下方法替代merge([tensor1, tensor2], mode='concat', concat_axis=3)：
from keras.layers import concatenate
merge = concatenate([tensor1, tensor2], axis=3)

有时候会报错：
ValueError: Graph disconnected: cannot obtain value for tensor Tensor
问题分析：
一般是Input和下面的变量重名了，导致model里面的input变成了第二次出现的Input变量，而不是最开始模型中作为输入的Input变量
解决方案：
重新给第二个变量赋值下名字即可；

在已有的模型上继续训练时，感觉模型还没开始训练就结束了，训练日志什么的，也没有：
        hist = model.fit(
            *train_data,
            epochs=6,
            initial_epoch=8,
            validation_data=validation_data,
            callbacks=[early_stopping, model_checkpoint, tb]
        )
        print(hist.history.items())
问题分析：
主要是因为设置不对；
epochs：整数，训练终止时的epoch值，训练将在达到该epoch值时停止，当没有设置initial_epoch时，它就是训练的总轮数，否则训练的总轮数为epochs - inital_epoch
而此处的 epochs 设置的值比 inital_epoch 还小，故而出现还没训练，流程就已经结束了。
解决方案，重新设置 epochs、inital_epoch：
        hist = model.fit(
            *train_data,
            epochs=14,
            initial_epoch=8,
            validation_data=validation_data,
            callbacks=[early_stopping, model_checkpoint, tb]
        )
        print(hist.history.items())

问题：
ImportError: No module named 'keras_layer_normalization'
解决方案：
~# pip3 install keras-transformer==0.29.0 -i http://pypi.douban.com/simple --trusted-host=pypi.douban.com

训练一开始的loss,acc都是nan:
loss: nan - acc: nan
问题出现原因：
数据异常所致，比如传入的数据为空等。

模型训练时，有时报下面的错误：
InvalidArgumentError: Incompatible shapes: [1920] vs. [32,60]
     [[{{node metrics_3/acc/Equal}}]]
可能是metrics函数`accuracy`不会改变y_pred的形状。您可以使用自定义指标功能，例如：
def custom_sparse_categorical_accuracy(y_true, y_pred):
    flatten_y_true = K.cast( K.reshape(y_true,(-1,1) ), K.floatx())
    flatten_y_pred = K.cast(K.reshape(y_pred, (-1, y_pred.shape[-1])), K.floatx())
    y_pred_labels = K.cast(K.argmax(flatten_y_pred, axis=-1), K.floatx())
    return K.cast(K.equal(flatten_y_true,y_pred_labels), K.floatx())
然后符合以下指标：
model.compile（...，metrics = [custom_sparse_categorical_accuracy]）

问题：keras 通过 ModelCheckpoint 保存最佳的模型，出错：
RuntimeWarning: Can save best model only with val_loss available, skipping.
  'skipping.' % (self.monitor), RuntimeWarning)
主要是训练时候，没有val训练数据，且又没有设置：validation_split
解决方案：设置validation_data数据，或者设置 validation_split, 或者设置 save_best_only=False

# 预测的时候，报错：
tensorflow.python.framework.errors_impl.InvalidArgumentError: Expected multiples[0] >= 0, but got -4
	 [[{{node Memory-0/Tile}}]]
可能是因为机器内存不够，这个时候，将batch_size改小即可解决。

问题：
ImportError: cannot import name 'transpose_shape'
问题原因及解决方案：
因为 tensorflow 的版本 与 keras 版本有冲突，故而卸载旧版本，安装新版本即可
pip uninstall -y tensorflow keras tf-nightly keras-nightly
pip install tensorflow
注：pip install tensorflow 会安装：keras tensorflow tensorflow-estimator

