#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# èµ„æ–™æ¥æºï¼šhttps://github.com/huggingface-cn/translation/blob/main/philschmid/2023-03-23-fine-tune-flan-t5-peft.ipynb

import os
USERNAME = os.getenv("USERNAME")

from datasets import load_dataset

# https://huggingface.co/datasets/samsum
dataset_path = rf"D:\Users\{USERNAME}\data\samsum\data"
dataset = load_dataset(dataset_path)

print(f"Train dataset size: {len(dataset['train'])}")
print(f"Test dataset size: {len(dataset['test'])}")

# Train dataset size: 14732
# Test dataset size: 819

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

# æ¨¡å‹æ¥æºï¼šhttps://huggingface.co/google/flan-t5-small
model_id = rf"D:\Users\{USERNAME}\data\flan-t5-small"

# Load tokenizer of FLAN-t5-XL
tokenizer = AutoTokenizer.from_pretrained(model_id)

# åœ¨å¼€å§‹è®­ç»ƒä¹‹å‰ï¼Œæˆ‘ä»¬è¿˜éœ€è¦å¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†ã€‚ç”Ÿæˆå¼æ–‡æœ¬æ‘˜è¦å±äºæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ã€‚æˆ‘ä»¬å°†æ–‡æœ¬è¾“å…¥ç»™æ¨¡å‹ï¼Œæ¨¡å‹ä¼šè¾“å‡ºæ‘˜è¦ã€‚æˆ‘ä»¬éœ€è¦äº†è§£è¾“å…¥å’Œè¾“å‡ºæ–‡æœ¬çš„é•¿åº¦ä¿¡æ¯ï¼Œä»¥åˆ©äºæˆ‘ä»¬é«˜æ•ˆåœ°æ‰¹é‡å¤„ç†è¿™äº›æ•°æ®ã€‚

from datasets import concatenate_datasets
import numpy as np
# The maximum total input sequence length after tokenization.
# Sequences longer than this will be truncated, sequences shorter will be padded.
tokenized_inputs = concatenate_datasets([dataset["train"], dataset["test"]]).map(lambda x: tokenizer(x["dialogue"], truncation=True), batched=True, remove_columns=["dialogue", "summary"])
input_lenghts = [len(x) for x in tokenized_inputs["input_ids"]]
# take 85 percentile of max length for better utilization
max_source_length = int(np.percentile(input_lenghts, 85))
print(f"Max source length: {max_source_length}")
# Max source length: 255

# The maximum total sequence length for target text after tokenization.
# Sequences longer than this will be truncated, sequences shorter will be padded."
tokenized_targets = concatenate_datasets([dataset["train"], dataset["test"]]).map(lambda x: tokenizer(x["summary"], truncation=True), batched=True, remove_columns=["dialogue", "summary"])
target_lenghts = [len(x) for x in tokenized_targets["input_ids"]]
# take 90 percentile of max length for better utilization
max_target_length = int(np.percentile(target_lenghts, 90))
print(f"Max target length: {max_target_length}")
# Max target length: 50

# æˆ‘ä»¬å°†åœ¨è®­ç»ƒå‰ç»Ÿä¸€å¯¹æ•°æ®é›†è¿›è¡Œé¢„å¤„ç†å¹¶å°†é¢„å¤„ç†åçš„æ•°æ®é›†ä¿å­˜åˆ°ç£ç›˜ã€‚ä½ å¯ä»¥åœ¨æœ¬åœ°æœºå™¨æˆ– CPU ä¸Šè¿è¡Œæ­¤æ­¥éª¤å¹¶å°†å…¶ä¸Šä¼ åˆ° Hugging Face Hubã€‚

def preprocess_function(sample,padding="max_length"):
    # add prefix to the input for t5
    inputs = ["summarize: " + item for item in sample["dialogue"]]

    # tokenize inputs
    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)

    # Tokenize targets with the `text_target` keyword argument
    labels = tokenizer(text_target=sample["summary"], max_length=max_target_length, padding=padding, truncation=True)

    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore
    # padding in the loss.
    if padding == "max_length":
        labels["input_ids"] = [
            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels["input_ids"]
        ]

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=["dialogue", "summary", "id"])
print(f"Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}")
# Keys of tokenized dataset: ['input_ids', 'attention_mask', 'labels']

# save datasets to disk for later easy loading
tokenized_dataset["train"].save_to_disk(f"{dataset_path}/train")
tokenized_dataset["test"].save_to_disk(f"{dataset_path}/eval")

# ä½¿ç”¨ LoRA å’Œ bnb int-8 å¾®è°ƒ T5
# é™¤äº† LoRA æŠ€æœ¯ï¼Œæˆ‘ä»¬è¿˜ä½¿ç”¨ bitsanbytes LLM.int8() æŠŠå†»ç»“çš„ LLM é‡åŒ–ä¸º int8ã€‚è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿå°† FLAN-T5 XXL æ‰€éœ€çš„å†…å­˜é™ä½åˆ°çº¦å››åˆ†ä¹‹ä¸€ã€‚
#
# è®­ç»ƒçš„ç¬¬ä¸€æ­¥æ˜¯åŠ è½½æ¨¡å‹ã€‚
import torch
from transformers import AutoModelForSeq2SeqLM
from transformers.utils.quantization_config import BitsAndBytesConfig
# load model from the hub
model = AutoModelForSeq2SeqLM.from_pretrained(model_id, load_in_8bit=True, torch_dtype=torch.float16, device_map="auto", quantization_config = BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload=True))

# æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ peft ä¸º LoRA int-8 è®­ç»ƒä½œå‡†å¤‡äº†ã€‚

from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training, TaskType

# Define LoRA Config
lora_config = LoraConfig(
 r=16,
 lora_alpha=32,
 target_modules=["q", "v"],
 lora_dropout=0.05,
 bias="none",
 task_type=TaskType.SEQ_2_SEQ_LM
)
# prepare int-8 model for training
model = prepare_model_for_int8_training(model)

# add LoRA adaptor
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()
# trainable params: 688128 || all params: 77649280 || trainable%: 0.8862001038515747

# å¦‚ä½ æ‰€è§ï¼Œè¿™é‡Œæˆ‘ä»¬åªè®­ç»ƒäº†æ¨¡å‹å‚æ•°çš„ 0.88%ï¼è¿™ä¸ªå·¨å¤§çš„å†…å­˜å¢ç›Šè®©æˆ‘ä»¬å®‰å¿ƒåœ°å¾®è°ƒæ¨¡å‹ï¼Œè€Œä¸ç”¨æ‹…å¿ƒå†…å­˜é—®é¢˜ã€‚
# æ¥ä¸‹æ¥éœ€è¦åˆ›å»ºä¸€ä¸ª DataCollatorï¼Œè´Ÿè´£å¯¹è¾“å…¥å’Œæ ‡ç­¾è¿›è¡Œå¡«å……ï¼Œæˆ‘ä»¬ä½¿ç”¨ ğŸ¤— Transformers åº“ä¸­çš„DataCollatorForSeq2Seq æ¥å®Œæˆè¿™ä¸€ç¯èŠ‚ã€‚
from transformers import DataCollatorForSeq2Seq

# we want to ignore tokenizer pad token in the loss
label_pad_token_id = -100
# Data collator
data_collator = DataCollatorForSeq2Seq(
    tokenizer,
    model=model,
    label_pad_token_id=label_pad_token_id,
    pad_to_multiple_of=8
)

# æœ€åä¸€æ­¥æ˜¯å®šä¹‰è®­ç»ƒè¶…å‚ (TrainingArguments)ã€‚

from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments
output_dir = rf"D:\Users\{USERNAME}\data\lora-flan-t5-small"

# Define training args
training_args = Seq2SeqTrainingArguments(
    output_dir=output_dir,
		auto_find_batch_size=True,
    learning_rate=1e-3, # higher learning rate
    num_train_epochs=5,
    logging_dir=f"{output_dir}/logs",
    logging_strategy="steps",
    logging_steps=500,
    save_strategy="no",
    report_to="tensorboard",
)

# ä»ç£ç›˜ä¸Šè¯»å–å·²å¤„ç†å¥½çš„æ•°æ®
tokenized_dataset = {"train": datasets.load_from_disk((f"{dataset_path}/train")),
                     "test": datasets.load_from_disk((f"{dataset_path}/eval"))}

# Create Trainer instance
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=tokenized_dataset["train"],
)
model.config.use_cache = False  # silence the warnings. Please re-enable for inference!


# è¿è¡Œä¸‹é¢çš„ä»£ç ï¼Œå¼€å§‹è®­ç»ƒæ¨¡å‹ã€‚è¯·æ³¨æ„ï¼Œå¯¹äº T5ï¼Œå‡ºäºæ”¶æ•›ç¨³å®šæ€§è€ƒé‡ï¼ŒæŸäº›å±‚æˆ‘ä»¬ä»ä¿æŒ float32 ç²¾åº¦ã€‚

# train model
trainer.train()
# windowsä¸‹CPUè®­ç»ƒæŠ¥é”™ï¼š
# "bernoulli_scalar_cpu_" not implemented for 'Half'
# åŸå› ï¼šCPUç¯å¢ƒä¸æ”¯æŒtorch.float16

# æˆ‘ä»¬å¯ä»¥å°†æ¨¡å‹ä¿å­˜ä¸‹æ¥ä»¥ç”¨äºåé¢çš„æ¨ç†å’Œè¯„ä¼°ã€‚æˆ‘ä»¬æš‚æ—¶å°†å…¶ä¿å­˜åˆ°ç£ç›˜ï¼Œä½†ä½ ä¹Ÿå¯ä»¥ä½¿ç”¨ model.push_to_hub æ–¹æ³•å°†å…¶ä¸Šä¼ åˆ° Hugging Face Hubã€‚

# Save our LoRA model & tokenizer results
peft_model_id="results"
trainer.model.save_pretrained(peft_model_id)
tokenizer.save_pretrained(peft_model_id)
# if you want to save the base model to call
# trainer.model.base_model.save_pretrained(peft_model_id)
# æœ€åç”Ÿæˆçš„ LoRA checkpoint æ–‡ä»¶å¾ˆå°ï¼Œä»…éœ€ 84MB å°±åŒ…å«äº†ä» samsum æ•°æ®é›†ä¸Šå­¦åˆ°çš„æ‰€æœ‰çŸ¥è¯†ã€‚

# ä½¿ç”¨ LoRA FLAN-T5 è¿›è¡Œè¯„ä¼°å’Œæ¨ç†
# æˆ‘ä»¬å°†ä½¿ç”¨ evaluate åº“æ¥è¯„ä¼° rogue åˆ†æ•°ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ PEFT å’Œ transformers æ¥å¯¹ FLAN-T5 XXL æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚å¯¹ FLAN-T5 XXL æ¨¡å‹ï¼Œæˆ‘ä»¬è‡³å°‘éœ€è¦ 18GB çš„ GPU æ˜¾å­˜ã€‚

import torch
from peft import PeftModel, PeftConfig
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

# Load peft config for pre-trained checkpoint etc. 
peft_model_id = "results"
config = PeftConfig.from_pretrained(peft_model_id)

# load base LLM model and tokenizer
model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path,  load_in_8bit=True,  device_map={"":0})
tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)

# Load the Lora model
model = PeftModel.from_pretrained(model, peft_model_id, device_map={"":0})
model.eval()

print("Peft model loaded")
# æˆ‘ä»¬ç”¨æµ‹è¯•æ•°æ®é›†ä¸­çš„ä¸€ä¸ªéšæœºæ ·æœ¬æ¥è¯•è¯•æ‘˜è¦æ•ˆæœã€‚

from datasets import load_dataset 
from random import randrange


# Load dataset from the hub and get a sample
dataset = load_dataset(dataset_path)
sample = dataset['test'][randrange(len(dataset["test"]))]

input_ids = tokenizer(sample["dialogue"], return_tensors="pt", truncation=True).input_ids.cuda()
# with torch.inference_mode():
outputs = model.generate(input_ids=input_ids, max_new_tokens=10, do_sample=True, top_p=0.9)
print(f"input sentence: {sample['dialogue']}\n{'---'* 20}")

print(f"summary:\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0]}")
# ä¸é”™ï¼æˆ‘ä»¬çš„æ¨¡å‹æœ‰æ•ˆï¼ç°åœ¨ï¼Œè®©æˆ‘ä»¬ä»”ç»†çœ‹çœ‹ï¼Œå¹¶ä½¿ç”¨ test é›†ä¸­çš„å…¨éƒ¨æ•°æ®å¯¹å…¶è¿›è¡Œè¯„ä¼°ã€‚
# ä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦å®ç°ä¸€äº›å·¥å…·å‡½æ•°æ¥å¸®åŠ©ç”Ÿæˆæ‘˜è¦å¹¶å°†å…¶ä¸ç›¸åº”çš„å‚è€ƒæ‘˜è¦ç»„åˆåˆ°ä¸€èµ·ã€‚è¯„ä¼°æ‘˜è¦ä»»åŠ¡æœ€å¸¸ç”¨çš„æŒ‡æ ‡æ˜¯ rogue_scoreï¼Œå®ƒçš„å…¨ç§°æ˜¯ Recall-Oriented Understudy for Gisting Evaluationã€‚
# ä¸å¸¸ç”¨çš„å‡†ç¡®ç‡æŒ‡æ ‡ä¸åŒï¼Œå®ƒå°†ç”Ÿæˆçš„æ‘˜è¦ä¸ä¸€ç»„å‚è€ƒæ‘˜è¦è¿›è¡Œæ¯”è¾ƒã€‚

import evaluate
import numpy as np
from datasets import load_from_disk
from tqdm import tqdm

# Metric
metric = evaluate.load("rouge")

def evaluate_peft_model(sample,max_target_length=50):
    # generate summary
    outputs = model.generate(input_ids=sample["input_ids"].unsqueeze(0).cuda(), do_sample=True, top_p=0.9, max_new_tokens=max_target_length)    
    prediction = tokenizer.decode(outputs[0].detach().cpu().numpy(), skip_special_tokens=True)
    # decode eval sample
    # Replace -100 in the labels as we can't decode them.
    labels = np.where(sample['labels'] != -100, sample['labels'], tokenizer.pad_token_id)
    labels = tokenizer.decode(labels, skip_special_tokens=True)

    # Some simple post-processing
    return prediction, labels

# load test dataset from distk
test_dataset = load_from_disk(f"{dataset_path}/eval").with_format("torch")

# run predictions
# this can take ~45 minutes
predictions, references = [] , []
for sample in tqdm(test_dataset):
    p,l = evaluate_peft_model(sample)
    predictions.append(p)
    references.append(l)

# compute metric 
rogue = metric.compute(predictions=predictions, references=references, use_stemmer=True)

# print results 
print(f"Rogue1: {rogue['rouge1']* 100:2f}%")
print(f"rouge2: {rogue['rouge2']* 100:2f}%")
print(f"rougeL: {rogue['rougeL']* 100:2f}%")
print(f"rougeLsum: {rogue['rougeLsum']* 100:2f}%")

# Rogue1: 50.386161%
# rouge2: 24.842412%
# rougeL: 41.370130%
# rougeLsum: 41.394230%
# æˆ‘ä»¬ PEFT å¾®è°ƒåçš„ FLAN-T5-XXL åœ¨æµ‹è¯•é›†ä¸Šå–å¾—äº† 50.38% çš„ rogue1 åˆ†æ•°ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œflan-t5-base çš„å…¨æ¨¡å‹å¾®è°ƒè·å¾—äº† 47.23 çš„ rouge1 åˆ†æ•°ã€‚rouge1 åˆ†æ•°æé«˜äº† 3% ã€‚

# ä»¤äººéš¾ä»¥ç½®ä¿¡çš„æ˜¯ï¼Œæˆ‘ä»¬çš„ LoRA checkpoint åªæœ‰ 84MBï¼Œè€Œä¸”æ€§èƒ½æ¯”å¯¹æ›´å°çš„æ¨¡å‹è¿›è¡Œå…¨æ¨¡å‹å¾®è°ƒåçš„ checkpoint æ›´å¥½ã€‚

def main():
    pass


if __name__ == '__main__':
    main()
