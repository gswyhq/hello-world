
# æ£€ç´¢å¢å¼ºç”Ÿæˆ ï¼ˆRAGï¼‰ æ˜¯ä¸€ç§å°†ä¿¡æ¯æ£€ç´¢ä¸ç”Ÿæˆæ¨¡å‹ç›¸ç»“åˆçš„æ··åˆæ–¹æ³•ã€‚å®ƒé€šè¿‡æ•´åˆå¤–éƒ¨çŸ¥è¯†æ¥å¢å¼ºè¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼Œä»è€Œæé«˜å‡†ç¡®æ€§å’Œäº‹å®æ­£ç¡®æ€§ã€‚

####################################################################################################################################
# æ–¹æ³•21ï¼š
# å°†ä½¿ç”¨å¼ºåŒ–å­¦ä¹ (RL) RAGæ–¹æ³•ï¼Œä½¿ç”¨æä¾›çš„æ–‡æ¡£ç”Ÿæˆç»™å®šé—®é¢˜çš„ç­”æ¡ˆã€‚

## ç›®å½•

# - ç¯å¢ƒè®¾ç½®
# - æ•°æ®é¢„å¤„ç†
# - æ–‡æ¡£åµŒå…¥ç”Ÿæˆ
# - å‘é‡å­˜å‚¨å®ç°
# - ç®€å•æ£€ç´¢å®ç°
# - ä½™å¼¦ç›¸ä¼¼åº¦
# - ç›¸ä¼¼åº¦æœç´¢
# - LLMå“åº”ç”Ÿæˆ
# - åŸºæœ¬RAGç®¡é“
# - åŸºæœ¬RAGè¯„ä¼°
# - RAGä¸­çš„å¼ºåŒ–å­¦ä¹ 
# - çŠ¶æ€ã€åŠ¨ä½œç©ºé—´å’Œå¥–åŠ±æ–¹æ³•
# - ç­–ç•¥ç½‘ç»œ
# - å•ä¸ªRLæ­¥éª¤
# - è®­ç»ƒå‚æ•°å’Œç­–ç•¥æ›´æ–°
# - è®­ç»ƒå¾ªç¯
# - æ€§èƒ½æ¯”è¾ƒé€»è¾‘
# - è¯„ä¼°æ¡†æ¶
# - è¯„ä¼°RLä¸ç®€å•RAG
# - ä¿å­˜æ¯”è¾ƒç»“æœ
# - ç»“è®º

## ç¯å¢ƒè®¾ç½®

# å¯¼å…¥osæ¨¡å—ä»¥ä¸æ“ä½œç³»ç»Ÿäº¤äº’
import os
# å¯¼å…¥OpenAIæ¨¡å—ä»¥ä½¿ç”¨OpenAIçš„API
from openai import OpenAI
# å¯¼å…¥numpyè¿›è¡Œæ•°å€¼è¿ç®—
import numpy as np

# å¯¼å…¥jsonå¤„ç†JSONæ•°æ®
import json

# å¯¼å…¥typingæ¨¡å—è¿›è¡Œç±»å‹æç¤º
from typing import Dict, List, Tuple, Optional, Union

# ä½¿ç”¨OpenAIå®¢æˆ·ç«¯è®¾ç½®APIè¿æ¥
# æ›¿æ¢base_urlå’Œapi_keyä¸ºä½ è‡ªå·±çš„å€¼

client = OpenAI(
    base_url="https://api.studio.nebius.com/v1/",
    api_key=os.environ["OPENAI_API_KEY"]
)

# æ•°æ®é¢„å¤„ç†
# ç°åœ¨æˆ‘ä»¬è¿›å…¥æ•°æ®é¢„å¤„ç†é˜¶æ®µï¼Œéœ€è¦åŠ è½½æ•°æ®å¹¶è¿›è¡Œé¢„å¤„ç†ã€‚è®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå‡½æ•°ï¼Œä»ç›®å½•ä¸­åŠ è½½æ‰€æœ‰.txtæ–‡ä»¶ï¼Œå¹¶è¿”å›ä¸€ä¸ªæ–‡æ¡£åˆ—è¡¨ã€‚
# åŠ è½½ç›®å½•ä¸­æ‰€æœ‰æ–‡æ¡£çš„å‡½æ•°
def load_documents(directory_path: str) -> List[str]:
    """åŠ è½½æŒ‡å®šç›®å½•ä¸­çš„æ‰€æœ‰æ–‡æœ¬æ–‡ä»¶ã€‚

    Args:
        directory_path (str): åŒ…å«æ–‡æœ¬æ–‡ä»¶çš„ç›®å½•è·¯å¾„ã€‚

    Returns:
        List[str]: åŒ…å«æ¯ä¸ªæ–‡æœ¬æ–‡ä»¶å†…å®¹çš„å­—ç¬¦ä¸²åˆ—è¡¨ã€‚
    """
    documents = []  # åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨å­˜å‚¨æ–‡æ¡£å†…å®¹
    for filename in os.listdir(directory_path):  # éå†ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶
        if filename.endswith(".txt"):  # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸º.txtæ‰©å±•å
            # ä»¥UTF-8ç¼–ç æ‰“å¼€æ–‡ä»¶å¹¶å°†å…¶å†…å®¹æ·»åŠ åˆ°åˆ—è¡¨ä¸­
            with open(os.path.join(directory_path, filename), 'r', encoding='utf-8') as file:
                documents.append(file.read())
    return documents  # è¿”å›æ–‡æ¡£å†…å®¹åˆ—è¡¨


# æˆ‘ä»¬éœ€è¦åˆ›å»ºä¸€ä¸ªå‡½æ•°ï¼ŒåŠ è½½æ–‡æ¡£åå¯¹å…¶è¿›è¡Œåˆ†å—å¤„ç†ã€‚æˆ‘ä»¬ä½¿ç”¨100ä¸ªå­—ç¬¦çš„å—å¤§å°ï¼Œä½†ä½ å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´ã€‚
# å°†æ–‡æ¡£åˆ†å‰²æˆå—çš„å‡½æ•°
def split_into_chunks(documents: List[str], chunk_size: int = 30) -> List[str]:
    """å°†æ–‡æ¡£åˆ†å‰²æˆæŒ‡å®šå¤§å°çš„å°å—ã€‚

    Args:
        documents (List[str]): è¦åˆ†å‰²æˆå—çš„æ–‡æ¡£å­—ç¬¦ä¸²åˆ—è¡¨ã€‚
        chunk_size (int): æ¯ä¸ªå—ä¸­çš„æœ€å¤§å•è¯æ•°ã€‚é»˜è®¤ä¸º100ã€‚

    Returns:
        List[str]: åŒ…å«æ‰€æœ‰å—çš„å­—ç¬¦ä¸²åˆ—è¡¨ï¼Œæ¯ä¸ªå—æœ€å¤šåŒ…å«chunk_sizeä¸ªå•è¯ã€‚
    """
    chunks = []  # åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨å­˜å‚¨å—
    for doc in documents:  # éå†æ¯ä¸ªæ–‡æ¡£
        words = doc.split()  # å°†æ–‡æ¡£æŒ‰å•è¯åˆ†å‰²
        # åˆ›å»ºæŒ‡å®šå¤§å°çš„å—
        for i in range(0, len(words), chunk_size):
            chunk = " ".join(words[i:i + chunk_size])  # å°†å•è¯ç»„åˆæˆå—
            chunks.append(chunk)  # å°†å—æ·»åŠ åˆ°åˆ—è¡¨ä¸­
    return chunks  # è¿”å›å—åˆ—è¡¨


# æ­¤æ­¥éª¤æ˜¯å¯é€‰çš„ï¼Œå…¶ä¸­æˆ‘ä»¬é€šè¿‡åˆ é™¤ç‰¹æ®Šå­—ç¬¦ã€è½¬æ¢ä¸ºå°å†™ç­‰æ–¹å¼å¯¹æ¯ä¸ªå—è¿›è¡Œé¢„å¤„ç†ã€‚
# é¢„å¤„ç†æ–‡æœ¬çš„å‡½æ•°
def preprocess_text(text: str) -> str:
    """é€šè¿‡è½¬æ¢ä¸ºå°å†™å’Œåˆ é™¤ç‰¹æ®Šå­—ç¬¦æ¥é¢„å¤„ç†è¾“å…¥æ–‡æœ¬ã€‚

    Args:
        text (str): è¦é¢„å¤„ç†çš„è¾“å…¥æ–‡æœ¬ã€‚

    Returns:
        str: ä»…åŒ…å«å­—æ¯æ•°å­—å­—ç¬¦å’Œç©ºæ ¼çš„é¢„å¤„ç†æ–‡æœ¬ã€‚
    """
    # å°†æ–‡æœ¬è½¬æ¢ä¸ºå°å†™
    text = text.lower()
    # åˆ é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œä»…ä¿ç•™å­—æ¯æ•°å­—å­—ç¬¦å’Œç©ºæ ¼
    text = ''.join(char for char in text if char.isalnum() or char.isspace())
    return text


# å¦‚æœä½ ä½¿ç”¨äº†ä¹‹å‰çš„é¢„å¤„ç†æ­¥éª¤ï¼Œå¯ä»¥åˆ›å»ºä¸€ä¸ªå‡½æ•°æ¥é¢„å¤„ç†æ‰€æœ‰å—ã€‚
# é¢„å¤„ç†æ‰€æœ‰å—çš„å‡½æ•°
def preprocess_chunks(chunks: List[str]) -> List[str]:
    """å¯¹æ‰€æœ‰æ–‡æœ¬å—åº”ç”¨é¢„å¤„ç†ã€‚

    Args:
        chunks (List[str]): è¦é¢„å¤„ç†çš„æ–‡æœ¬å—åˆ—è¡¨ã€‚

    Returns:
        List[str]: é¢„å¤„ç†åçš„æ–‡æœ¬å—åˆ—è¡¨ã€‚
    """
    # å¯¹åˆ—è¡¨ä¸­çš„æ¯ä¸ªå—åº”ç”¨preprocess_textå‡½æ•°
    return [preprocess_text(chunk) for chunk in chunks]


# ç°åœ¨æˆ‘ä»¬å·²ç»å®ç°äº†æ‰€æœ‰æ•°æ®é¢„å¤„ç†å‡½æ•°ï¼Œå¯ä»¥ä»ç›®å½•ä¸­åŠ è½½æ–‡æ¡£ï¼Œå°†å®ƒä»¬åˆ†å‰²æˆå—ï¼Œå¹¶å¯¹å—è¿›è¡Œé¢„å¤„ç†ã€‚
# æŒ‡å®šåŒ…å«æ–‡æœ¬æ–‡ä»¶çš„ç›®å½•è·¯å¾„
directory_path = "data"

# ä»æŒ‡å®šç›®å½•åŠ è½½æ‰€æœ‰æ–‡æœ¬æ–‡æ¡£
documents = load_documents(directory_path)

# å°†åŠ è½½çš„æ–‡æ¡£åˆ†å‰²æˆè¾ƒå°çš„æ–‡æœ¬å—
chunks = split_into_chunks(documents)

# å¯¹å—è¿›è¡Œé¢„å¤„ç†ï¼ˆä¾‹å¦‚ï¼Œè½¬å°å†™ï¼Œåˆ é™¤ç‰¹æ®Šå­—ç¬¦ï¼‰
preprocessed_chunks = preprocess_chunks(chunks)

# æ–‡æ¡£åµŒå…¥ç”Ÿæˆ
# åœ¨ä¸Šä¸€æ­¥ä¸­ï¼Œæˆ‘ä»¬å¯¹æ–‡æ¡£è¿›è¡Œäº†åˆ†å—å¤„ç†ã€‚ç°åœ¨æ˜¯æ—¶å€™ä¸ºå—æ•°æ®é›†ç”ŸæˆåµŒå…¥ã€‚ç”±äºRAGçš„çŸ¥è¯†åº“é€šå¸¸éå¸¸å¤§ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦æ‰¹é‡ç”ŸæˆåµŒå…¥ã€‚è®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªæ ¸å¿ƒå‡½æ•°ï¼Œç”¨äºæ‰¹é‡ç”Ÿæˆå—çš„åµŒå…¥ã€‚
# ä¸ºå•ä¸ªæ‰¹æ¬¡çš„æ–‡æœ¬å—ç”ŸæˆåµŒå…¥çš„å‡½æ•°
def generate_embeddings_batch(chunks_batch: List[str], model: str = "BAAI/bge-en-icl") -> List[List[float]]:
    """ä½¿ç”¨OpenAIå®¢æˆ·ç«¯ä¸ºä¸€æ‰¹æ–‡æœ¬å—ç”ŸæˆåµŒå…¥ã€‚

    Args:
        chunks_batch (List[str]): è¦ç”ŸæˆåµŒå…¥çš„ä¸€æ‰¹æ–‡æœ¬å—ã€‚
        model (str): ç”¨äºåµŒå…¥ç”Ÿæˆçš„æ¨¡å‹ã€‚é»˜è®¤ä¸º"BAAI/bge-en-icl"ã€‚

    Returns:
        List[List[float]]: åŒ…å«æ‰€æœ‰å—åµŒå…¥çš„åˆ—è¡¨ï¼Œæ¯ä¸ªåµŒå…¥æ˜¯ä¸€ä¸ªæµ®ç‚¹æ•°åˆ—è¡¨ã€‚
    """
    # ä½¿ç”¨OpenAIå®¢æˆ·ç«¯ä¸ºè¾“å…¥æ‰¹æ¬¡åˆ›å»ºåµŒå…¥
    response = client.embeddings.create(
        model=model,  # æŒ‡å®šç”¨äºåµŒå…¥ç”Ÿæˆçš„æ¨¡å‹
        input=chunks_batch  # æä¾›è¦å¤„ç†çš„æ–‡æœ¬å—åˆ—è¡¨ä½œä¸ºè¾“å…¥
    )
    # ä»å“åº”ä¸­æå–åµŒå…¥å¹¶è¿”å›
    embeddings = [item.embedding for item in response.data]
    return embeddings


# æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œä¸ºæ‰€æœ‰æ–‡æœ¬å—ç”ŸæˆåµŒå…¥ã€‚è¯¥å‡½æ•°å°†æ–‡æœ¬å—åˆ—è¡¨ä½œä¸ºè¾“å…¥ï¼Œå¹¶ä½¿ç”¨OpenAIå®¢æˆ·ç«¯ä¸ºæ¯ä¸ªå—æ‰¹æ¬¡ç”ŸæˆåµŒå…¥ã€‚å‡½æ•°è¿”å›ä¸€ä¸ªåŒ…å«æ‰€æœ‰å—åµŒå…¥çš„NumPyæ•°ç»„ã€‚

# ä¸ºæ‰€æœ‰å—ç”ŸæˆåµŒå…¥çš„å‡½æ•°
def generate_embeddings(chunks: List[str], batch_size: int = 10) -> np.ndarray:
    """ä¸ºæ‰€æœ‰æ–‡æœ¬å—ç”ŸæˆåµŒå…¥ã€‚

    Args:
        chunks (List[str]): è¦ç”ŸæˆåµŒå…¥çš„æ–‡æœ¬å—åˆ—è¡¨ã€‚
        batch_size (int): æ¯æ‰¹å¤„ç†çš„å—æ•°ã€‚é»˜è®¤ä¸º10ã€‚

    Returns:
        np.ndarray: åŒ…å«æ‰€æœ‰å—åµŒå…¥çš„NumPyæ•°ç»„ã€‚
    """
    all_embeddings = []  # åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨å­˜å‚¨æ‰€æœ‰åµŒå…¥

    # éå†å—ï¼ŒæŒ‰æ‰¹æ¬¡å¤„ç†
    for i in range(0, len(chunks), batch_size):
        # æå–å½“å‰æ‰¹æ¬¡çš„å—
        batch = chunks[i:i + batch_size]
        # ä¸ºå½“å‰æ‰¹æ¬¡ç”ŸæˆåµŒå…¥
        embeddings = generate_embeddings_batch(batch)
        # å°†å½“å‰æ‰¹æ¬¡çš„åµŒå…¥æ‰©å±•åˆ°æ‰€æœ‰åµŒå…¥åˆ—è¡¨ä¸­
        all_embeddings.extend(embeddings)

    # å°†åµŒå…¥åˆ—è¡¨è½¬æ¢ä¸ºNumPyæ•°ç»„å¹¶è¿”å›
    return np.array(all_embeddings)


# è®©æˆ‘ä»¬åˆ›å»ºå¦ä¸€ä¸ªå‡½æ•°ï¼Œå°†åµŒå…¥ä¿å­˜åˆ°JSONæ–‡ä»¶ä¸­ã€‚
# ä¿å­˜åµŒå…¥åˆ°æ–‡ä»¶çš„å‡½æ•°
def save_embeddings(embeddings: np.ndarray, output_file: str) -> None:
    """å°†åµŒå…¥ä¿å­˜åˆ°JSONæ–‡ä»¶ä¸­ã€‚

    Args:
        embeddings (np.ndarray): è¦ä¿å­˜çš„åµŒå…¥NumPyæ•°ç»„ã€‚
        output_file (str): è¾“å‡ºJSONæ–‡ä»¶çš„è·¯å¾„ï¼ŒåµŒå…¥å°†ä¿å­˜åˆ°æ­¤æ–‡ä»¶ã€‚

    Returns:
        None
    """
    # ä»¥UTF-8ç¼–ç æ‰“å¼€æŒ‡å®šæ–‡ä»¶
    with open(output_file, 'w', encoding='utf-8') as file:
        # å°†NumPyæ•°ç»„è½¬æ¢ä¸ºåˆ—è¡¨å¹¶ä¿å­˜ä¸ºJSON
        json.dump(embeddings.tolist(), file)


# ç°åœ¨æˆ‘ä»¬å·²ç»å®ç°äº†æ‰€æœ‰åµŒå…¥ç”Ÿæˆå‡½æ•°ï¼Œå¯ä»¥ä¸ºé¢„å¤„ç†åçš„æ–‡æœ¬å—ç”ŸæˆåµŒå…¥ï¼Œå¹¶å°†å®ƒä»¬ä¿å­˜åˆ°JSONæ–‡ä»¶ä¸­ã€‚
# ç¡®ä¿å—å·²é¢„å¤„ç†åå†ç”ŸæˆåµŒå…¥
preprocessed_chunks = preprocess_chunks(chunks)

# ä¸ºé¢„å¤„ç†åçš„å—ç”ŸæˆåµŒå…¥
embeddings = generate_embeddings(preprocessed_chunks)

# å°†ç”Ÿæˆçš„åµŒå…¥ä¿å­˜åˆ°åä¸º"embeddings.json"çš„JSONæ–‡ä»¶ä¸­
save_embeddings(embeddings, "embeddings.json")

# å‘é‡å­˜å‚¨å®ç°
# ç”±äºæˆ‘ä»¬æ²¡æœ‰ä½¿ç”¨ä»»ä½•Pythonåº“è¿›è¡Œå‘é‡å­˜å‚¨ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªç®€å•çš„å­—å…¸å®ç°å‘é‡å­˜å‚¨ã€‚
# åˆå§‹åŒ–å†…å­˜ä¸­çš„å‘é‡å­˜å‚¨ï¼Œä½œä¸ºå­—å…¸
# é”®æ˜¯å”¯ä¸€çš„æ ‡è¯†ç¬¦ï¼ˆæ•´æ•°ï¼‰ï¼Œå€¼æ˜¯åŒ…å«åµŒå…¥å’Œå¯¹åº”æ–‡æœ¬å—çš„å­—å…¸
vector_store: dict[int, dict[str, object]] = {}


# å°†åµŒå…¥å’Œå¯¹åº”çš„æ–‡æœ¬å—æ·»åŠ åˆ°å‘é‡å­˜å‚¨ä¸­çš„å‡½æ•°
def add_to_vector_store(embeddings: np.ndarray, chunks: List[str]) -> None:
    """å°†åµŒå…¥åŠå…¶å¯¹åº”çš„æ–‡æœ¬å—æ·»åŠ åˆ°å‘é‡å­˜å‚¨ä¸­ã€‚

    Args:
        embeddings (np.ndarray): è¦æ·»åŠ çš„åµŒå…¥NumPyæ•°ç»„ã€‚
        chunks (List[str]): ä¸åµŒå…¥å¯¹åº”çš„æ–‡æœ¬å—åˆ—è¡¨ã€‚

    Returns:
        None
    """
    # åŒæ—¶éå†åµŒå…¥å’Œå—
    for embedding, chunk in zip(embeddings, chunks):
        # å°†æ¯ä¸ªåµŒå…¥åŠå…¶å¯¹åº”çš„å—æ·»åŠ åˆ°å‘é‡å­˜å‚¨ä¸­
        # ä½¿ç”¨å‘é‡å­˜å‚¨çš„å½“å‰é•¿åº¦ä½œä¸ºå”¯ä¸€çš„é”®
        vector_store[len(vector_store)] = {"embedding": embedding, "chunk": chunk}


# ç®€å•æ£€ç´¢å®ç°
# æˆ‘ä»¬çŸ¥é“ï¼Œè¦æ£€ç´¢ä¸ç»™å®šæŸ¥è¯¢æœ€ç›¸ä¼¼çš„æ–‡æœ¬å—ï¼Œå¯ä»¥ä½¿ç”¨æŸ¥è¯¢åµŒå…¥ä¸æ‰€æœ‰æ–‡æœ¬å—åµŒå…¥ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦ã€‚ä½™å¼¦ç›¸ä¼¼åº¦è¶Šé«˜ï¼Œæ–‡æœ¬å—è¶Šç›¸ä¼¼ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥æ ¹æ®ç›¸ä¼¼åº¦åˆ†æ•°å¯¹å—è¿›è¡Œæ’åºï¼Œå¹¶è¿”å›æœ€ç›¸ä¼¼çš„å‰kä¸ªå—ã€‚
#
# è®©æˆ‘ä»¬å®ç°ä¸€ä¸ªåŸºäºä½™å¼¦ç›¸ä¼¼åº¦çš„ç®€å•æ£€ç´¢å‡½æ•°ã€‚

# è®¡ç®—ä¸¤ä¸ªå‘é‡ä¹‹é—´ä½™å¼¦ç›¸ä¼¼åº¦çš„å‡½æ•°
def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:
    """è®¡ç®—ä¸¤ä¸ªå‘é‡ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦ã€‚

    Args:
        vec1 (np.ndarray): ç¬¬ä¸€ä¸ªå‘é‡ã€‚
        vec2 (np.ndarray): ç¬¬äºŒä¸ªå‘é‡ã€‚

    Returns:
        float: ä¸¤ä¸ªå‘é‡ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼ŒèŒƒå›´åœ¨-1åˆ°1ä¹‹é—´ã€‚
    """
    # è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ç‚¹ç§¯
    dot_product = np.dot(vec1, vec2)
    # è®¡ç®—ç¬¬ä¸€ä¸ªå‘é‡çš„æ¨¡ï¼ˆèŒƒæ•°ï¼‰
    norm_vec1 = np.linalg.norm(vec1)
    # è®¡ç®—ç¬¬äºŒä¸ªå‘é‡çš„æ¨¡ï¼ˆèŒƒæ•°ï¼‰
    norm_vec2 = np.linalg.norm(vec2)
    # è¿”å›ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œå³ç‚¹ç§¯ä¸æ¨¡çš„ä¹˜ç§¯çš„æ¯”å€¼
    return dot_product / (norm_vec1 * norm_vec2)


# å½“æˆ‘ä»¬è®¡ç®—æŸ¥è¯¢ä¸æ‰€æœ‰å—ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦æ—¶ï¼Œå¯ä»¥æ‰§è¡Œç›¸ä¼¼åº¦æœç´¢ã€‚æ ¹æ®top_kå‚æ•°ï¼Œæ£€ç´¢æœ€ç›¸ä¼¼çš„å‰kä¸ªå—ã€‚
# åœ¨å‘é‡å­˜å‚¨ä¸­æ‰§è¡Œç›¸ä¼¼åº¦æœç´¢çš„å‡½æ•°
def similarity_search(query_embedding: np.ndarray, top_k: int = 5) -> List[str]:
    """åœ¨å‘é‡å­˜å‚¨ä¸­æ‰§è¡Œç›¸ä¼¼åº¦æœç´¢ï¼Œå¹¶è¿”å›æœ€ç›¸ä¼¼çš„å‰kä¸ªå—ã€‚

    Args:
        query_embedding (np.ndarray): æŸ¥è¯¢çš„åµŒå…¥å‘é‡ã€‚
        top_k (int): è¦æ£€ç´¢çš„æœ€ç›¸ä¼¼å—çš„æ•°é‡ã€‚é»˜è®¤ä¸º5ã€‚

    Returns:
        List[str]: åŒ…å«æœ€ç›¸ä¼¼æ–‡æœ¬å—çš„åˆ—è¡¨ã€‚
    """
    similarities = []  # åˆå§‹åŒ–ä¸€ä¸ªåˆ—è¡¨å­˜å‚¨ç›¸ä¼¼åº¦åˆ†æ•°å’Œå¯¹åº”çš„é”®

    # éå†å‘é‡å­˜å‚¨ä¸­çš„æ‰€æœ‰é¡¹ç›®
    for key, value in vector_store.items():
        # è®¡ç®—æŸ¥è¯¢åµŒå…¥ä¸å­˜å‚¨åµŒå…¥ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦
        similarity = cosine_similarity(query_embedding, value["embedding"])
        # å°†é”®å’Œç›¸ä¼¼åº¦åˆ†æ•°ä½œä¸ºå…ƒç»„é™„åŠ åˆ°åˆ—è¡¨ä¸­
        similarities.append((key, similarity))

    # æ ¹æ®ç›¸ä¼¼åº¦åˆ†æ•°æŒ‰é™åºæ’åºåˆ—è¡¨
    similarities = sorted(similarities, key=lambda x: x[1], reverse=True)

    # æ ¹æ®é”®æ£€ç´¢æœ€ç›¸ä¼¼çš„å—
    return [vector_store[key]["chunk"] for key, _ in similarities[:top_k]]


# ä¸€æ—¦æˆ‘ä»¬å‡†å¤‡å¥½ç›¸ä¼¼åº¦æœç´¢å‡½æ•°ï¼Œå°±å¯ä»¥åœ¨ä¸Šé¢æ„å»ºä¸€ä¸ªæ£€ç´¢å‡½æ•°ï¼Œæ ¹æ®æŸ¥è¯¢æä¾›ç›¸å…³çš„å—ã€‚
# ä¸ºæŸ¥è¯¢æ£€ç´¢ç›¸å…³æ–‡æ¡£å—çš„å‡½æ•°
def retrieve_relevant_chunks(query_text: str, top_k: int = 5) -> List[str]:
    """ä¸ºç»™å®šæŸ¥è¯¢æ–‡æœ¬æ£€ç´¢æœ€ç›¸å…³çš„æ–‡æ¡£å—ã€‚

    Args:
        query_text (str): è¦æ£€ç´¢ç›¸å…³å—çš„æŸ¥è¯¢æ–‡æœ¬ã€‚
        top_k (int): è¦æ£€ç´¢çš„æœ€ç›¸å…³å—çš„æ•°é‡ã€‚é»˜è®¤ä¸º5ã€‚

    Returns:
        List[str]: åŒ…å«æœ€ç›¸å…³æ–‡æœ¬å—çš„åˆ—è¡¨ã€‚
    """
    # ä½¿ç”¨åµŒå…¥æ¨¡å‹ä¸ºæŸ¥è¯¢æ–‡æœ¬ç”ŸæˆåµŒå…¥
    query_embedding = generate_embeddings([query_text])[0]

    # æ‰§è¡Œç›¸ä¼¼åº¦æœç´¢ä»¥æ‰¾åˆ°æœ€ç›¸å…³çš„å—
    relevant_chunks = similarity_search(query_embedding, top_k=top_k)

    # è¿”å›ç›¸å…³å—åˆ—è¡¨
    return relevant_chunks


# ç°åœ¨æˆ‘ä»¬å·²ç»å®ç°äº†æ‰€æœ‰æ£€ç´¢å‡½æ•°ï¼Œå¯ä»¥æµ‹è¯•æ£€ç´¢ç³»ç»Ÿåœ¨ç¤ºä¾‹æŸ¥è¯¢ä¸Šçš„æ€§èƒ½ã€‚

# å°†ç”Ÿæˆçš„åµŒå…¥åŠå…¶å¯¹åº”çš„é¢„å¤„ç†å—æ·»åŠ åˆ°å‘é‡å­˜å‚¨ä¸­
add_to_vector_store(embeddings, preprocessed_chunks)

# å®šä¹‰ä¸€ä¸ªæŸ¥è¯¢æ–‡æœ¬ï¼Œä»¥æ£€ç´¢ç›¸å…³çš„æ–‡æ¡£å—
query_text = "ä»€ä¹ˆæ˜¯é‡å­è®¡ç®—ï¼Ÿ"

# æ ¹æ®æŸ¥è¯¢æ–‡æœ¬ä»å‘é‡å­˜å‚¨ä¸­æ£€ç´¢æœ€ç›¸å…³çš„å—
relevant_chunks = retrieve_relevant_chunks(query_text)

# æ‰“å°æ¯ä¸ªæ£€ç´¢åˆ°çš„ç›¸å…³å—çš„å‰50ä¸ªå­—ç¬¦
for idx, chunk in enumerate(relevant_chunks):
    print(f"å— {idx + 1}: {chunk[:50]} ... ")
    print("-" * 50)  # æ‰“å°åˆ†éš”çº¿

# LLMå“åº”ç”Ÿæˆ
# å½“æˆ‘ä»¬æœ‰ä¸€ä¸ªæŸ¥è¯¢å’Œä¸€ç»„ç›¸å…³çš„æ–‡æ¡£å—æ—¶ï¼Œå¯ä»¥ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ ¹æ®æŸ¥è¯¢å’Œæ£€ç´¢åˆ°çš„ä¿¡æ¯ç”Ÿæˆå“åº”ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨OpenAI APIæ ¹æ®æŸ¥è¯¢å’Œæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ç”ŸæˆæŸ¥è¯¢çš„å“åº”ã€‚

# é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªå‡½æ•°ï¼Œæ ¹æ®æŸ¥è¯¢å’Œç›¸å…³å—æ„å»ºLLMçš„è¾“å…¥æç¤ºã€‚

# æ„å»ºåŒ…å«ä¸Šä¸‹æ–‡çš„æç¤ºçš„å‡½æ•°
def construct_prompt(query: str, context_chunks: List[str]) -> str:
    """é€šè¿‡ç»“åˆæŸ¥è¯¢å’Œæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡å—æ„å»ºæç¤ºã€‚

    Args:
        query (str): è¦æ„å»ºæç¤ºçš„æŸ¥è¯¢æ–‡æœ¬ã€‚
        context_chunks (List[str]): åŒ…å«ç›¸å…³ä¸Šä¸‹æ–‡å—çš„åˆ—è¡¨ã€‚

    Returns:
        str: è¦ç”¨äºLLMè¾“å…¥çš„æ„å»ºæç¤ºã€‚
    """
    # å°†æ‰€æœ‰ä¸Šä¸‹æ–‡å—ç»„åˆæˆä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œå—ä¹‹é—´ç”¨æ¢è¡Œç¬¦åˆ†éš”
    context = "\n".join(context_chunks)

    # å®šä¹‰æŒ‡å¯¼LLMè¡Œä¸ºçš„ç³»ç»Ÿæ¶ˆæ¯
    system_message = (
        "ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚ä»…ä½¿ç”¨æä¾›çš„ä¸Šä¸‹æ–‡æ¥å›ç­”é—®é¢˜ã€‚"
        "å¦‚æœä¸Šä¸‹æ–‡ä¸­ä¸åŒ…å«æ‰€éœ€ä¿¡æ¯ï¼Œè¯·è¯´'æˆ‘æ²¡æœ‰è¶³å¤Ÿçš„ä¿¡æ¯æ¥å›ç­”è¿™ä¸ªé—®é¢˜ã€‚'"
    )

    # é€šè¿‡ç»“åˆç³»ç»Ÿæ¶ˆæ¯ã€ä¸Šä¸‹æ–‡å’ŒæŸ¥è¯¢æ„å»ºæœ€ç»ˆæç¤º
    prompt = f"ç³»ç»Ÿ: {system_message}\n\nä¸Šä¸‹æ–‡:\n{context}\n\né—®é¢˜:\n{query}\n\nå›ç­”:"
    return prompt


# ä¸ºäº†ç”ŸæˆLLMå“åº”ï¼Œæˆ‘ä»¬éœ€è¦å®ç°ä¸€ä¸ªå‡½æ•°ï¼Œå°†æ„å»ºçš„è¾“å…¥æç¤ºå‘é€åˆ°OpenAI APIä»¥ç”Ÿæˆå“åº”ã€‚

# ä½¿ç”¨OpenAIèŠå¤©æ¨¡å‹ç”Ÿæˆå“åº”çš„å‡½æ•°
def generate_response(
        prompt: str,
        model: str = "google/gemma-2-2b-it",
        max_tokens: int = 512,
        temperature: float = 1,
        top_p: float = 0.9,
        top_k: int = 50
) -> str:
    """ä½¿ç”¨OpenAIèŠå¤©æ¨¡å‹æ ¹æ®æ„å»ºçš„æç¤ºç”Ÿæˆå“åº”ã€‚

    Args:
        prompt (str): è¦æä¾›ç»™èŠå¤©æ¨¡å‹çš„è¾“å…¥æç¤ºã€‚
        model (str): ç”¨äºç”Ÿæˆå“åº”çš„æ¨¡å‹ã€‚é»˜è®¤ä¸º"google/gemma-2-2b-it"ã€‚
        max_tokens (int): å“åº”ä¸­çš„æœ€å¤§ä»¤ç‰Œæ•°ã€‚é»˜è®¤ä¸º512ã€‚
        temperature (float): é‡‡æ ·æ¸©åº¦ï¼Œæ§åˆ¶å“åº”çš„å¤šæ ·æ€§ã€‚é»˜è®¤ä¸º0.5ã€‚
        top_p (float): æ ¸é‡‡æ ·ä¸­çš„æ¦‚ç‡è´¨é‡ã€‚é»˜è®¤ä¸º0.9ã€‚
        top_k (int): è€ƒè™‘çš„æœ€é«˜æ¦‚ç‡ä»¤ç‰Œæ•°ã€‚é»˜è®¤ä¸º50ã€‚

    Returns:
        str: èŠå¤©æ¨¡å‹ç”Ÿæˆçš„å“åº”ã€‚
    """
    # ä½¿ç”¨OpenAIå®¢æˆ·ç«¯åˆ›å»ºèŠå¤©å®Œæˆ
    response = client.chat.completions.create(
        model=model,  # æŒ‡å®šç”¨äºç”Ÿæˆå“åº”çš„æ¨¡å‹
        max_tokens=max_tokens,  # å“åº”ä¸­çš„æœ€å¤§ä»¤ç‰Œæ•°
        temperature=temperature,  # é‡‡æ ·æ¸©åº¦ï¼Œæ§åˆ¶å“åº”çš„å¤šæ ·æ€§
        top_p=top_p,  # æ ¸é‡‡æ ·ä¸­çš„æ¦‚ç‡è´¨é‡
        extra_body={  # è¯·æ±‚çš„é¢å¤–å‚æ•°
            "top_k": top_k  # è€ƒè™‘çš„æœ€é«˜æ¦‚ç‡ä»¤ç‰Œæ•°
        },
        messages=[  # æä¾›ä¸Šä¸‹æ–‡çš„æ¶ˆæ¯åˆ—è¡¨
            {
                "role": "user",  # æ¶ˆæ¯å‘é€è€…çš„è§’è‰²ï¼ˆç”¨æˆ·ï¼‰
                "content": [  # æ¶ˆæ¯å†…å®¹
                    {
                        "type": "text",  # å†…å®¹ç±»å‹ï¼ˆæ–‡æœ¬ï¼‰
                        "text": prompt  # å®é™…æç¤ºæ–‡æœ¬
                    }
                ]
            }
        ]
    )
    # è¿”å›ç¬¬ä¸€ä¸ªé€‰æ‹©çš„å“åº”å†…å®¹
    return response.choices[0].message.content


# åŸºæœ¬RAGç®¡é“
# æˆ‘ä»¬ä¸èƒ½åå¤è¿è¡Œå°æ®µä»£ç ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦åˆ›å»ºä¸€ä¸ªç®€å•çš„RAGç®¡é“ï¼Œå®ƒåªéœ€è¦ä¸€ä¸ªå‚æ•°ï¼ˆæŸ¥è¯¢ï¼‰ï¼Œå¹¶è¿”å›LLMå“åº”ã€‚

# å®ç°åŸºæœ¬çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç®¡é“çš„å‡½æ•°
def basic_rag_pipeline(query: str) -> str:
    """å®ç°åŸºæœ¬çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç®¡é“ï¼šæ£€ç´¢ç›¸å…³å—ï¼Œæ„å»ºæç¤ºï¼Œç”Ÿæˆå“åº”ã€‚

    Args:
        query (str): è¦ç”Ÿæˆå“åº”çš„è¾“å…¥æŸ¥è¯¢ã€‚

    Returns:
        str: LLMæ ¹æ®æŸ¥è¯¢å’Œæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ç”Ÿæˆçš„å“åº”ã€‚
    """
    # æ­¥éª¤1ï¼šæ£€ç´¢ä¸æŸ¥è¯¢æœ€ç›¸å…³çš„å—
    relevant_chunks: List[str] = retrieve_relevant_chunks(query)

    # æ­¥éª¤2ï¼šä½¿ç”¨æŸ¥è¯¢å’Œæ£€ç´¢åˆ°çš„å—æ„å»ºæç¤º
    prompt: str = construct_prompt(query, relevant_chunks)

    # æ­¥éª¤3ï¼šä½¿ç”¨æ„å»ºçš„æç¤ºç”ŸæˆLLMå“åº”
    response: str = generate_response(prompt)

    # è¿”å›ç”Ÿæˆçš„å“åº”
    return response


# è¯„ä¼°åŸºæœ¬RAG
# ç°åœ¨æˆ‘ä»¬å·²ç»å®ç°äº†åŸºæœ¬çš„RAGç®¡é“ï¼Œå¯ä»¥ä½¿ç”¨å®ƒè¿›è¡Œè¯„ä¼°ã€‚æˆ‘ä»¬çš„è¯„ä¼°æŸ¥è¯¢åŒ…å«ä¸åŒçš„ç›®æ ‡æ®µï¼Œå¦‚äº‹å®æ€§æŸ¥è¯¢å’Œå¤æ‚æ€§è´¨ã€‚æˆ‘ä»¬å°†æµ‹è¯•RAGç®¡é“çš„äº‹å®çŸ¥è¯†ã€‚

# è®©æˆ‘ä»¬åŠ è½½æˆ‘ä»¬çš„è¯„ä¼°æŸ¥è¯¢åŠå…¶é¢„æœŸç­”æ¡ˆã€‚
# æ‰“å¼€éªŒè¯æ•°æ®æ–‡ä»¶å¹¶åŠ è½½å…¶å†…å®¹ä½œä¸ºå­—å…¸
with open('data/val_rl.json', 'r') as file:
    validation_data = json.load(file)

# ä½¿ç”¨åŸºæœ¬RAGç®¡é“æµ‹è¯•ç¤ºä¾‹æŸ¥è¯¢
sample_query = validation_data['basic_factual_questions'][0]['question']  # æå–æŸ¥è¯¢æ–‡æœ¬
expected_answer = validation_data['basic_factual_questions'][0]['answer']  # æå–æ­£ç¡®ç­”æ¡ˆ

# æ‰“å°ç¤ºä¾‹æŸ¥è¯¢å’Œé¢„æœŸç­”æ¡ˆ
print(f"ç¤ºä¾‹æŸ¥è¯¢: {sample_query}\n")
print(f"é¢„æœŸç­”æ¡ˆ: {expected_answer}\n")

# è®©æˆ‘ä»¬æµ‹è¯•åŸºæœ¬RAGç®¡é“åœ¨è¯„ä¼°æŸ¥è¯¢ä¸Šçš„æ€§èƒ½ã€‚
# æ‰“å°æ¶ˆæ¯ï¼ŒæŒ‡ç¤ºRAGç®¡é“æ­£åœ¨è¿è¡Œ
print("ğŸ” æ­£åœ¨è¿è¡Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç®¡é“...")
print(f"ğŸ“¥ æŸ¥è¯¢: {sample_query}\n")

# è¿è¡ŒRAGç®¡é“å¹¶è·å–å“åº”
response = basic_rag_pipeline(sample_query)

# ä»¥æ›´å¥½çš„æ ¼å¼æ‰“å°å“åº”
print("ğŸ¤– AIå“åº”:")
print("-" * 50)
print(response.strip())
print("-" * 50)

# æ‰“å°çœŸå®ç­”æ¡ˆä»¥è¿›è¡Œæ¯”è¾ƒ
print("âœ… çœŸå®ç­”æ¡ˆ:")
print("-" * 50)
print(expected_answer)
print("-" * 50)

# åŸºæœ¬RAGç®¡é“ä¼¼ä¹åœ¨å½“å‰çŠ¶æ€ä¸‹è¡¨ç°ä¸ä½³ã€‚ç”Ÿæˆçš„å“åº”ä¸ä»…ä¸çœŸå®ç­”æ¡ˆæ— å…³ï¼Œè¿˜ç¼ºå°‘å…³é”®ä¿¡æ¯ã€‚

# ä½†åœ¨æ¥ä¸‹æ¥çš„æ­¥éª¤ä¸­ï¼Œæˆ‘ä»¬å°†å®ç°ä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ çš„RAGç®¡é“ï¼Œä»¥è§£å†³è¿™äº›ä¸è¶³ã€‚è¿™å°†å¸®åŠ©æˆ‘ä»¬æ”¹è¿›æ£€ç´¢å’Œç”Ÿæˆè¿‡ç¨‹ï¼Œä½¿å“åº”æ›´å‡†ç¡®ã€ä¸Šä¸‹æ–‡æ›´ç›¸å…³ã€‚

# æ•¬è¯·æœŸå¾…ï¼Œæˆ‘ä»¬å°†RAGç®¡é“æå‡åˆ°ä¸€ä¸ªæ–°çš„é«˜åº¦ï¼ğŸš€

# å¼ºåŒ–å­¦ä¹ åœ¨RAGä¸­çš„åº”ç”¨
# å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learning, RLï¼‰æ˜¯ä¸€ç§æœºå™¨å­¦ä¹ ç±»å‹ï¼Œå…¶ä¸­ä»£ç†é€šè¿‡åœ¨ç¯å¢ƒä¸­é‡‡å–è¡ŒåŠ¨æ¥å­¦ä¹ å¦‚ä½•åšå‡ºå†³ç­–ï¼Œä»¥æœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±çš„æ¦‚å¿µã€‚ä¸ç›‘ç£å­¦ä¹ ä¸åŒï¼Œä»£ç†æ²¡æœ‰æ˜ç¡®è¢«å‘ŠçŸ¥é‡‡å–å“ªäº›è¡ŒåŠ¨ï¼Œè€Œæ˜¯å¿…é¡»é€šè¿‡è¯•é”™å‘ç°å“ªäº›è¡ŒåŠ¨ä¼šå¸¦æ¥æœ€å¤§çš„å¥–åŠ±ã€‚
#
# ä»¥ä¸‹æ˜¯å¼ºåŒ–å­¦ä¹ ç³»ç»Ÿçš„ä¸»è¦ç»„æˆéƒ¨åˆ†ï¼š
#
# ä»£ç†ï¼ˆAgentï¼‰ï¼šå­¦ä¹ å’Œå†³ç­–çš„ä¸»ä½“ã€‚
# ç¯å¢ƒï¼ˆEnvironmentï¼‰ï¼šä»£ç†ä¸å…¶äº¤äº’çš„ä¸–ç•Œã€‚
# çŠ¶æ€ï¼ˆStateï¼‰ï¼šä»£ç†åœ¨ç¯å¢ƒä¸­å½“å‰çš„æƒ…å†µã€‚
# åŠ¨ä½œï¼ˆActionï¼‰ï¼šä»£ç†å¯ä»¥é‡‡å–çš„ä¸€ç³»åˆ—å¯èƒ½è¡ŒåŠ¨ã€‚
# å¥–åŠ±ï¼ˆRewardï¼‰ï¼šä»£ç†åœ¨é‡‡å–è¡ŒåŠ¨åä»ç¯å¢ƒä¸­è·å¾—çš„åé¦ˆã€‚
# ç­–ç•¥ï¼ˆPolicyï¼‰ï¼šä»£ç†éµå¾ªçš„ç­–ç•¥ï¼Œä»¥å†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨ã€‚
# å¼ºåŒ–å­¦ä¹ çš„ç›®æ ‡æ˜¯å­¦ä¹ ä¸€ä¸ªç­–ç•¥Ï€ï¼Œä½¿å¾—åœ¨ç¯å¢ƒä¸­ç´¯ç§¯å¥–åŠ±çš„æœŸæœ›æœ€å¤§åŒ–ï¼š


# åœ¨RAGç³»ç»Ÿä¸­ï¼Œå¼ºåŒ–å­¦ä¹ å¯ä»¥ç”¨äºï¼š
#
# é€šè¿‡å­¦ä¹ å“ªäº›æ–‡æ¡£æœ€æœ‰å¸®åŠ©æ¥æ”¹è¿›æ£€ç´¢
# æ ¹æ®ç”¨æˆ·åé¦ˆè°ƒæ•´æç¤ºæ„é€ 
# é€šè¿‡ä»æˆåŠŸå“åº”ä¸­å­¦ä¹ æ¥ä¼˜åŒ–ç”Ÿæˆè¿‡ç¨‹
# çŠ¶æ€ã€åŠ¨ä½œç©ºé—´å’Œå¥–åŠ±æ–¹æ³•
# åœ¨ç¼–ç RLç®—æ³•æ—¶ï¼Œé¦–å…ˆè¦å®šä¹‰ä¸‰ä»¶äº‹ï¼š
#
# çŠ¶æ€ï¼ˆStateï¼‰ï¼šå½“å‰ç¯å¢ƒçš„æƒ…å†µã€‚åœ¨æˆ‘ä»¬çš„æƒ…å†µä¸‹ï¼Œåˆå§‹çŠ¶æ€æ˜¯æˆ‘ä»¬çš„åŸºæœ¬RAGç®¡é“ï¼ˆæŸ¥è¯¢ã€ä¸Šä¸‹æ–‡ã€å“åº”ï¼‰ã€‚
# åŠ¨ä½œç©ºé—´ï¼ˆAction Spaceï¼‰ï¼šä»£ç†æ ¹æ®çŠ¶æ€é‡‡å–çš„å†³ç­–ã€‚åœ¨æˆ‘ä»¬çš„æƒ…å†µä¸‹ï¼ŒåŠ¨ä½œå¯ä»¥åŒ…æ‹¬æ›´æ”¹æ¨¡å‹ã€ä¿®æ”¹ä¸Šä¸‹æ–‡ã€æ›´æ”¹æŸ¥è¯¢ç­‰ã€‚
# å¥–åŠ±ï¼ˆRewardï¼‰ï¼šä»£ç†åœ¨é‡‡å–è¡ŒåŠ¨åè·å¾—çš„åé¦ˆã€‚åœ¨æˆ‘ä»¬çš„æƒ…å†µä¸‹ï¼Œå¥–åŠ±å¯ä»¥æ˜¯ç”Ÿæˆçš„å“åº”ä¸çœŸå®ç­”æ¡ˆä¹‹é—´çš„ç›¸ä¼¼åº¦ã€‚æˆ‘ä»¬çš„çŠ¶æ€å°†éšç€è®­ç»ƒä¸æ–­å˜åŒ–ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦åœ¨æ¯æ¬¡è®­ç»ƒå›åˆåä¿å­˜çŠ¶æ€ï¼Œä»¥ä¾¿RLä»£ç†å¯ä»¥ä»ä¸­å­¦ä¹ ï¼Œå¹¶é¿å…é‡å¤çŠ¯é”™ã€‚

# å®šä¹‰å¼ºåŒ–å­¦ä¹ ä»£ç†çš„çŠ¶æ€è¡¨ç¤ºçš„å‡½æ•°
def define_state(
        query: str,
        context_chunks: List[str],
        rewritten_query: str = None,
        previous_responses: List[str] = None,
        previous_rewards: List[float] = None
) -> dict:
    """å®šä¹‰å¼ºåŒ–å­¦ä¹ ä»£ç†çš„çŠ¶æ€è¡¨ç¤ºã€‚

    Args:
        query (str): åŸå§‹ç”¨æˆ·æŸ¥è¯¢ã€‚
        context_chunks (List[str]): ä»çŸ¥è¯†åº“æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡å—ã€‚
        rewritten_query (str, optional): æŸ¥è¯¢çš„é‡æ–°è¡¨è¿°ç‰ˆæœ¬ã€‚
        previous_responses (List[str], optional): ä¹‹å‰ç”Ÿæˆçš„å“åº”åˆ—è¡¨ã€‚
        previous_rewards (List[float], optional): ä¹‹å‰è·å¾—çš„å¥–åŠ±åˆ—è¡¨ã€‚

    Returns:
        dict: åŒ…å«æ‰€æœ‰ç›¸å…³ä¿¡æ¯çš„å½“å‰çŠ¶æ€å­—å…¸ã€‚
    """
    state = {
        "original_query": query,  # ç”¨æˆ·çš„åˆå§‹æŸ¥è¯¢
        "current_query": rewritten_query if rewritten_query else query,  # å½“å‰æŸ¥è¯¢ç‰ˆæœ¬ï¼ˆå¯èƒ½å·²é‡æ–°è¡¨è¿°ï¼‰
        "context": context_chunks,  # ä»çŸ¥è¯†åº“æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡å—
        "previous_responses": previous_responses if previous_responses else [],  # ä¹‹å‰ç”Ÿæˆçš„å“åº”å†å²
        "previous_rewards": previous_rewards if previous_rewards else []  # ä¹‹å‰è·å¾—çš„å¥–åŠ±å†å²
    }
    return state


# æˆ‘ä»¬å·²ç»å®šä¹‰äº†RLä»£ç†çš„çŠ¶æ€è¡¨ç¤ºï¼ŒåŒ…æ‹¬ç”¨æˆ·æŸ¥è¯¢ã€æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡å—ã€é‡æ–°è¡¨è¿°çš„æŸ¥è¯¢ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰ã€ä»¥åŠä¹‹å‰ç”Ÿæˆçš„å“åº”å’Œå¥–åŠ±å†å²ã€‚è¿™ä¸ªçŠ¶æ€å°†æŒ‡å¯¼ä»£ç†ç”Ÿæˆæ›´å¥½çš„å“åº”ã€‚
# æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦å®šä¹‰å¼ºåŒ–å­¦ä¹ ä»£ç†çš„åŠ¨ä½œç©ºé—´ã€‚åŠ¨ä½œç©ºé—´ç”±ä»£ç†åœ¨æ¯ä¸€æ­¥å¯ä»¥é‡‡å–çš„ä¸€ç³»åˆ—å¯èƒ½åŠ¨ä½œç»„æˆã€‚åœ¨æœ¬ä¾‹ä¸­ï¼Œæˆ‘ä»¬å®šä¹‰äº†å››ä¸ªåŠ¨ä½œï¼š

# rewrite_queryï¼šé‡æ–°è¡¨è¿°åŸå§‹æŸ¥è¯¢ä»¥æ”¹è¿›æ£€ç´¢
# expand_contextï¼šæ£€ç´¢é¢å¤–çš„ä¸Šä¸‹æ–‡å—
# filter_contextï¼šç§»é™¤ä¸ç›¸å…³çš„ä¸Šä¸‹æ–‡å—
# generate_responseï¼šæ ¹æ®å½“å‰æŸ¥è¯¢å’Œä¸Šä¸‹æ–‡ç”Ÿæˆå“åº”

# å®šä¹‰å¼ºåŒ–å­¦ä¹ ä»£ç†çš„åŠ¨ä½œç©ºé—´çš„å‡½æ•°
def define_action_space() -> List[str]:
    """å®šä¹‰å¼ºåŒ–å­¦ä¹ ä»£ç†å¯ä»¥é‡‡å–çš„åŠ¨ä½œé›†åˆã€‚

    åŠ¨ä½œåŒ…æ‹¬ï¼š
    - rewrite_queryï¼šé‡æ–°è¡¨è¿°åŸå§‹æŸ¥è¯¢ä»¥æ”¹è¿›æ£€ç´¢
    - expand_contextï¼šæ£€ç´¢é¢å¤–çš„ä¸Šä¸‹æ–‡å—
    - filter_contextï¼šç§»é™¤ä¸ç›¸å…³çš„ä¸Šä¸‹æ–‡å—
    - generate_responseï¼šæ ¹æ®å½“å‰æŸ¥è¯¢å’Œä¸Šä¸‹æ–‡ç”Ÿæˆå“åº”

    Returns:
        List[str]: å¯ç”¨åŠ¨ä½œçš„åˆ—è¡¨ã€‚
    """
    # å®šä¹‰ä»£ç†å¯ä»¥é‡‡å–çš„åŠ¨ä½œé›†åˆ
    actions = ["rewrite_query", "expand_context", "filter_context", "generate_response"]
    return actions


# æ˜¾ç„¶ï¼Œå½“æˆ‘ä»¬çš„RLä»£ç†é‡‡å–è¡ŒåŠ¨æ—¶ï¼Œå®ƒæ˜¯åŸºäºå½“å‰çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´çš„ã€‚å®ƒå°†æ ¹æ®RAGç®¡é“ç”Ÿæˆçš„å“åº”è´¨é‡è·å¾—å¥–åŠ±ã€‚å¥–åŠ±å‡½æ•°åŸºäºç”Ÿæˆçš„å“åº”ä¸çœŸå®ç­”æ¡ˆä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦ã€‚

# æ ¹æ®å“åº”è´¨é‡è®¡ç®—å¥–åŠ±çš„å‡½æ•°
def calculate_reward(response: str, ground_truth: str) -> float:
    """é€šè¿‡æ¯”è¾ƒç”Ÿæˆçš„å“åº”ä¸çœŸå®ç­”æ¡ˆæ¥è®¡ç®—å¥–åŠ±å€¼ã€‚

    ä½¿ç”¨å“åº”å’ŒçœŸå®ç­”æ¡ˆåµŒå…¥ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦æ¥ç¡®å®šå“åº”ä¸é¢„æœŸç­”æ¡ˆçš„æ¥è¿‘ç¨‹åº¦ã€‚

    Args:
        response (str): RAGç®¡é“ç”Ÿæˆçš„å“åº”ã€‚
        ground_truth (str): é¢„æœŸçš„æ­£ç¡®ç­”æ¡ˆã€‚

    Returns:
        float: å¥–åŠ±å€¼ï¼ŒèŒƒå›´åœ¨-1åˆ°1ä¹‹é—´ï¼Œå€¼è¶Šé«˜è¡¨ç¤ºå“åº”è¶Šæ¥è¿‘çœŸå®ç­”æ¡ˆã€‚
    """
    # ä¸ºå“åº”å’ŒçœŸå®ç­”æ¡ˆç”ŸæˆåµŒå…¥
    response_embedding = generate_embeddings([response])[0]
    ground_truth_embedding = generate_embeddings([ground_truth])[0]

    # è®¡ç®—åµŒå…¥ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦ä½œä¸ºå¥–åŠ±
    similarity = cosine_similarity(response_embedding, ground_truth_embedding)
    return similarity


# æˆ‘ä»¬çš„ç›®æ ‡æ˜¯é€šè¿‡ç”Ÿæˆä¸çœŸå®ç­”æ¡ˆç›¸ä¼¼çš„å“åº”æ¥æœ€å¤§åŒ–å¥–åŠ±ã€‚å¥–åŠ±å€¼è¶Šé«˜ï¼Œè¡¨ç¤ºç”Ÿæˆçš„å“åº”è¶Šæ¥è¿‘é¢„æœŸç­”æ¡ˆã€‚

# ç­–ç•¥ç½‘ç»œ
# åœ¨ä¹‹å‰ï¼Œæˆ‘ä»¬å®šä¹‰äº†çŠ¶æ€ã€åŠ¨ä½œç©ºé—´å’Œå¥–åŠ±é€»è¾‘ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦åˆ›å»ºä¸€ä¸ªç­–ç•¥ç½‘ç»œï¼Œå®ƒå°†æ ¹æ®å½“å‰çŠ¶æ€é€‰æ‹©ä¸€ä¸ªåŠ¨ä½œã€‚

# ç­–ç•¥ç½‘ç»œæ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œå®ƒæ¥å—å½“å‰çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´ä½œä¸ºè¾“å…¥ï¼Œå¹¶æ ¹æ®çŠ¶æ€è¿”å›ä¸€ä¸ªåŠ¨ä½œã€‚

# ç­–ç•¥ç½‘ç»œå¯ä»¥ä½¿ç”¨ç®€å•çš„å¯å‘å¼æ–¹æ³•æ¥é€‰æ‹©åŠ¨ä½œã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ²¡æœ‰ä¹‹å‰çš„å“åº”ï¼Œç­–ç•¥ç½‘ç»œå¯ä»¥ä¼˜å…ˆé‡æ–°è¡¨è¿°æŸ¥è¯¢ã€‚å¦‚æœä¸Šä¸‹æ–‡å—è¿‡å¤šï¼Œç­–ç•¥ç½‘ç»œå¯ä»¥é€‰æ‹©è¿‡æ»¤ä¸Šä¸‹æ–‡ã€‚

# å®šä¹‰ç­–ç•¥ç½‘ç»œä»¥æ ¹æ®å½“å‰çŠ¶æ€é€‰æ‹©åŠ¨ä½œçš„å‡½æ•°
def policy_network(
        state: dict,
        action_space: List[str],
        epsilon: float = 0.2
) -> str:
    """å®šä¹‰ä¸€ä¸ªç­–ç•¥ç½‘ç»œï¼Œä½¿ç”¨epsilon-greedyç­–ç•¥æ ¹æ®å½“å‰çŠ¶æ€é€‰æ‹©åŠ¨ä½œã€‚

    Args:
        state (dict): åŒ…å«æŸ¥è¯¢ã€ä¸Šä¸‹æ–‡ã€å“åº”å’Œå¥–åŠ±çš„å½“å‰ç¯å¢ƒçŠ¶æ€ã€‚
        action_space (List[str]): ä»£ç†å¯ä»¥é‡‡å–çš„åŠ¨ä½œåˆ—è¡¨ã€‚
        epsilon (float): æ¢ç´¢æ¦‚ç‡ï¼Œç”¨äºå¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ã€‚é»˜è®¤ä¸º0.2ã€‚

    Returns:
        str: ä»åŠ¨ä½œç©ºé—´ä¸­é€‰æ‹©çš„åŠ¨ä½œã€‚
    """
    # ä½¿ç”¨epsilon-greedyç­–ç•¥ï¼šéšæœºæ¢ç´¢ä¸åˆ©ç”¨
    if np.random.random() < epsilon:
        # æ¢ç´¢ï¼šä»åŠ¨ä½œç©ºé—´ä¸­éšæœºé€‰æ‹©ä¸€ä¸ªåŠ¨ä½œ
        action = np.random.choice(action_space)
    else:
        # åˆ©ç”¨ï¼šæ ¹æ®å½“å‰çŠ¶æ€ä½¿ç”¨ç®€å•å¯å‘å¼é€‰æ‹©æœ€ä½³åŠ¨ä½œ
        # æ‰€ä»¥æˆ‘ä»¬çš„ç­–ç•¥ç½‘ç»œçš„å·¥ä½œåŸç†å¦‚ä¸‹ï¼š
        # å¦‚æœæ²¡æœ‰ä¹‹å‰çš„å“åº”ï¼Œä¼˜å…ˆé‡æ–°è¡¨è¿°æŸ¥è¯¢ã€‚
        # å¦‚æœæœ‰ä¹‹å‰çš„å“åº”ä½†å¥–åŠ±è¾ƒä½ï¼Œå°è¯•æ‰©å±•ä¸Šä¸‹æ–‡ã€‚
        # å¦‚æœä¸Šä¸‹æ–‡å—è¿‡å¤šï¼Œå°è¯•è¿‡æ»¤ä¸Šä¸‹æ–‡ã€‚
        # å¦åˆ™ï¼Œç”Ÿæˆå“åº”ã€‚
        if len(state["previous_responses"]) == 0:
            action = "rewrite_query"
        elif state["previous_rewards"] and max(state["previous_rewards"]) < 0.7:
            action = "expand_context"
        elif len(state["context"]) > 5:
            action = "filter_context"
        else:
            action = "generate_response"

    return action

# å•ä¸ªRLæ­¥éª¤
# æˆ‘ä»¬å·²ç»ç¼–ç äº†RLç®¡é“çš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚å¯¹äºä»»ä½•è¿›è¡Œè¿‡è®­ç»ƒçš„å¼€å‘äººå‘˜æ¥è¯´ï¼Œè®­ç»ƒå¾ªç¯ä¸­çš„æ¯ä¸€æ¬¡è¿­ä»£éƒ½æ˜¯ä¸€ä¸ªå•ä¸ªæ­¥éª¤ï¼Œå…¶ä¸­RLä»£ç†é‡‡å–è¡ŒåŠ¨ï¼Œè®¡ç®—å¥–åŠ±ï¼Œæ›´æ–°çŠ¶æ€ç­‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦ç¼–ç ä¸€ä¸ªå•ä¸ªæ­¥éª¤çš„è®­ç»ƒå¾ªç¯ã€‚è®©æˆ‘ä»¬è¿™æ ·åšã€‚

# æ‰§è¡Œå•ä¸ªRLæ­¥éª¤çš„å‡½æ•°
def rl_step(
        state: dict,
        action_space: List[str],
        ground_truth: str
) -> tuple[dict, str, float, str]:
    """æ‰§è¡Œå•ä¸ªRLæ­¥éª¤ï¼šé€‰æ‹©ä¸€ä¸ªåŠ¨ä½œï¼Œæ‰§è¡Œå®ƒï¼Œå¹¶è®¡ç®—å¥–åŠ±ã€‚

    Args:
        state (dict): åŒ…å«æŸ¥è¯¢ã€ä¸Šä¸‹æ–‡ã€å“åº”å’Œå¥–åŠ±çš„å½“å‰ç¯å¢ƒçŠ¶æ€ã€‚
        action_space (List[str]): ä»£ç†å¯ä»¥é‡‡å–çš„åŠ¨ä½œåˆ—è¡¨ã€‚
        ground_truth (str): é¢„æœŸçš„æ­£ç¡®ç­”æ¡ˆï¼Œç”¨äºè®¡ç®—å¥–åŠ±ã€‚

    Returns:
        tuple: åŒ…å«ä»¥ä¸‹å†…å®¹çš„å…ƒç»„ï¼š
        - state (dict): æ‰§è¡ŒåŠ¨ä½œåæ›´æ–°çš„çŠ¶æ€ã€‚
        - action (str): ç­–ç•¥ç½‘ç»œé€‰æ‹©çš„åŠ¨ä½œã€‚
        - reward (float): é‡‡å–è¡ŒåŠ¨åè·å¾—çš„å¥–åŠ±ã€‚
        - response (str): ç”Ÿæˆçš„å“åº”ï¼ˆå¦‚æœé€‚ç”¨ï¼‰ã€‚
    """
    # ä½¿ç”¨ç­–ç•¥ç½‘ç»œé€‰æ‹©ä¸€ä¸ªåŠ¨ä½œ
    action: str = policy_network(state, action_space)
    response: str = None  # åˆå§‹åŒ–å“åº”ä¸ºNone
    reward: float = 0  # åˆå§‹åŒ–å¥–åŠ±ä¸º0

    # æ‰§è¡Œé€‰æ‹©çš„åŠ¨ä½œ
    if action == "rewrite_query":
        # é‡æ–°è¡¨è¿°æŸ¥è¯¢ä»¥æ”¹è¿›æ£€ç´¢
        rewritten_query: str = rewrite_query(state["original_query"], state["context"])
        state["current_query"] = rewritten_query  # æ›´æ–°çŠ¶æ€ä¸­çš„å½“å‰æŸ¥è¯¢
        # æ ¹æ®é‡æ–°è¡¨è¿°çš„æŸ¥è¯¢æ£€ç´¢æ–°çš„ä¸Šä¸‹æ–‡
        new_context: List[str] = retrieve_relevant_chunks(rewritten_query)
        state["context"] = new_context  # æ›´æ–°çŠ¶æ€ä¸­çš„ä¸Šä¸‹æ–‡

    elif action == "expand_context":
        # æ‰©å±•ä¸Šä¸‹æ–‡ï¼Œæ£€ç´¢é¢å¤–çš„å—
        expanded_context: List[str] = expand_context(state["current_query"], state["context"])
        state["context"] = expanded_context  # æ›´æ–°çŠ¶æ€ä¸­çš„ä¸Šä¸‹æ–‡

    elif action == "filter_context":
        # è¿‡æ»¤ä¸Šä¸‹æ–‡ï¼Œä¿ç•™æœ€ç›¸å…³çš„å—
        filtered_context: List[str] = filter_context(state["current_query"], state["context"])
        state["context"] = filtered_context  # æ›´æ–°çŠ¶æ€ä¸­çš„ä¸Šä¸‹æ–‡

    elif action == "generate_response":
        # ä½¿ç”¨å½“å‰æŸ¥è¯¢å’Œä¸Šä¸‹æ–‡æ„å»ºæç¤º
        prompt: str = construct_prompt(state["current_query"], state["context"])
        # ç”ŸæˆLLMå“åº”
        response: str = generate_response(prompt)
        # æ ¹æ®å“åº”ä¸çœŸå®ç­”æ¡ˆçš„ç›¸ä¼¼åº¦è®¡ç®—å¥–åŠ±
        reward: float = calculate_reward(response, ground_truth)
        # æ›´æ–°çŠ¶æ€ä¸­çš„å“åº”å’Œå¥–åŠ±å†å²
        state["previous_responses"].append(response)
        state["previous_rewards"].append(reward)

    # è¿”å›æ›´æ–°åçš„çŠ¶æ€ã€é€‰æ‹©çš„åŠ¨ä½œã€å¥–åŠ±å’Œå“åº”
    return state, action, reward, response


# åœ¨æˆ‘ä»¬çš„å•ä¸ªæ­¥éª¤å‡½æ•°ä¸­ï¼Œé¦–å…ˆä½¿ç”¨ç­–ç•¥ç½‘ç»œé€‰æ‹©ä¸€ä¸ªåŠ¨ä½œã€‚ç­–ç•¥ç½‘ç»œä½¿ç”¨epsilon - greedyç­–ç•¥æ¥å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ã€‚å¦‚æœéšæœºæ•°å°äºepsilonï¼Œæˆ‘ä»¬ä»åŠ¨ä½œç©ºé—´ä¸­éšæœºé€‰æ‹©ä¸€ä¸ªåŠ¨ä½œè¿›è¡Œæ¢ç´¢ã€‚å¦åˆ™ï¼Œæˆ‘ä»¬æ ¹æ®å½“å‰çŠ¶æ€ä½¿ç”¨ç®€å•å¯å‘å¼é€‰æ‹©æœ€ä½³åŠ¨ä½œã€‚

# è®­ç»ƒå‚æ•°å’Œç­–ç•¥æ›´æ–°
# æˆ‘ä»¬éœ€è¦å®šä¹‰ä¸€äº›è®­ç»ƒå‚æ•°ï¼Œç”¨äºè®­ç»ƒå¾ªç¯ï¼ŒåŒæ—¶å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œæ ¹æ®è·å¾—çš„å¥–åŠ±æ›´æ–°ç­–ç•¥ã€‚

# è™½ç„¶è®­ç»ƒå‚æ•°å‡½æ•°æ˜¯å¯é€‰çš„ï¼Œä½†å®ƒå¯ä»¥ç”¨äºRLç®¡é“çš„é«˜çº§å®ç°ã€‚

# åˆå§‹åŒ–è®­ç»ƒå‚æ•°çš„å‡½æ•°
def initialize_training_params() -> Dict[str, Union[float, int]]:
    """åˆå§‹åŒ–è®­ç»ƒå‚æ•°ï¼Œå¦‚å­¦ä¹ ç‡ã€å›åˆæ•°å’ŒæŠ˜æ‰£å› å­ã€‚

    Returns:
        Dict[str, Union[float, int]]: åŒ…å«åˆå§‹åŒ–è®­ç»ƒå‚æ•°çš„å­—å…¸ã€‚
    """
    params = {
        "learning_rate": 0.01,  # ç­–ç•¥æ›´æ–°çš„å­¦ä¹ ç‡
        "num_episodes": 100,  # è®­ç»ƒå›åˆæ€»æ•°
        "discount_factor": 0.99  # æœªæ¥å¥–åŠ±çš„æŠ˜æ‰£å› å­
    }
    return params


# ä¸çŠ¶æ€åœ¨RLè¿‡ç¨‹ä¸­çš„å˜åŒ–ç±»ä¼¼ï¼Œç­–ç•¥ä¹Ÿéœ€è¦æ ¹æ®è·å¾—çš„å¥–åŠ±è¿›è¡Œæ›´æ–°ã€‚update_policyå‡½æ•°æ¥å—å½“å‰ç­–ç•¥ã€çŠ¶æ€ã€åŠ¨ä½œã€å¥–åŠ±å’Œå­¦ä¹ ç‡ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¿”å›æ›´æ–°åçš„ç­–ç•¥ã€‚

# æ ¹æ®å¥–åŠ±æ›´æ–°ç­–ç•¥çš„å‡½æ•°
def update_policy(
        policy: Dict[str, Dict[str, Union[float, str]]],
        state: Dict[str, object],
        action: str,
        reward: float,
        learning_rate: float
) -> Dict[str, Dict[str, Union[float, str]]]:
    """æ ¹æ®è·å¾—çš„å¥–åŠ±æ›´æ–°ç­–ç•¥ã€‚

    Args:
        policy (Dict[str, Dict[str, Union[float, str]]]): è¦æ›´æ–°çš„å½“å‰ç­–ç•¥ã€‚
        state (Dict[str, object]): å½“å‰ç¯å¢ƒçŠ¶æ€ã€‚
        action (str): ä»£ç†é‡‡å–çš„åŠ¨ä½œã€‚
        reward (float): é‡‡å–è¡ŒåŠ¨åè·å¾—çš„å¥–åŠ±ã€‚
        learning_rate (float): ç­–ç•¥æ›´æ–°çš„å­¦ä¹ ç‡ã€‚

    Returns:
        Dict[str, Dict[str, Union[float, str]]]: æ›´æ–°åçš„ç­–ç•¥ã€‚
    """
    # ç¤ºä¾‹ï¼šç®€å•çš„ç­–ç•¥æ›´æ–°ï¼ˆå°†è¢«æ›´é«˜çº§çš„RLç®—æ³•æ›¿æ¢ï¼‰
    policy[state["query"]] = {
        "action": action,  # å­˜å‚¨é‡‡å–çš„åŠ¨ä½œ
        "reward": reward  # å­˜å‚¨è·å¾—çš„å¥–åŠ±
    }
    return policy


# åœ¨ä¸Šè¿°update_policyé€»è¾‘ä¸­ï¼Œæˆ‘ä»¬åœ¨ç­–ç•¥å­—å…¸ä¸­å­˜å‚¨äº†æŸ¥è¯¢ã€é‡‡å–çš„åŠ¨ä½œå’Œè·å¾—çš„å¥–åŠ±ã€‚åœ¨æ›´é«˜çº§çš„RLç®—æ³•ä¸­ï¼Œç­–ç•¥æ›´æ–°å°†æ¶‰åŠæ›´å¤æ‚çš„æ–¹æ³•ï¼Œå¦‚ç­–ç•¥æ¢¯åº¦æˆ–Qå­¦ä¹ ã€‚

# è®­ç»ƒå¾ªç¯
# ç°åœ¨æˆ‘ä»¬å·²ç»ç¼–ç äº†è®­ç»ƒå¾ªç¯çš„æ¯ä¸€ä¸ªéƒ¨åˆ†ï¼Œå¯ä»¥å°†å®ƒä»¬ç»„åˆåˆ°ä¸€ä¸ªå‡½æ•°ä¸­ï¼Œå®ç°RLå¢å¼ºçš„RAGç³»ç»Ÿçš„è®­ç»ƒå¾ªç¯ã€‚

# å®ç°è®­ç»ƒå¾ªç¯çš„å‡½æ•°
# è¯¦ç»†æ¥è¯´ï¼Œtraining_loopå‡½æ•°æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š
#
# åˆå§‹åŒ–è®­ç»ƒå‚æ•°ï¼ˆå¦‚æœæœªæä¾›ï¼‰ã€‚
# è·å–ç®€å•RAGç®¡é“çš„åˆå§‹æ€§èƒ½ä»¥è¿›è¡Œæ¯”è¾ƒã€‚
# å¼€å§‹è®­ç»ƒå¾ªç¯ï¼Œæ‰§è¡ŒæŒ‡å®šæ•°é‡çš„å›åˆã€‚
# åœ¨æ¯ä¸ªå›åˆä¸­æ‰§è¡Œå•ä¸ªRLæ­¥éª¤ã€‚
# æ›´æ–°å¥–åŠ±å’ŒåŠ¨ä½œå†å²ã€‚
# æ¯5ä¸ªå›åˆæ‰“å°è¿›åº¦ã€‚
# æ¯”è¾ƒRLå¢å¼ºçš„RAGä¸ç®€å•RAGçš„å¥–åŠ±ã€‚
# è¿”å›æ›´æ–°åçš„ç­–ç•¥ã€å¥–åŠ±å†å²ã€åŠ¨ä½œå†å²ï¼Œä»¥åŠæœ€ä½³å“åº”ã€‚
# æ€§èƒ½æ¯”è¾ƒé€»è¾‘
def training_loop(
        query_text: str,
        ground_truth: str,
        params: Optional[Dict[str, Union[float, int]]] = None
) -> Tuple[Dict[str, Dict[str, Union[float, str]]], List[float], List[List[str]], Optional[str]]:
    """å®ç°RLå¢å¼ºçš„RAGè®­ç»ƒå¾ªç¯ã€‚

    Args:
        query_text (str): è¾“å…¥æŸ¥è¯¢æ–‡æœ¬ã€‚
        ground_truth (str): æŸ¥è¯¢çš„é¢„æœŸæ­£ç¡®ç­”æ¡ˆã€‚
        params (Optional[Dict[str, Union[float, int]]]): è®­ç»ƒå‚æ•°ï¼Œå¦‚å­¦ä¹ ç‡ã€å›åˆæ•°å’ŒæŠ˜æ‰£å› å­ã€‚å¦‚æœä¸ºNoneï¼Œåˆ™ä½¿ç”¨é»˜è®¤å‚æ•°ã€‚

    Returns:
        Tuple: åŒ…å«ä»¥ä¸‹å†…å®¹çš„å…ƒç»„ï¼š
        - policy (Dict[str, Dict[str, Union[float, str]]]): è®­ç»ƒåçš„ç­–ç•¥ã€‚
        - rewards_history (List[float]): æ¯ä¸ªå›åˆè·å¾—çš„å¥–åŠ±åˆ—è¡¨ã€‚
        - actions_history (List[List[str]]): æ¯ä¸ªå›åˆé‡‡å–çš„åŠ¨ä½œåˆ—è¡¨ã€‚
        - best_response (Optional[str]): è®­ç»ƒè¿‡ç¨‹ä¸­ç”Ÿæˆçš„æœ€ä½³å“åº”ã€‚
    """
    # å¦‚æœæœªæä¾›è®­ç»ƒå‚æ•°ï¼Œåˆ™åˆå§‹åŒ–é»˜è®¤å‚æ•°
    if params is None:
        params = initialize_training_params()

    # åˆå§‹åŒ–å˜é‡ä»¥è·Ÿè¸ªè¿›åº¦
    rewards_history: List[float] = []  # å­˜å‚¨æ¯ä¸ªå›åˆå¥–åŠ±çš„åˆ—è¡¨
    actions_history: List[List[str]] = []  # å­˜å‚¨æ¯ä¸ªå›åˆé‡‡å–çš„åŠ¨ä½œçš„åˆ—è¡¨
    policy: Dict[str, Dict[str, Union[float, str]]] = {}  # ç­–ç•¥å­—å…¸ï¼Œå­˜å‚¨åŠ¨ä½œå’Œå¥–åŠ±
    action_space: List[str] = define_action_space()  # å®šä¹‰åŠ¨ä½œç©ºé—´
    best_response: Optional[str] = None  # å­˜å‚¨æœ€ä½³å“åº”çš„å˜é‡
    best_reward: float = -1  # åˆå§‹åŒ–æœ€ä½³å¥–åŠ±ä¸ºä¸€ä¸ªå¾ˆä½çš„å€¼

    # è·å–ç®€å•RAGç®¡é“çš„åˆå§‹æ€§èƒ½ä»¥è¿›è¡Œæ¯”è¾ƒ
    simple_response: str = basic_rag_pipeline(query_text)
    simple_reward: float = calculate_reward(simple_response, ground_truth)
    print(f"ç®€å•RAGå¥–åŠ±: {simple_reward:.4f}")

    # å¼€å§‹è®­ç»ƒå¾ªç¯
    for episode in range(params["num_episodes"]):
        # ä½¿ç”¨ç›¸åŒçš„æŸ¥è¯¢é‡ç½®ç¯å¢ƒ
        context_chunks: List[str] = retrieve_relevant_chunks(query_text)
        state: Dict[str, object] = define_state(query_text, context_chunks)
        episode_reward: float = 0  # åˆå§‹åŒ–å½“å‰å›åˆçš„å¥–åŠ±
        episode_actions: List[str] = []  # åˆå§‹åŒ–å½“å‰å›åˆé‡‡å–çš„åŠ¨ä½œåˆ—è¡¨

        # æ¯ä¸ªå›åˆçš„æœ€å¤§æ­¥éª¤æ•°ï¼Œä»¥é˜²æ­¢æ— é™å¾ªç¯
        for step in range(10):
            # æ‰§è¡Œå•ä¸ªRLæ­¥éª¤
            state, action, reward, response = rl_step(state, action_space, ground_truth)
            episode_actions.append(action)  # è®°å½•é‡‡å–çš„åŠ¨ä½œ

            # å¦‚æœç”Ÿæˆäº†å“åº”ï¼Œç»“æŸå›åˆ
            if response:
                episode_reward = reward  # æ›´æ–°å›åˆå¥–åŠ±

                # è·Ÿè¸ªæœ€ä½³å“åº”å’Œå¥–åŠ±
                if reward > best_reward:
                    best_reward = reward
                    best_response = response

                break  # é€€å‡ºå¾ªç¯ï¼Œå› ä¸ºå›åˆç»“æŸ

        # æ›´æ–°å¥–åŠ±å’ŒåŠ¨ä½œå†å²
        rewards_history.append(episode_reward)
        actions_history.append(episode_actions)

        # æ¯5ä¸ªå›åˆæ‰“å°è¿›åº¦
        if episode % 5 == 0:
            print(f"å›åˆ {episode}: å¥–åŠ± = {episode_reward:.4f}, åŠ¨ä½œ = {episode_actions}")

    # æ¯”è¾ƒRLå¢å¼ºçš„RAGä¸ç®€å•RAGçš„å¥–åŠ±
    improvement: float = best_reward - simple_reward
    print(f"\nè®­ç»ƒå®Œæˆ:")
    print(f"ç®€å•RAGå¥–åŠ±: {simple_reward:.4f}")
    print(f"æœ€ä½³RLå¢å¼ºRAGå¥–åŠ±: {best_reward:.4f}")
    print(f"æ”¹è¿›: {improvement:.4f} ({improvement * 100:.2f}%)")

    return policy, rewards_history, actions_history, best_response


# æ­¤å‡½æ•°å°†è¾“å…¥æŸ¥è¯¢æ–‡æœ¬ã€é¢„æœŸçœŸå®ç­”æ¡ˆä»¥åŠå¯é€‰çš„è®­ç»ƒå‚æ•°ä½œä¸ºè¾“å…¥ã€‚å®ƒè¿”å›æ›´æ–°åçš„ç­–ç•¥ã€æ¯ä¸ªå›åˆè·å¾—çš„å¥–åŠ±åˆ—è¡¨ã€æ¯ä¸ªå›åˆé‡‡å–çš„åŠ¨ä½œåˆ—è¡¨ï¼Œä»¥åŠè®­ç»ƒè¿‡ç¨‹ä¸­ç”Ÿæˆçš„æœ€ä½³å“åº”ã€‚

# è™½ç„¶æˆ‘ä»¬å¯ä»¥æ‰‹åŠ¨æ¯”è¾ƒç®€å•RAGç®¡é“ä¸RLå¢å¼ºçš„RAGç®¡é“ï¼Œä½†ä¸€ä¸ªå‡½æ•°è‚¯å®šèƒ½å¸®åŠ©æˆ‘ä»¬å®Œæˆæ­¤ä»»åŠ¡ã€‚å› æ­¤ï¼Œè®©æˆ‘ä»¬å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œæ¯”è¾ƒç®€å•RAGä¸RLå¢å¼ºçš„RAGçš„æ€§èƒ½ã€‚

# æ¯”è¾ƒç®€å•RAGä¸RLå¢å¼ºRAGçš„å‡½æ•°
def compare_rag_approaches(query_text: str, ground_truth: str) -> Tuple[str, str, float, float]:
    """æ¯”è¾ƒç®€å•RAGä¸RLå¢å¼ºRAGçš„è¾“å‡ºã€‚

    Args:
        query_text (str): è¾“å…¥æŸ¥è¯¢æ–‡æœ¬ã€‚
        ground_truth (str): æŸ¥è¯¢çš„é¢„æœŸæ­£ç¡®ç­”æ¡ˆã€‚

    Returns:
        Tuple[str, str, float, float]: åŒ…å«ä»¥ä¸‹å†…å®¹çš„å…ƒç»„ï¼š
        - simple_response (str): ç®€å•RAGç®¡é“ç”Ÿæˆçš„å“åº”ã€‚
        - best_rl_response (str): RLå¢å¼ºRAGç®¡é“ç”Ÿæˆçš„æœ€ä½³å“åº”ã€‚
        - simple_similarity (float): ç®€å•RAGå“åº”ä¸çœŸå®ç­”æ¡ˆçš„ç›¸ä¼¼åº¦ã€‚
        - rl_similarity (float): RLå¢å¼ºRAGå“åº”ä¸çœŸå®ç­”æ¡ˆçš„ç›¸ä¼¼åº¦ã€‚
    """
    print("=" * 80)
    print(f"æŸ¥è¯¢: {query_text}")
    print("=" * 80)

    # æ­¥éª¤1ï¼šä½¿ç”¨ç®€å•RAGç®¡é“ç”Ÿæˆå“åº”
    simple_response: str = basic_rag_pipeline(query_text)
    # è®¡ç®—ç®€å•RAGå“åº”ä¸çœŸå®ç­”æ¡ˆçš„ç›¸ä¼¼åº¦
    simple_similarity: float = calculate_reward(simple_response, ground_truth)

    print("\nç®€å•RAGè¾“å‡º:")
    print("-" * 40)
    print(simple_response)
    print(f"ä¸çœŸå®ç­”æ¡ˆçš„ç›¸ä¼¼åº¦: {simple_similarity:.4f}")

    # æ­¥éª¤2ï¼šè®­ç»ƒRLå¢å¼ºçš„RAGæ¨¡å‹
    print("\nè®­ç»ƒRLå¢å¼ºçš„RAGæ¨¡å‹...")
    # åˆå§‹åŒ–è®­ç»ƒå‚æ•°ï¼ˆå¦‚å­¦ä¹ ç‡ã€å›åˆæ•°ã€æŠ˜æ‰£å› å­ï¼‰
    params: Dict[str, float | int] = initialize_training_params()
    # å°†å›åˆæ•°è®¾ç½®ä¸ºè¾ƒå°çš„å€¼ä»¥è¿›è¡Œæ¼”ç¤º
    params["num_episodes"] = 5

    # è¿è¡Œè®­ç»ƒå¾ªç¯ä»¥è®­ç»ƒRLå¢å¼ºçš„RAGæ¨¡å‹
    _, rewards_history, actions_history, best_rl_response = training_loop(
        query_text, ground_truth, params
    )

    # å¦‚æœè®­ç»ƒè¿‡ç¨‹ä¸­æ²¡æœ‰ç”Ÿæˆå“åº”ï¼Œä½¿ç”¨å½“å‰æŸ¥è¯¢å’Œä¸Šä¸‹æ–‡ç”Ÿæˆä¸€ä¸ªå“åº”
    if best_rl_response is None:
        # æ£€ç´¢ä¸æŸ¥è¯¢ç›¸å…³çš„å—
        context_chunks: List[str] = retrieve_relevant_chunks(query_text)
        # ä½¿ç”¨æŸ¥è¯¢å’Œæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡æ„å»ºæç¤º
        prompt: str = construct_prompt(query_text, context_chunks)
        # ç”ŸæˆLLMå“åº”
        best_rl_response: str = generate_response(prompt)

    # è®¡ç®—RLå¢å¼ºRAGå“åº”ä¸çœŸå®ç­”æ¡ˆçš„ç›¸ä¼¼åº¦
    rl_similarity: float = calculate_reward(best_rl_response, ground_truth)

    print("\nRLå¢å¼ºRAGè¾“å‡º:")
    print("-" * 40)
    print(best_rl_response)
    print(f"ä¸çœŸå®ç­”æ¡ˆçš„ç›¸ä¼¼åº¦: {rl_similarity:.4f}")

    # æ­¥éª¤3ï¼šè¯„ä¼°å’Œæ¯”è¾ƒç»“æœ
    # è®¡ç®—RLå¢å¼ºRAGæ¨¡å‹ç›¸å¯¹äºç®€å•RAGæ¨¡å‹çš„æ”¹è¿›
    improvement: float = rl_similarity - simple_similarity

    print("\nè¯„ä¼°ç»“æœ:")
    print("-" * 40)
    print(f"ç®€å•RAGä¸çœŸå®ç­”æ¡ˆçš„ç›¸ä¼¼åº¦: {simple_similarity:.4f}")
    print(f"RLå¢å¼ºRAGä¸çœŸå®ç­”æ¡ˆçš„ç›¸ä¼¼åº¦: {rl_similarity:.4f}")
    print(f"æ”¹è¿›: {improvement * 100:.2f}%")

    # æ­¥éª¤4ï¼šç»˜åˆ¶å¥–åŠ±å†å²ï¼ˆå¦‚æœæœ‰è¶³å¤Ÿçš„å›åˆæ•°ä¸”matplotlibå¯ç”¨ï¼‰
    if len(rewards_history) > 1:
        try:
            import matplotlib.pyplot as plt
            # ç»˜åˆ¶RLè®­ç»ƒæœŸé—´çš„å¥–åŠ±å†å²å›¾
            plt.figure(figsize=(10, 6))
            plt.plot(rewards_history)
            plt.title('RLè®­ç»ƒæœŸé—´çš„å¥–åŠ±å†å²')
            plt.xlabel('å›åˆ')
            plt.ylabel('å¥–åŠ±')
            plt.grid(True)
            plt.show()
        except ImportError:
            # å¦‚æœæ²¡æœ‰matplotlibï¼Œæ‰“å°æ¶ˆæ¯è€Œä¸æ˜¯ç»˜å›¾
            print("matplotlibä¸å¯ç”¨ï¼Œæ— æ³•ç»˜åˆ¶å¥–åŠ±å›¾")

    # è¿”å›ç»“æœï¼šä¸¤ç§æ–¹æ³•çš„å“åº”åŠå…¶ç›¸ä¼¼åº¦
    return simple_response, best_rl_response, simple_similarity, rl_similarity


# æ‰€ä»¥æˆ‘ä»¬çš„æ€§èƒ½æ¯”è¾ƒé€»è¾‘å¹¶ä¸å¤æ‚ï¼Œè€Œæ˜¯åŸºäºä»¥ä¸‹4ä¸ªæ­¥éª¤ï¼š
#
# ä½¿ç”¨ç®€å•RAGç®¡é“ç”Ÿæˆå“åº”ã€‚
# ä½¿ç”¨è®­ç»ƒå¾ªç¯è®­ç»ƒRLå¢å¼ºçš„RAGæ¨¡å‹ã€‚
# è¯„ä¼°å¹¶æ¯”è¾ƒç»“æœã€‚
# ç»˜åˆ¶å¥–åŠ±å†å²ï¼ˆå¦‚æœå¯ç”¨ï¼‰ã€‚
# è¯„ä¼°æ¡†æ¶ï¼ˆå¯é€‰ï¼‰
# æ­¤æ­¥éª¤æ˜¯å¯é€‰çš„ï¼Œä½†å¦‚æœä½ æƒ³åœ¨éªŒè¯æ•°æ®ä¸Šè¯„ä¼°æ‰€æœ‰è¯„ä¼°æŸ¥è¯¢ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä»£ç ã€‚
#
# é¦–å…ˆï¼Œä¸ºäº†è¯„ä¼°æ£€ç´¢åˆ°çš„å—ä¸çœŸå®ç­”æ¡ˆçš„ç›¸å…³æ€§ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªå‡½æ•°æ¥è¯„ä¼°æ£€ç´¢åˆ°çš„å—çš„ç›¸å…³æ€§ã€‚

# è¯„ä¼°æ£€ç´¢åˆ°çš„å—çš„ç›¸å…³æ€§çš„å‡½æ•°
def evaluate_relevance(retrieved_chunks: List[str], ground_truth_chunks: List[str]) -> float:
    """é€šè¿‡æ¯”è¾ƒæ£€ç´¢åˆ°çš„å—ä¸çœŸå®å—æ¥è¯„ä¼°ç›¸å…³æ€§ã€‚

    Args:
        retrieved_chunks (List[str]): æ£€ç´¢ç³»ç»Ÿè¿”å›çš„æ–‡æœ¬å—åˆ—è¡¨ã€‚
        ground_truth_chunks (List[str]): çœŸå®æ–‡æœ¬å—åˆ—è¡¨ï¼Œç”¨äºæ¯”è¾ƒã€‚

    Returns:
        float: æ£€ç´¢åˆ°çš„å—ä¸çœŸå®å—ä¹‹é—´çš„å¹³å‡ç›¸å…³æ€§åˆ†æ•°ã€‚
    """
    relevance_scores: List[float] = []  # åˆå§‹åŒ–ä¸€ä¸ªåˆ—è¡¨å­˜å‚¨ç›¸å…³æ€§åˆ†æ•°

    # éå†æ£€ç´¢å—ä¸çœŸå®å—çš„é…å¯¹
    for retrieved, ground_truth in zip(retrieved_chunks, ground_truth_chunks):
        # è®¡ç®—æ£€ç´¢å—ä¸çœŸå®å—åµŒå…¥ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦
        relevance: float = cosine_similarity(
            generate_embeddings([retrieved])[0],
            generate_embeddings([ground_truth])[0]
        )
        # å°†ç›¸å…³æ€§åˆ†æ•°æ·»åŠ åˆ°åˆ—è¡¨ä¸­
        relevance_scores.append(relevance)

    # è¿”å›ç›¸å…³æ€§åˆ†æ•°çš„å¹³å‡å€¼
    return np.mean(relevance_scores)


# ä¸ºäº†è¯„ä¼°ç”Ÿæˆå“åº”çš„å‡†ç¡®æ€§ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å“åº”ä¸çœŸå®ç­”æ¡ˆä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œæ ¹æ®è¿™ç§ç›¸ä¼¼åº¦æŒ‡æ ‡è¯„ä¼°å“åº”çš„å‡†ç¡®æ€§ã€‚

# è¯„ä¼°ç”Ÿæˆå“åº”å‡†ç¡®æ€§çš„å‡½æ•°
def evaluate_accuracy(responses: List[str], ground_truth_responses: List[str]) -> float:
    """é€šè¿‡æ¯”è¾ƒç”Ÿæˆçš„å“åº”ä¸çœŸå®å“åº”æ¥è¯„ä¼°å‡†ç¡®æ€§ã€‚

    Args:
        responses (List[str]): è¦è¯„ä¼°çš„ç”Ÿæˆå“åº”åˆ—è¡¨ã€‚
        ground_truth_responses (List[str]): ç”¨äºæ¯”è¾ƒçš„çœŸå®å“åº”åˆ—è¡¨ã€‚

    Returns:
        float: å“åº”å‡†ç¡®æ€§çš„å¹³å‡åˆ†æ•°ï¼Œè®¡ç®—ä¸ºå“åº”åµŒå…¥ä¸çœŸå®å“åº”åµŒå…¥ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦çš„å¹³å‡å€¼ã€‚
    """
    accuracy_scores: List[float] = []  # åˆå§‹åŒ–ä¸€ä¸ªåˆ—è¡¨å­˜å‚¨å‡†ç¡®æ€§åˆ†æ•°

    # éå†æ¯ä¸ªç”Ÿæˆå“åº”ä¸çœŸå®å“åº”çš„é…å¯¹
    for response, ground_truth in zip(responses, ground_truth_responses):
        # è®¡ç®—å“åº”ä¸çœŸå®å“åº”åµŒå…¥ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦
        accuracy: float = cosine_similarity(
            generate_embeddings([response])[0],
            generate_embeddings([ground_truth])[0]
        )
        # å°†å‡†ç¡®æ€§åˆ†æ•°æ·»åŠ åˆ°åˆ—è¡¨ä¸­
        accuracy_scores.append(accuracy)

    # è¿”å›å‡†ç¡®æ€§åˆ†æ•°çš„å¹³å‡å€¼
    return np.mean(accuracy_scores)


# æˆ‘ä»¬è¿˜éœ€è¦è¡¡é‡å“åº”è´¨é‡ï¼Œå¹¶ä¸ºæ­¤åˆ†é…ä¸€ä¸ªç›¸å…³åˆ†æ•°ï¼Œç”¨äºå¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ã€‚

# è¯„ä¼°å“åº”è´¨é‡çš„å‡½æ•°
def evaluate_response_quality(responses: List[str]) -> float:
    """ä½¿ç”¨å¯å‘å¼æ–¹æ³•æˆ–å¤–éƒ¨æ¨¡å‹è¯„ä¼°å“åº”è´¨é‡ã€‚

    Args:
        responses (List[str]): è¦è¯„ä¼°çš„ç”Ÿæˆå“åº”åˆ—è¡¨ã€‚

    Returns:
        float: å“åº”è´¨é‡çš„å¹³å‡åˆ†æ•°ï¼ŒèŒƒå›´åœ¨0åˆ°1ä¹‹é—´ã€‚
    """
    quality_scores: List[float] = []  # åˆå§‹åŒ–ä¸€ä¸ªåˆ—è¡¨å­˜å‚¨æ¯ä¸ªå“åº”çš„è´¨é‡åˆ†æ•°

    for response in responses:
        # ç¤ºä¾‹å¯å‘å¼ï¼šæ ¹æ®å“åº”é•¿åº¦è®¡ç®—è´¨é‡åˆ†æ•°
        # å°†é•¿åº¦æ ‡å‡†åŒ–ä¸ºæœ€å¤š100ä¸ªå•è¯ï¼Œå¹¶å°†åˆ†æ•°é™åˆ¶åœ¨1.0ä»¥ä¸‹
        quality: float = len(response.split()) / 100
        quality_scores.append(min(quality, 1.0))  # å°†å—é™çš„è´¨é‡åˆ†æ•°æ·»åŠ åˆ°åˆ—è¡¨ä¸­

    # è¿”å›æ‰€æœ‰å“åº”è´¨é‡åˆ†æ•°çš„å¹³å‡å€¼
    return np.mean(quality_scores)


# ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥åœ¨éªŒè¯æ•°æ®é›†ä¸Šè¯„ä¼°RLå¢å¼ºçš„RAGæ¨¡å‹çš„æ€§èƒ½ï¼š
# è¯„ä¼°RAGæ€§èƒ½çš„å‡½æ•°

def evaluate_rag_performance(
    queries: List[str],
    ground_truth_chunks: List[str],
    ground_truth_responses: List[str]
    ) -> Dict[str, float]:
    """ä½¿ç”¨ç›¸å…³æ€§ã€å‡†ç¡®æ€§å’Œå“åº”è´¨é‡æŒ‡æ ‡è¯„ä¼°RAGç®¡é“çš„æ€§èƒ½ã€‚


    å‚æ•°:
        queries (List[str]): è¦è¯„ä¼°çš„æŸ¥è¯¢å­—ç¬¦ä¸²åˆ—è¡¨ã€‚
        ground_truth_chunks (List[str]): ä¸æŸ¥è¯¢å¯¹åº”çš„çœŸå®æ–‡æœ¬å—åˆ—è¡¨ã€‚
        ground_truth_responses (List[str]): ä¸æŸ¥è¯¢å¯¹åº”çš„çœŸå®å“åº”åˆ—è¡¨ã€‚

    è¿”å›:
        Dict[str, float]: åŒ…å«å¹³å‡ç›¸å…³æ€§ã€å‡†ç¡®æ€§å’Œè´¨é‡åˆ†æ•°çš„å­—å…¸ã€‚
    """
    # åˆå§‹åŒ–åˆ—è¡¨å­˜å‚¨æ¯ä¸ªæŒ‡æ ‡çš„åˆ†æ•°
    relevance_scores: List[float] = []
    accuracy_scores: List[float] = []
    quality_scores: List[float] = []

    # éå†æ¯ä¸ªæŸ¥è¯¢åŠå…¶å¯¹åº”çš„çœŸå®æ•°æ®
    for query, ground_truth_chunk, ground_truth_response in zip(queries, ground_truth_chunks, ground_truth_responses):
        # æ£€ç´¢ä¸æŸ¥è¯¢ç›¸å…³çš„å—
        retrieved_chunks: List[str] = retrieve_relevant_chunks(query)

        # è¯„ä¼°æ£€ç´¢åˆ°çš„å—ä¸çœŸå®å—çš„ç›¸å…³æ€§
        relevance: float = evaluate_relevance(retrieved_chunks, [ground_truth_chunk])
        relevance_scores.append(relevance)

        # ä½¿ç”¨åŸºæœ¬çš„RAGç®¡é“ç”Ÿæˆå“åº”
        response: str = basic_rag_pipeline(query)

        # è¯„ä¼°ç”Ÿæˆçš„å“åº”ä¸çœŸå®å“åº”çš„å‡†ç¡®æ€§
        accuracy: float = evaluate_accuracy([response], [ground_truth_response])
        accuracy_scores.append(accuracy)

        # è¯„ä¼°ç”Ÿæˆçš„å“åº”çš„è´¨é‡
        quality: float = evaluate_response_quality([response])
        quality_scores.append(quality)

    # è®¡ç®—æ¯ä¸ªæŒ‡æ ‡çš„å¹³å‡åˆ†æ•°
    avg_relevance: float = np.mean(relevance_scores)
    avg_accuracy: float = np.mean(accuracy_scores)
    avg_quality: float = np.mean(quality_scores)

    # ä»¥å­—å…¸å½¢å¼è¿”å›å¹³å‡åˆ†æ•°
    return {
        "average_relevance": avg_relevance,
        "average_accuracy": avg_accuracy,
        "average_quality": avg_quality
    }

# æ‰“å°ä¸€æ¡æ¶ˆæ¯ï¼Œè¡¨ç¤ºRAGç®¡é“çš„å¼€å§‹
print("ğŸ” è¿è¡Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç®¡é“...")
print(f"ğŸ“¥ æŸ¥è¯¢: {sample_query}\n")

# è¿è¡ŒRAGç®¡é“å¹¶è·å–å“åº”
response = basic_rag_pipeline(sample_query)

# ä»¥æ›´å¥½çš„æ ¼å¼æ‰“å°å“åº”
print("ğŸ¤– AIå“åº”:")
print("-" * 50)
print(response.strip())
print("-" * 50)

# æ‰“å°çœŸå®ç­”æ¡ˆä»¥ä¾›æ¯”è¾ƒ
print("âœ… çœŸå®ç­”æ¡ˆ:")
print("-" * 50)
print(expected_answer)
print("-" * 50)

# ä¿å­˜ç»“æœä»¥ä¾¿åç»­æ¯”è¾ƒ
results = {
    "query": query_text, # è¾“å…¥çš„æŸ¥è¯¢æ–‡æœ¬
    "ground_truth": expected_answer, # æŸ¥è¯¢çš„é¢„æœŸæ­£ç¡®ç­”æ¡ˆ
    "simple_rag": {
        "response": simple_response, # ç”±ç®€å•RAGç®¡é“ç”Ÿæˆçš„å“åº”
        "similarity": float(simple_sim) # ç®€å•RAGå“åº”ä¸çœŸå®ç­”æ¡ˆçš„ç›¸ä¼¼åº¦åˆ†æ•°
        },
    "rl_rag": {
        "response": rl_response, # ç”±RLå¢å¼ºçš„RAGç®¡é“ç”Ÿæˆçš„å“åº”
        "similarity": float(rl_sim) # RLå¢å¼ºçš„RAGå“åº”ä¸çœŸå®ç­”æ¡ˆçš„ç›¸ä¼¼åº¦åˆ†æ•°
        },
    "improvement": float(rl_sim - simple_sim) # RLå¢å¼ºçš„RAGç›¸æ¯”ç®€å•RAGåœ¨ç›¸ä¼¼åº¦åˆ†æ•°ä¸Šçš„æå‡
}

# å°†ç»“æœä¿å­˜åˆ°JSONæ–‡ä»¶ä¸­ï¼Œä»¥ä¾¿å°†æ¥å‚è€ƒ
with open('rl_rag_results.json', 'w') as f:
    json.dump(results, f, indent=2) # å°†ç»“æœå­—å…¸å†™å…¥æ–‡ä»¶ï¼Œå¹¶ä½¿ç”¨ç¼©è¿›ä»¥æé«˜å¯è¯»æ€§

# æ‰“å°ä¸€æ¡ç¡®è®¤æ¶ˆæ¯ï¼Œè¡¨ç¤ºç»“æœå·²ä¿å­˜
print("\nç»“æœå·²ä¿å­˜åˆ°rl_rag_results.json")


# èµ„æ–™æ¥æºï¼šhttps://github.com/FareedKhan-dev/all-rag-techniques.git


