

Name:LDC2013T21.tgz： https://wakespace.lib.wfu.edu/bitstream/handle/10339/39379/LDC2013T21.tgz?sequence=1

toutiao_data.csv: 
    今日头条中文新闻（文本）分类数据集 
    每行为一条数据，以!分割的个字段，从前往后分别是 新闻ID，分类code（见下文），分类名称（见下文），新闻字符串（仅含标题），新闻关键词。
    来源: https://github.com/mattzheng/LangueOne/toutiao_data.rar; https://github.com/fateleak/toutiao-text-classfication-dataset.git

GoogleNews-vectors-negative300.bin.gz
gunzip -c GoogleNews-vectors-negative300.bin.gz > GoogleNews-vectors-negative300.bin
    https://github.com/3Top/word2vec-api
    https://pan.baidu.com/s/1kTCQqft

/home/gswyhq/data/kaggle_dogs-vs-cats
猫狗分类数据集不包含在 Keras 中。它由 Kaggle 在 2013 年末公开并作为一项计算视觉竞赛的一部分,当时卷积神经网络还不是主流算法。
# 你可以从 https://www.kaggle.com/c/dogs-vs-cats/data 下载原始数据集
#  数据集 链接：https://pan.baidu.com/s/13hw4LK8ihR6-6-8mpjLKDA 密码：dmp4

icwb2-data.rar
    http://sighan.cs.uchicago.edu/bakeoff2005/data/icwb2-data.rar

SogouC.reduced.zip
    https://pan.baidu.com/s/1pLrORJt

蛋白质功能预测：
https://github.com/tbepler/protein-sequence-embedding-iclr2019
- [SCOPe data](http://bergerlab-downloads.csail.mit.edu/bepler-protein-sequence-embeddings-from-structure-iclr2019/scope.tar.gz)
- [Pfam data](http://bergerlab-downloads.csail.mit.edu/bepler-protein-sequence-embeddings-from-structure-iclr2019/pfam.tar.gz)
- [Protein secondary structure data](http://bergerlab-downloads.csail.mit.edu/bepler-protein-sequence-embeddings-from-structure-iclr2019/secstr.tar.gz)
- [Transmembrane data](http://bergerlab-downloads.csail.mit.edu/bepler-protein-sequence-embeddings-from-structure-iclr2019/transmembrane.tar.gz)
- [CASP12 contact map data](http://bergerlab-downloads.csail.mit.edu/bepler-protein-sequence-embeddings-from-structure-iclr2019/casp12.tar.gz)

词性标注@人民日报199801.txt
https://pan.baidu.com/s/1gd6mslt

中华古诗词数据库
最全中华古诗词数据集，唐宋两朝近一万四千古诗人, 接近5.5万首唐诗加26万宋诗. 两宋时期1564位词人，21050首词。
https://github.com/chinese-poetry/chinese-poetry

汉语拆字字典
https://github.com/kfcd/chaizi

中华新华字典数据库。包括歇后语，成语，词语，汉字。
https://github.com/pwxcoo/chinese-xinhua

中文实体情感知识库
刻画人们如何描述某个实体，包含新闻、旅游、餐饮，共计30万对。
https://github.com/rainarch/SentiBridge

# 《TensorFlow+Keras深度学习人工智能实践应用》随书示例
https://pan.baidu.com/s/1c2rXnH2#list/path=%2FMP21710_example.zip
MP21710_example.zip

分词数据集 people-2014.7z：
数据集: https://pan.baidu.com/s/1EtXdhPR0lGF8c7tT8epn6Q 验证码: yj9j

http://183.61.19.162/zdoc/%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BA(%E7%AC%AC3%E7%89%88).pdf
http://183.61.19.162/zdoc/算法导论(第3版).pdf

七律百科知识图谱
7lore.zip
http://openkg.cn/dataset/7lore
http://openkg1.oss-cn-beijing.aliyuncs.com/30ee798a-3199-4de4-9792-7746bc8891e0/7lore.zip
7Lore_triple.csv

中文百科知识图谱Zhishi.me-提供Dump
zhishimejson.tar.gz
~$ tar -zxvf zhishimejson.tar.gz
zhishime_json/
zhishime_json/sameAs/
zhishime_json/sameAs/2.9_baidubaike_zhwiki_links_zh.zip
zhishime_json/sameAs/2.9_hudongbaike_zhwiki_links_zh.zip
zhishime_json/sameAs/2.9_baidubaike_hudongbaike_links_zh.zip
zhishime_json/sameAs/2.9_zhwiki_baidubaike_links_zh.zip
zhishime_json/sameAs/2.9_zhwiki_hudongbaike_links_zh.zip
zhishime_json/sameAs/2.9_hudongbaike_baidubaike_links_zh.zip
zhishime_json/3.0_zhishi_ontology_zh.zip
zhishime_json/baidubaike/
zhishime_json/baidubaike/3.0_baidubaike_redirects_zh.zip
zhishime_json/baidubaike/3.0_baidubaike_abstracts_zh.zip
zhishime_json/baidubaike/3.0_baidubaike_external_links_zh.zip
zhishime_json/baidubaike/3.0_baidubaike_images_zh.zip
zhishime_json/baidubaike/3.0_baidubaike_infobox_property_definitions_zh.zip
zhishime_json/baidubaike/3.0_baidubaike_internal_links_zh.zip
zhishime_json/baidubaike/3.0_baidubaike_related_pages_zh.zip
zhishime_json/baidubaike/3.0_baidubaike_labels_zh.zip
zhishime_json/baidubaike/3.0_baidubaike_article_categories_zh.zip
zhishime_json/baidubaike/3.0_baidubaike_image_information_zh.zip
zhishime_json/baidubaike/baidubaike_instance_types_zh.zip
zhishime_json/baidubaike/3.0_baidubaike_disambiguations_zh.zip
zhishime_json/baidubaike/3.0_baidubaike_infobox_properties_zh.zip
zhishime_json/baidubaike/3.0_baidubaike_article_links_zh.zip
zhishime_json/baidubaike/3.0_baidubaike_category_labels_zh.zip
zhishime_json/zhwiki/
zhishime_json/zhwiki/2.0_zhwiki_external_links_zh.zip
zhishime_json/zhwiki/2.0_zhwiki_article_links_zh.zip
zhishime_json/zhwiki/2.0_zhwiki_resource_ids_zh.zip
zhishime_json/zhwiki/2.0_zhwiki_revisions_zh.zip
zhishime_json/zhwiki/2.0_zhwiki_abstracts_zh.zip
zhishime_json/zhwiki/2.0_zhwiki_dbpedia_links_zh.zip
zhishime_json/zhwiki/2.0_zhwiki_skos_categories_zh.zip
zhishime_json/zhwiki/2.0_zhwiki_internal_links_zh.zip
zhishime_json/zhwiki/2.0_zhwiki_images_zh.zip
zhishime_json/zhwiki/2.0_zhwiki_aliases_zh.zip
zhishime_json/zhwiki/2.0_zhwiki_infobox_property_definitions_zh.zip
zhishime_json/zhwiki/2.0_zhwiki_labels_zh.zip
zhishime_json/zhwiki/2.0_zhwiki_category_labels_zh.zip
zhishime_json/zhwiki/zhwiki_instance_types_zh.zip
zhishime_json/zhwiki/2.0_zhwiki_redirects_zh.zip
zhishime_json/zhwiki/2.0_zhwiki_disambiguations_zh.zip
zhishime_json/zhwiki/2.0_zhwiki_infobox_properties_zh.zip
zhishime_json/zhwiki/2.0_zhwiki_article_categories_zh.zip
zhishime_json/hudongbaike/
zhishime_json/hudongbaike/3.0_hudongbaike_abstracts_zh.zip
zhishime_json/hudongbaike/3.0_hudongbaike_article_links_zh.zip
zhishime_json/hudongbaike/3.0_hudongbaike_category_labels_zh.zip
zhishime_json/hudongbaike/3.0_hudongbaike_infobox_properties_zh.zip
zhishime_json/hudongbaike/3.0_hudongbaike_external_links_zh.zip
zhishime_json/hudongbaike/3.0_hudongbaike_image_information_zh.zip
zhishime_json/hudongbaike/hudongbaike_instance_types_zh.zip
zhishime_json/hudongbaike/3.0_hudongbaike_internal_links_zh.zip
zhishime_json/hudongbaike/3.0_hudongbaike_disambiguations_zh.zip
zhishime_json/hudongbaike/3.0_hudongbaike_labels_zh.zip
zhishime_json/hudongbaike/3.0_hudongbaike_images_zh.zip
zhishime_json/hudongbaike/3.0_hudongbaike_related_pages_zh.zip
zhishime_json/hudongbaike/3.0_hudongbaike_redirects_zh.zip
zhishime_json/hudongbaike/3.0_hudongbaike_article_categories_zh.zip
zhishime_json/hudongbaike/3.0_hudongbaike_infobox_property_definitions_zh.zip
zhishime_json/hudongbaike/3.0_hudongbaike_skos_categories_zh.zip
http://openkg1.oss-cn-beijing.aliyuncs.com/d71361be-b741-480f-95ca-e0eae5931877/zhishimejson.tar.gz
http://openkg.cn/dataset/zhishi-me-dump

通用知识图谱（ownthink）
http://openkg.cn/dataset/ownthink
五千万知识图谱数据拿走不谢
链接: https://pan.baidu.com/s/1iATmmV51XwejAAGLsWJ3OA 提取码: fv49 
data.tar.gz
~$ tar -zxvf data.tar.gz
data.txt
~$ md5sum data.txt
c937abff957fdf75d633df14a21b01ea  data.txt
1.4亿中文知识图谱：
ownthink_v2.zip（链接: https://pan.baidu.com/s/1LZjs9Dsta0yD9NH-1y0sAw 提取码: 3hpp ）
unzip -P "https://www.ownthink.com/" ownthink_v2.zip
Archive:  ownthink_v2.zip
  inflating: ownthink_v2.csv
~$ md5sum ownthink_v2.csv
f4c491f051b8af6bc4ab2d68b6b8b82a  ownthink_v2.csv

医疗知识图谱数据
链接: https://pan.baidu.com/s/178DMxv9P-kK7DssofH6URw 提取码: 5x4m 复制这段内容后打开百度网盘手机App，操作更方便哦
解压密码是：www.ownthink.com
Disease.zip

数据下载于：http://openkg.cn/dataset/cndbpedia
/home/gswyhq/data/中文通用百科知识图谱（CN-DBpedia）/baiketriples.zip
$ unzip baiketriples.zip
Archive:  baiketriples.zip
  inflating: baike_triples.txt
CN-DBpedia Dump数据（2015.07）CSV
包含900万+的百科实体以及6600万+的三元组关系。其中摘要信息400万+，标签信息1980万+，infobox信息4100万+


/home/gswyhq/data/中文通用百科知识图谱（CN-DBpedia）/m2e.zip
$ unzip m2e.zip
Archive:  m2e.zip
  inflating: m2e.txt
CN-DBpedia Mention2Entity数据（2015.07）CSV
包含110万+的mention2entity数据

语音合成：
https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2
http://cn-mirror.openslr.org/resources/47/primewords_md_2018_set1.tar.gz

聊天语料：
raw_chat_corpus.zip
存在 https://pan.baidu.com/s/1szmNZQrwh9y994uO8DFL_A 提取码：f2ex 中。

百度中文问答数据集：
https://pan.baidu.com/s/1QUsKcFWZ7Tg1dk_AbldZ1A, 提取码：2dva
WebQA.v1.0.7z

纯净版：
链接: https://pan.baidu.com/s/1pLXEYtd 密码: 6fbf
文件列表：
WebQA.v1.0/readme.txt
WebQA.v1.0/me_test.ann.json （一个问题只配一段材料，材料中有答案）
WebQA.v1.0/me_test.ir.json （一个问题配多段材料，材料可能有也可能没有答案）
WebQA.v1.0/me_train.json （混合的训练语料）
WebQA.v1.0/me_validation.ann.json （一个问题只配一段材料，材料中有答案）
WebQA.v1.0/me_validation.ir.json （一个问题配多段材料，材料可能有也可能没有答案）

mnist.npz:
下载链接：https://pan.baidu.com/s/1jH6uFFC 密码: dw3d
测试keras的代码时显示需要下载'https://s3.amazonaws.com/img-datasets/mnist.npz'，下载了多次都无法访问该地址，致使测试搁置。翻墙下载下来，需要的可以从这里下载解压后放在~/.keras/datases/里面，然后可以运行。

aclImdb.zip
http://s3.amazonaws.com/text-datasets/aclImdb.zip
原始 IMDB 电影评论数据集(不是使用 Keras 内置的已经预先分词的 IMDB 数据)

Keras 内置的IMDB 数据
将从https://s3.amazonaws.com/text-datasets/imdb.npz 下载数据到：.keras/datasets/imdb.npz
链接：https://pan.baidu.com/s/1L7rNOHsFsAJSNirWM4ykMw 密码：kjpa

wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz
tar zxvf aclImdb_v1.tar.gz

https://github.com/charlesliucn/kaggle-in-python/tree/master/kaggle_competitions/IMDB
labeledTrainData.tsv  testData.tsv  unlabeledTrainData.tsv

2014 年英文维基百科的预计算嵌入
这是一个 822 MB 的压缩文件,文件名是 glove.6B.zip,里面包含 400 000 个单词(或非单词的标记)的 100 维嵌入向量。
http://nlp.stanford.edu/data/glove.6B.zip

glove.6B.zip
http://nlp.stanford.edu/data/glove.6B.zip

gswyhq@gswyhq-PC:~/data/glove.6B$ unzip ../glove.6B.zip 
Archive:  ../glove.6B.zip
  inflating: glove.6B.50d.txt        
  inflating: glove.6B.100d.txt       
  inflating: glove.6B.200d.txt       
  inflating: glove.6B.300d.txt  

花卉图像数据集下载
http://download.tensorflow.org/example_images/flower_photos.tgz
5种花卉类型，接近4000张图像，分为训练集与测试集。

atec_nlp_sim_train.csv
相似度匹配语料
数据来源： https://github.com/ziweipolaris/atec2018-nlp.git

同义句语料：
中文文本语义相似度（Chinese Semantic Text Similarity）语料库建设
https://github.com/IAdmireu/ChineseSTS.git
simtrain_to05sts_same.txt  simtrain_to05sts.txt

保险问答用户日志.xlsx

zhrs_faq_8_updateqa_question_answer.json
一些保险相关相似句

意图识别数据_all.txt

cased_L-24_H-1024_A-16.zip
https://storage.googleapis.com/xlnet/released_models/cased_L-24_H-1024_A-16.zip
https://github.com/zihangdai/xlnet

人脸检测预训练模型
https://github.com/ChiCheng123/SRN
SRN.pth
https://pan.baidu.com/share/init?surl=ambmu1Bu6Oi7zTcEnigFyg 密码：6fba

chinese_wwm_L-12_H-768_A-12.zip
中文全词覆盖BERT模型
https://github.com/ymcui/Chinese-BERT-wwm
https://mp.weixin.qq.com/s/nlFXfgM5KKZXnPdwd97JYg

task_data.tgz 任务数据
ERNIE_stable.tgz 模型，包含预训练模型参数
https://github.com/PaddlePaddle/LARK/tree/develop/ERNIE

ChnSentiCorp_htl_all.csv
7000 多条酒店评论数据，5000 多条正向评论，2000 多条负向评论
https://github.com/SophonPlus/ChineseNlpCorpus/raw/master/datasets/ChnSentiCorp_htl_all/ChnSentiCorp_htl_all.csv

waimai_10k.csv
某外卖平台收集的用户评价，正向 4000 条，负向 约 8000 条
https://raw.githubusercontent.com/SophonPlus/ChineseNlpCorpus/master/datasets/waimai_10k/waimai_10k.csv

online_shopping_10_cats.zip
10 个类别，共 6 万多条评论数据，正、负向评论各约 3 万条， 包括书籍、平板、手机、水果、洗发水、热水器、蒙牛、衣服、计算机、酒店
https://github.com/SophonPlus/ChineseNlpCorpus/raw/master/datasets/online_shopping_10_cats/online_shopping_10_cats.zip

weibo_senti_100k.zip
10 万多条，带情感标注 新浪微博，正负向评论约各 5 万条
https://pan.baidu.com/s/1DoQbki3YwqkuwQUOj64R_g

simplifyweibo_4_moods.zip
36 万多条，带情感标注 新浪微博，包含 4 种情感，其中喜悦约 20 万条，愤怒、厌恶、低落各约 5 万条
https://pan.baidu.com/s/16c93E5x373nsGozyWevITg

dmsc
movies.csv ratings.zip
 28 部电影，超 70 万 用户，超 200 万条 评分/评论 数据
https://pan.baidu.com/s/1c0yn3TlkzHYTdEBz3T5arA#list/path=%2F
原始数据集：Douban Movie Short Comments Dataset V2  大小：DMSC.csv 387 MB ；https://www.kaggle.com/utmhikari/doubanmovieshortcomments#DMSC.csv

yf_dianping
ratings.zip links.csv restaurants.csv
https://pan.baidu.com/s/1yMNvHLl6QYsGbjT7u51Nfg
24 万家餐馆，54 万用户，440 万条评论/评分数据

yf_amazon
https://pan.baidu.com/s/1SbfpZb5cm-g2LmnYV_af8Q
52 万件商品，1100 多个类目，142 万用户，720 万条评论/评分数据

微博实体识别.
https://github.com/hltcoe/golden-horse

ez_douban
5 万多部电影（3 万多有电影名称，2 万多没有电影名称），2.8 万 用户，280 万条评分数据
1.1M	links.csv
948K	movies.csv
17M	ratings.zip
8.0K	使用说明.txt
ratings.zip 解压后，会得到文件：62M	ratings.csv
下载地址：https://pan.baidu.com/s/1DkN1LmdSMzm_jCBKhbPbig
数据说明：https://github.com/SophonPlus/ChineseNlpCorpus/blob/master/datasets/ez_douban/intro.ipynb

ACL2019-DuConv
数据下载地址：
http://ai.baidu.com/broad/download

Knowledge Extraction
知识抽取，标注的三元组数据
https://github.com/chenwanyuan/knowledge_extraction/blob/master/data/corpu.tar.gz

MS-Celeb-1M 名人图片数据集
https://hyper.ai/datasets/5543

CCKS 2018 微众银行智能客服问句匹配大赛
10w对左右的标注训练问句对数据集和1w对左右的验证问句对数据集。
https://biendata.com/competition/CCKS2018_3/datadescribe/
https://github.com/zoulala/CCKS_QA/blob/master/data/task3_train.txt

# 旅行险日志：
zdal_log

语义匹配语料：
new_esim
LCQMC: http://icrc.hitsz.edu.cn/info/1037/1146.htm
sentencesim
chinese_sts_corpus
ChineseSTSCorpus = CCKS + LCQMC + sentencesim + chinese_sts_corpus

Iris Data Set(鸢尾属植物数据集)
iris.data 
http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data

腾讯超过800万个中文单词和短语提供200维矢量表示
https://ai.tencent.com/ailab/nlp/data/Tencent_AILab_ChineseEmbedding.tar.gz

cipslong.v1
搜狗阅读理解比赛	大概80k的数据集

dureader_raw
阅读理解数据，百度dureader	100k的问答集

ChineseSTSListCorpus
dureader的文章title	20k类，大概可以生成 400K个不重复的句子对

preprocessed
百度知道数据；包括一些相似句等；
来源：百度 智能问答大赛 
https://www.kesci.com/home/competition/5ad56e667238515d80b53704/content/2

nietzsche.txt
#尼采的一些作品
https://s3.amazonaws.com/text-datasets/nietzsche.txt

cc.zh.300.vec
wiki，200万，300维的词向量

WordVector_60dimensional
60维维基百科词向量
https://pan.baidu.com/s/1o8f1ELs

词向量
sgns.target.word-character.char1-1.dynwin5.thr10.neg5.dim300.iter5.bz2
https://github.com/Embedding/Chinese-Word-Vectors
https://pan.baidu.com/s/1c9yiosHKNIZwRlLzD_F1ig

120G+训练好的word2vec模型（中文词向量）
$ md5sum pack.zip
fa9af79358c746805fa61662928e019b  pack.zip
$ unzip pack.zip
Archive:  pack.zip
   creating: dong/word2vec/
 extracting: dong/word2vec/news_12g_baidubaike_20g_novel_90g_embedding_64.model
 extracting: dong/word2vec/news_12g_baidubaike_20g_novel_90g_embedding_64.bin
 extracting: dong/word2vec/news12g_bdbk20g_nov90g_dim128.tar.gz
 extracting: dong/word2vec/news_12g_baidubaike_20g_novel_90g_embedding_64.model.syn0.npy
 模型参数：
 window=5
 min_count=5
 size=64
 ps：其它参数见gensim库，执行代码为：Word2Vec(sentence, window=5, min_count=5,size=64, workers=4)
 其它相关：
 分词词典使用了130w+词典。分词代码：jieba.lcut(sentence)，默认使用了HMM识别新词；
 剔除了所有非中文字符；
 最终得到的词典大小为6115353；
 目前只跑了64维的结果，后期更新128维词向量；
 模型格式有两种bin和model；
 下载链接：链接: https://pan.baidu.com/s/1o7MWrnc 密码:wzqv
 https://weibo.com/p/23041816d74e01f0102x77v?luicode=20000061&lfid=4098518198740187&featurecode=newtitle

官方的中文预训练bert权重
chinese_L-12_H-768_A-12.zip
https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip

情感语料：
neg.xls  pos.xls
https://www.cnblogs.com/sumuncle/p/6370686.html
http://spaces.ac.cn/usr/uploads/2015/08/646864264.zip
https://github.com/zhuanxuhit/nd101/tree/master/1.Intro_to_Deep_Learning/3.How_to_Do_Sentiment_Analysis/data

中文语音语料（13388个片段）
thchs30: 清华大学30小时的数据集，可以在http://www.openslr.org/18/下载
data_thchs30.tgz [6.4G]
http://blog.sina.com.cn/s/blog_8af106960102xdev.html

评论数据
sum.xls
https://www.cnblogs.com/sumuncle/p/6370686.html
http://spaces.ac.cn/usr/uploads/2015/09/829078856.zip

CCKS2019关系抽取数据
- origin_data [origin data](https://pan.baidu.com/s/1EGPYAQp90usvpzROdNilPw) d6vp
- preprocess_data [preprocessed data](https://pan.baidu.com/s/1lHD-IO7zI4EwyNfi_Fe1HQ) 4f93

维基百科`598454`个词 50维的词向量模型，包括原始的训练数据
zhwiki.zip

中文Wiki语料获取
/home/gswyhq/Downloads/zhwiki-latest-pages-articles.xml.bz2
~$ bunzip2 zhwiki-latest-pages-articles.xml.bz2
~$ ls
zhwiki-latest-pages-articles.xml
wiki中文数据的下载地址是：https://dumps.wikimedia.org/zhwiki/latest/zhwiki-latest-pages-articles.xml.bz2

短文本语义匹配数据集
QA_corpus
https://github.com/terrifyzhao/text_matching/blob/master/input/train.csv

医疗知识图谱NLP项目，实体规模4.4万，实体关系规模30万,实体类型包括：诊断检查项目,医疗科目，疾病，药品，食物，在售药品，疾病症状
https://github.com/liuhuanyong/QASystemOnMedicalKG.git

glyph 文本分类数据：
来源：https://github.com/zhangxiangxiao/glyph
数据集	类	训练	测试
大众点评网(Dianping)	2	2000000	500000
JD全部(JD full)	5	3000000	250000
JD二类(JD binary)	2	4000000	360000
凤凰网(lfeng)	5	800000	50000
中新网(Chinanews)	7	1,400,000	112000

dataSets.zip
中英文语义匹配数据集
中文数据为atec2018数据
https://drive.google.com/file/d/15FTuNdF5YswRMo3USL_lT_h1hI3ylrAc/view?usp=sharing

英文单词音频数据集
speech_commands_v0.01.tar.gz
https://pan.baidu.com/s/1Au85kI_oeDjode2hWumUvQ
speech_commands_v0.02.tar.gz
http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz


multinli_1.0.zip
https://www.nyu.edu/projects/bowman/multinli/multinli_1.0.zip

snli_1.0.zip
https://nlp.stanford.edu/projects/snli/snli_1.0.zip
/home/gswyhq/github_projects/DIIN-in-Keras/data/snli_1.0

Glove预训练词嵌入
该预训练词嵌入根据斯坦福大学提出的Glove模型进行训练，主要包括如下四个文件：
1） glove.6B：Wikipedia 2014 + Gigaword 5 (6B tokens, 400K vocab, uncased, 50d, 100d, 200d, & 300d vectors, 822 MB download)
2） glove.42B.300d：Common Crawl (42B tokens, 1.9M vocab, uncased, 300d vectors, 1.75 GB download)
3）glove.840B.300d：Common Crawl (840B tokens, 2.2M vocab, cased, 300d vectors, 2.03 GB download)
4）glove.twitter.27B：Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased, 25d, 50d, 100d, & 200d vectors, 1.42 GB download)
glove.840B.300d.zip
http://nlp.stanford.edu/data/glove.840B.300d.zip
/home/gswyhq/github_projects/DIIN-in-Keras/data/glove.840B.300d.txt

图像数据集
http://www.nurs.or.jp/~nagadomi/animeface-character-dataset/data/animeface-character-dataset.zip
https://www.crcv.ucf.edu/data/Selfie/Selfie-dataset.tar.gz

汉字、词语解释、成语、歇后语
https://github.com/pwxcoo/chinese-xinhua

OPPO手机搜索排序query-title语义匹配数据集。
链接:https://pan.baidu.com/s/1Hg2Hubsn3GEuu4gubbHCzw 提取码:7p3n

MSRA微软亚洲研究院数据集。
5 万多条中文命名实体识别标注数据（包括地点、机构、人物）
https://github.com/InsaneLife/ChineseNLPCorpus/tree/master/NER/MSRA
https://github.com/Determined22/zh-NER-TF/blob/master/data_path/train_data

中文完形填空, 阅读理解数据集
https://github.com/ymcui/Chinese-RC-Dataset

英汉互译语料
cmn-eng.zip
http://www.manythings.org/anki/cmn-eng.zip
英法互译语料
http://www.manythings.org/anki/fra-eng.zip

/home/gswyhq/data/ERNIE_1.0/ERNIE_stable-1.0.1.tar.gz
ERNIE 1.0 中文 Base 模型	包含预训练模型参数、词典 vocab.txt、模型配置 ernie_config.json
https://baidu-nlp.bj.bcebos.com/ERNIE_stable-1.0.1.tar.gz

ERNIE_1.0中文数据
/home/gswyhq/data/ERNIE_1.0/task_data_zh.tgz
https://ernie.bj.bcebos.com/task_data_zh.tgz

XNLI 15种语言的翻译语料
包含10,000个句子的15种语言平行语料库
XNLI-15way.zip
https://dl.fbaipublicfiles.com/XNLI/XNLI-15way.zip


跨语言自然语言推理（XNLI）语料库是MultiNLI语料库的5,000个测试和2,500个开发对的众包集合。这对配有文字蕴涵注释，并翻译成14种语言：法语，西班牙语，德语，希腊语，保加利亚语，俄语，土耳其语，阿拉伯语，越南语，泰语，中文，印地语，斯瓦希里语和乌尔都语。
XNLI-1.0.zip
https://dl.fbaipublicfiles.com/XNLI/XNLI-1.0.zip

瑞金医院MMC人工智能辅助构建知识图谱大赛
ner数据：data_source.zip
链接：https://pan.baidu.com/s/1jKUpBdKh_prpZ1Seb3R3cQ
提取码：7h1l

kvret_dataset_public.zip
http://nlp.stanford.edu/projects/kvret/kvret_dataset_public.zip

### 中文文本数据集

- [Chinese Treebank](https://catalog.ldc.upenn.edu/LDC2013T21)：来自中国新闻专线，政府文件，杂志文章和各种广播新闻的大约150万字的注释和解析文本
- [Mandarin Chinese News Text](https://github.com/Lab41/sunny-side-up/wiki/Chinese-Datasets)：人民日报，新华社，中国国际广播电台的2.5亿汉字新闻。
- [腾讯AI实验室嵌入中文单词和短语](https://ai.tencent.com/ailab/nlp/embedding.html)语料库：该语料库为超过800万个中文单词和短语提供200维矢量表示，即嵌入，这些单词和短语是在大规模，高质量数据上预先培训的。
- [大规模中文短文摘要数据集](https://arxiv.org/abs/1506.05865)：该语料库由超过200万个真正的中文短文组成，每个文本的作者给出简短的摘要。

### 中文OCR和手写数据集

- [汉字](http://www.iapr-tc11.org/mediawiki/index.php?title=Harbin_Institute_of_Technology_Opening_Recognition_Corpus_for_Chinese_Characters_(HIT-OR3C))：包含909,818张图像的手写汉字数据集，对应于约10篇新闻文章。
- [汉字生成器](https://www.kaggle.com/dylanli/chinesecharacter)：可用于中文文本OCR的中文字体数据集。
- [中文文本](https://ctwdataset.github.io/)：中文文本数据集，大约有一百万个汉字由专家在超过30,000个街景图像中注释。对于数据集中的每个字符，注释包括其基础字符，边界框和6个属性。属性表明它是否具有复杂的背景，是否有凸起，是手写还是打印等。

### 中文翻译和并行文本数据集

- [中英文电子邮件](http://catalog.elra.info/en-us/repository/browse/ELRA-W0113/)：包含15,000个中文字符（相当于10,000个单词）和电子邮件，以及英文参考译文。
- [OntoNotes](https://catalog.ldc.upenn.edu/ldc2013t19)：[带有](https://catalog.ldc.upenn.edu/ldc2013t19)各种文本类型的注释语料库 - 新闻，会话电话语音，网络日志，usenet新闻组，广播，脱口秀 - 中文，英文和阿拉伯文。
- [新加坡国立大学语料库](http://wing.comp.nus.edu.sg:8080/SMSCorpus/history.jsp)：这个语料库是为社交媒体文本规范化和翻译而创建的。它是通过从新加坡国立大学英语短信语料库中随机选择2,000条消息然后翻译成正式中文而构建的。
- [中法文本](https://catalog.ldc.upenn.edu/LDC2018T17)：中文广播新闻中约30,000个汉字子集的法语翻译。
- [GALE第1阶段中文博客平行文本](https://catalog.ldc.upenn.edu/LDC2008T06)：277个中文博客文章翻译成英文。

### 中国情绪分析数据集

- [Ren-CECps](http://a1-www.is.tokushima-u.ac.jp/member/ren/Ren-CECps1.0/Ren-CECps1.0.html)：1,500篇博文（11k段，35k句），文档段落和句子级别带有情感和情感注释。
- [微博PCU](https://archive.ics.uci.edu/ml/datasets/microblogPCU#)：来自西安交通大学的研究人员，这个数据集有来自新浪微博的50,000个帖子，包括用户元数据，包括跟随者信息。

样例三元组数据：
https://github.com/percent4/Neo4j_movie_demo

LIC2019 信息抽取 实体及关系抽取
https://github.com/melancholicwang/lic2019-information-extraction-baseline

Toxic Comment Classification Challenge
https://raw.githubusercontent.com/gangqing/Toxic-Comment-Classification-Challenge/master/train.csv

AISHELL-1
http://www.openslr.org/resources/33/data_aishell.tgz
AISHELL-ASR0009录音文本涉及智能家居、无人驾驶、工业生产等11个领域。录制过程在安静室内环境中， 同时使用3种不同设备： 高保真麦克风（44.1kHz，16-bit）；Android系统手机（16kHz，16-bit）；iOS系统手机（16kHz，16-bit）。高保真麦克风录制的音频降采样为16kHz，用于制作AISHELL-ASR0009-OS1。400名来自中国不同口音区域的发言人参与录制。经过专业语音校对人员转写标注，并通过严格质量检验，此数据库文本正确率在95%以上。分为训练集、开发集、测试集。（支持学术研究，未经允许禁止商用。）

百科 QA问答数据集
baike_qa_train.json
baike_qa_valid.json

中文八卦闲聊问答语料
https://github.com/zake7749/Gossiping-Chinese-Corpus

一千多个选择题及其答案：
https://github.com/lindianyin/yol/blob/master/setting/quiz/quiz_issue_question.txt
https://github.com/prime51/Brainstorm/blob/master/Brainstorm_GamePage/database/QuestionBank.csv
https://github.com/pagoda-animation/QAhero/blob/master/assets/questions/questions.json

一些文学作品数据，txt格式：
https://github.com/BlankRain/ebooks
https://github.com/CaptionwWaterfall/nihong-novel
https://github.com/HeywoodKing/infinite/tree/master/%E6%94%B6%E5%BD%95

汉语古典文本资料库
本数据库有13000种文本，10万卷，近13亿字，3.14 GB。可以作为对比的是，《四库全书》共收书3503种，79337卷，近230万页，约8亿字。据《中国古籍总目》，全球现存汉语古籍的总量为177107种。
https://github.com/mahavivo/scripta-sinica

DeepFashion 服装数据集
https://www.aiuai.cn/aifarm112.html

维基百科json版(wiki2019zh)
104万个词条(1,043,224条; 原始文件大小1.6G，压缩文件519M；数据更新时间：2019.2.7)
https://pan.baidu.com/s/1uPMlIY3vhusdnhAge318TA

新闻语料json版(news2016zh)
250万篇新闻( 原始数据9G，压缩文件3.6G；新闻内容跨度：2014-2016年)
包含了250万篇新闻。新闻来源涵盖了6.3万个媒体，含标题、关键词、描述、正文。
new2016zh.zip
https://drive.google.com/file/d/1TMKu1FpTr6kcjWXWlQHX7YJsMfhhcVKp/view
或者：https://pan.baidu.com/s/1MLLM-CdM6BhJkj8D0u3atA 密码：k265

百科类问答json版(baike2018qa)
150万个问答( 原始数据1G多，压缩文件663M；数据更新时间：2018年)
baike_qa2019.zip
~$ unzip baike_qa2019.zip
Archive:  baike_qa2019.zip
  inflating: baike_qa_train.json
  inflating: baike_qa_valid.json
https://pan.baidu.com/share/init?surl=2TCEwC_Q3He65HtPKN17cA
fu45
含有150万个预先过滤过的、高质量问题和答案，每个问题属于一个类别。总共有492个类别，其中频率达到或超过10次的类别有434个。

社区问答json版(webtext2019zh) ：大规模高质量数据集
410万个问答( 过滤后数据3.7G，压缩文件1.7G；数据跨度：2015-2016年)
webtext2019zh.zip
https://drive.google.com/file/d/1u2yW_XohbYL2YAK6Bzc5XrngHstQTf0v/view
含有410万个预先过滤过的、高质量问题和回复。每个问题属于一个【话题】，总共有2.8万个各式话题，话题包罗万象。
从1400万个原始问答中，筛选出至少获得3个点赞以上的的答案，代表了回复的内容比较不错或有趣，从而获得高质量的数据集。
除了对每个问题对应一个话题、问题的描述、一个或多个回复外，每个回复还带有点赞数、回复ID、回复者的标签。

翻译语料(translation2019zh)
translation2019zh.zip (596M) 
520万个中英文平行语料( 原始数据1.1G，压缩文件596M)
https://drive.google.com/file/d/1EX8eE5YWBxCaohBO8Fh4e2j3b9C2bTVQ/view
中英文平行语料520万对。每一个对，包含一个英文和对应的中文。中文或英文，多数情况是一句带标点符号的完整的话。
对于一个平行的中英文对，中文平均有36个字，英文平均有19个单词(单词如“she”)

500条中文情感分析语料：
https://github.com/Zephery/weiboanalysis/blob/master/train/train.txt
其中1表示积极，2表示消极，3表示客观

NLPCC 2018 开放领域问答
http://tcci.ccf.org.cn/conference/2018/taskdata.php
训练数据：
https://pan.baidu.com/s/1dGtGmBZ
密码：f77i
测试数据：http://tcci.ccf.org.cn/conference/2018/dldoc/tasktestdata07.zip

NLPCC 2017 开放领域问答
http://tcci.ccf.org.cn/conference/2017/taskdata.php
训练数据：http://pan.baidu.com/s/1dEYcQXz
测试数据：http://tcci.ccf.org.cn/conference/2017/dldoc/tasktestdata05.zip

NLPCC 2016 开放领域问答
http://tcci.ccf.org.cn/conference/2016/pages/page05_evadata.html
训练数据：https://github.com/WenRichard/KBQA-BERT/tree/master/Data/NLPCC2016KBQA
https://github.com/huangxiangzhou/NLPCC2016KBQA.git
测试数据：http://tcci.ccf.org.cn/conference/2016/dldoc/evatestdata2-kbqa.testing-data
测试数据：http://tcci.ccf.org.cn/conference/2016/dldoc/evatestdata2-dbqa.testing-data

中文NL2SQL
/home/gswyhq/github_projects/nl2sql/data/train
https://github.com/eguilg/nl2sql.git

中文预训练模型 XLNet-mid, Chinese, TensorFlow 
XLNet-mid：24-layer, 768-hidden, 12-heads, 209M parameters
chinese_xlnet_mid_L-24_H-768_A-12.zip (741M) 
https://drive.google.com/open?id=1342uBc7ZmQwV6Hm6eUIN_OnBSz1LcvfA
XLNet-base：12-layer, 768-hidden, 12-heads, 117M parameters
chinese_xlnet_base_L-12_H-768_A-12.zip
https://drive.google.com/open?id=1m9t-a4gKimbkP5rqGXXsEAEPhJSZ8tvx
https://github.com/ymcui/Chinese-PreTrained-XLNet

人脸识别数据 widerface
wider_face_add_lm_10_10.zip 3.06G
（1）过滤掉10px*10px 小人脸后的干净widerface数据压缩包 ：https://pan.baidu.com/s/1m600pp-AsNot6XgIiqDlOw 提取码：x5gt
wider_face.zip 3.42G
（2）未过滤小人脸的完整widerface数据压缩包 ：https://pan.baidu.com/s/1ijvZFSb3l7C63Nbz7i6IuQ 提取码：8748

开源生物识别数据
features.tar.gz 2.4G
Google Audioset：扩展了 632 个音频分类样本，并从 YouTube 视频中提取了 2，084，320 个人类标记的 10 秒声音片段
地址：https://research.google.com/audioset/
https://research.google.com/audioset/download.html
https://github.com/audioset/ontology.git

chineseGLUEdatasets
https://github.com/chineseGLUE/chineseGLUE
https://storage.googleapis.com/chineseglue/chineseGLUEdatasets.v0.0.1.zip

英文命名实体识别数据：
/home/gswyhq/.keras/datasets/conll2000.zip
https://master.dl.sourceforge.net/project/nltk/OldFiles/conll2000.zip

中文机器阅读理解评测（CMRC 2018）]所使用的数据
原始数据(Original Data)
(https://worksheets.codalab.org/bundles/0x0e6dced40a0d40d980da88882aa8c13a) | cmrc2018_evaluate.py | 4.1k 
(https://worksheets.codalab.org/bundles/0xcd4c755829064426896ef942a249aced) | cmrc2018_trial.json  | 1.0m                  
(https://worksheets.codalab.org/bundles/0x296baa11dfbc4ab08cdeb5b4adf182e2) | cmrc2018_train.json  | 9.0m                
(https://worksheets.codalab.org/bundles/0xb70e5e281fcd437d9aa8f1c4da107ae4) | cmrc2018_dev.json    | 3.4m
SQuAD样式的数据(SQuAD-style Data)
(https://worksheets.codalab.org/bundles/0x1ecd59f275a64bbd8cd79868ed059204) | cmrc2018_evaluate.py | 4.2k
(https://worksheets.codalab.org/bundles/0x182c2e71fac94fc2a45cc1a3376879f7) | cmrc2018_trial.json | 781k 
(https://worksheets.codalab.org/bundles/0x15022f0c4d3944a599ab27256686b9ac) | cmrc2018_train.json | 7.1m
(https://worksheets.codalab.org/bundles/0x72252619f67b4346a85e122049c3eabd) | cmrc2018_dev.json  | 3.1m
或者：https://github.com/ymcui/cmrc2018.git

pytorch版本，预训练的bert模型
Tokenizer需要的是词表: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt
模型：https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese.tar.gz
tar -zxvf bert-base-chinese.tar.gz 
./pytorch_model.bin
./bert_config.json
$md5sum bert-base-chinese.tar.gz 
22ab8d5c0f45ab0705cfff2cebcc53b4  bert-base-chinese.tar.gz
nohup wget -c -t 0 https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese.tar.gz -O bert-base-chinese.tar.gz > bert-base-chinese.log &
nohup wget -c -t 0 https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt -O bert-base-chinese-vocab.txt > bert-base-chinese-vocab.log &
nohup wget -c -t 0 https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-pytorch_model.bin > bert-base-chinese-pytorch_model.log &
最后下载的`bert-base-chinese-pytorch_model.bin`文件，实际上是与`bert-base-chinese.tar.gz`解压出来的文件`pytorch_model.bin`是同一个文件；

繁体中文阅读理解语料：DRCD
https://github.com/DRCKnowledgeTeam/DRCD

食品，餐饮情感分析二分类数据：
https://github.com/YIMIxxxxx/NLP-bert-try

法研杯CAIL2019阅读理解数据
CAIL2019-RC-big.zip
数据集：https://pan.baidu.com/s/1p4NJDhboKSsbFQOwRgDszg 提取码：8w0y

15亿参数 GPT2 中文预训练模型( 15G 语料，训练 10w 步 )
model.ckpt-100000.data-00000-of-00001 (5.2G) 
https://drive.google.com/open?id=1n_5-tgPpQ1gqbyLPbP1PwiFi2eo7SWw_

pytorch: https://github.com/qywu/Chinese-GPT
encoder.pth
https://drive.google.com/uc?id=1Mr2-x_qT2hgyo0RalPjc09NmyNi6a_gs&export=download
model_state_epoch_62.th
https://drive.google.com/uc?id=1W6n7Kv6kvHthUX18DhdGSzBYkyzDvxYh&export=download

真假新闻数据： George Mclntire 的假新闻数据集的子集。它包含大约 1000 篇假新闻和真实新闻的文章
https://raw.githubusercontent.com/cabhijith/Fake-News/master/fake_or_real_news.csv.zip
~$ unzip fake_or_real_news.csv.zip 
Archive:  fake_or_real_news.csv.zip
  inflating: fake_or_real_news.csv 

Fashion MNIST数据集 是kaggle上提供的一个图像分类入门级的数据集，其中包含10个类别的70000个灰度图像
wget -c -t 0 https://gitlab.aiacademy.tw/aai/examples/raw/c13d5198f80d4763216cafa63982ec58d9116365/Fashion-Mnist/fashion-mnist_train.csv
wget -c -t 0 https://gitlab.aiacademy.tw/aai/examples/raw/c13d5198f80d4763216cafa63982ec58d9116365/Fashion-Mnist/fashion-mnist_test.csv
http://yann.lecun.com/exdb/mnist/
wget -c -t 0 http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz
wget -c -t 0 http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz

THUCTC(THU Chinese Text Classification)清华大学自然语言处理实验室，中文文本分类数据集
THUCNews.zip	THUCNews中文文本数据集	1.56GB
http://thuctc.thunlp.org/#%E8%8E%B7%E5%8F%96%E9%93%BE%E6%8E%A5

斗破苍穹、诗词、知乎语料
https://github.com/GaoPeng97/transformer-xl-chinese/blob/master/data/doupo/train.txt

保险问答语料，中文是由英文翻译而成
https://github.com/Samurais/insuranceqa-corpus-zh.git

XLNet-Base, Cased: 12-layer, 768-hidden, 12-heads.
wget -c -t 0 https://storage.googleapis.com/xlnet/released_models/cased_L-12_H-768_A-12.zip .
mv cased_L-12_H-768_A-12.zip xlnet_base_L-12_H-768_A-12.zip
~$ md5sum xlnet_base_L-12_H-768_A-12.zip 
4edf4bf8712997899fcf4c8a678ec6d9  xlnet_base_L-12_H-768_A-12.zip

微博语料
7z x weibo.7z
~$ ls
weibo.7z  weibo.txt
$ md5sum weibo.txt 
78b4644d6baabbab1b425931a99d0753  weibo.txt

BERT-Base, Multilingual Cased (New)    104 languages, 12-layer, 768-hidden, 12-heads, 110M parameters
https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip

MNLI.zip
https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FMNLI.zip?alt=media&token=50329ea1-e339-40e2-809c-10c40afff3ce
TASKS = ["CoLA", "SST", "MRPC", "QQP", "STS", "MNLI", "SNLI", "QNLI", "RTE", "WNLI", "diagnostic"]
TASK2PATH = {"CoLA":'https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FCoLA.zip?alt=media&token=46d5e637-3411-4188-bc44-5809b5bfb5f4',
             "SST":'https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FSST-2.zip?alt=media&token=aabc5f6b-e466-44a2-b9b4-cf6337f84ac8',
             "MRPC":'https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2Fmrpc_dev_ids.tsv?alt=media&token=ec5c0836-31d5-48f4-b431-7480817f1adc',
             "QQP":'https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FQQP.zip?alt=media&token=700c6acf-160d-4d89-81d1-de4191d02cb5',
             "STS":'https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FSTS-B.zip?alt=media&token=bddb94a7-8706-4e0d-a694-1109e12273b5',
             "MNLI":'https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FMNLI.zip?alt=media&token=50329ea1-e339-40e2-809c-10c40afff3ce',
             "SNLI":'https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FSNLI.zip?alt=media&token=4afcfbb2-ff0c-4b2d-a09a-dbf07926f4df',
             "QNLI": 'https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FQNLIv2.zip?alt=media&token=6fdcf570-0fc5-4631-8456-9505272d1601',
             "RTE":'https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FRTE.zip?alt=media&token=5efa7e85-a0bb-4f19-8ea2-9e1840f077fb',
             "WNLI":'https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FWNLI.zip?alt=media&token=068ad0a0-ded7-4bd7-99a5-5e00222e0faf',
             "diagnostic":'https://storage.googleapis.com/mtl-sentence-representations.appspot.com/tsvsWithoutLabels%2FAX.tsv?GoogleAccessId=firebase-adminsdk-0khhl@mtl-sentence-representations.iam.gserviceaccount.com&Expires=2498860800&Signature=DuQ2CSPt2Yfre0C%2BiISrVYrIFaZH1Lc7hBVZDD4ZyR7fZYOMNOUGpi8QxBmTNOrNPjR3z1cggo7WXFfrgECP6FBJSsURv8Ybrue8Ypt%2FTPxbuJ0Xc2FhDi%2BarnecCBFO77RSbfuz%2Bs95hRrYhTnByqu3U%2FYZPaj3tZt5QdfpH2IUROY8LiBXoXS46LE%2FgOQc%2FKN%2BA9SoscRDYsnxHfG0IjXGwHN%2Bf88q6hOmAxeNPx6moDulUF6XMUAaXCSFU%2BnRO2RDL9CapWxj%2BDl7syNyHhB7987hZ80B%2FwFkQ3MEs8auvt5XW1%2Bd4aCU7ytgM69r8JDCwibfhZxpaa4gd50QXQ%3D%3D'}
MRPC_TRAIN = 'https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_train.txt'
MRPC_TEST = 'https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_test.txt'
来源：https://github.com/alibaba/ai-matrix/blob/7e15f4f31a081cfe8e0d526f8f27d2cbe2de3691/macro_benchmark/BERT_Google/download_glue_data.py

SQuAD2.0
https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json
https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json

Nottingham : 超过 1000 首民谣
地址：http://abc.sourceforge.net/NMD/

# 中文症状知识图谱
包含症状实体和症状相关三元组的数据集。
dsc.nlp-bigdatalab.org.rar 
http://openkg.cn/dataset/symptom-in-chinese

四千多部电影
豆瓣电影的知识图谱，展示导演、编剧、演员与电影的关系
dbmovies.json
http://openkg1.oss-cn-beijing.aliyuncs.com/be21c7c4-78a4-416f-b8aa-8d4bf6d2ae0d/dbmovies.json

索答菜谱本体信息
索答50w菜谱本体信息，每个菜谱包含菜名，食材，味道，烹饪时间等属性。
summbarecipedataowl.tar.gz
http://openkg.cn/dataset/summba-recipe

200万条商品画像数据
使用说明： 每一行表示一条商品信息，包括字段：商品标题、商品url、价格、商品描述、品牌、对应百分点类目
http://openkg.cn/dataset/200
gswyhq@gswyhq-PC:~/data$ md5sum 200.zip 
618d0be75945827928ebc6b3c8ca48b5  200.zip

音乐数据
四千多首歌曲及歌词：musicknowledgegraph.rdf; 来源：http://openkg.cn/dataset/music
两万多首歌曲，音乐知识图谱，包括歌曲名、歌手、原唱、语种、热门歌手、热门歌曲等属性，来源：
http://openkg1.oss-cn-beijing.aliyuncs.com/ded3c185-fb07-4dfc-8c44-7a2626ab3f09/musicknowledge.ttl
http://openkg.cn/dataset/http-www-summba-com-ontologies-music-tdb

MovieLen 1M数据及简介
MovieLens 1M数据集包含包含6000个用户在近4000部电影上的100万条评分，也包括电影元数据信息和用户属性信息。下载地址为：
http://files.grouplens.org/datasets/movielens/ml-1m.zip
或者：
wget -c -t 0 https://dgl.ai.s3.us-east-2.amazonaws.com/dataset/ml-1m.tar.gz --no-check-certificate
数据集分为三个文件：电影元数据信息（movie.data）、用户属性信息（users.data)和用户评分数据（ratings.dat)。
$ tar -zxvf ml-1m.tar.gz 
ml-1m/
ml-1m/README
ml-1m/users.dat
ml-1m/movies.dat
ml-1m/ratings.dat
$ wget -c -t 0 http://files.grouplens.org/datasets/movielens/ml-20m.zip

电影数据集：ml-100k.zip
wget -c -t 0 http://files.grouplens.org/datasets/movielens/ml-100k.zip .

电影数据集：ml-latest-small.zip
http://files.grouplens.org/datasets/movielens/ml-latest-small.zip

电影数据集 Moviedata-10M
moviedata-10m.tar.gz
数据集包含如下文件：
gswyhq@gswyhq-PC:~/data/moviedata-10m$ ls |xargs -i md5sum {}
dcd5e4753b8c8610f9cfd8163ae4e28a  comments.csv
937fc28e93da49989b6d0ec6a70a9dcd  movies.csv
e499b499fc3bbd89ff6c64d110fd66b7  person.csv
c3597594060c46eef57e37cdc473b537  ratings.csv
94af28a4700aa447d7d117df580e28f7  README.md
18993f170d30faead920c879b3a6de13  users.csv
74f86fd42ca2544e90f6f578ed5416e5  wx.png
下载链接：https://pan.baidu.com/s/1YdCTPOcnowJuP5XZrjOiVA
密码获取方式如下(不定期更换):
微信搜索【斗码小院】公众号并点击关注;
后台回复【电影数据集】获取密码.

情感分析数据集：
链接:https://pan.baidu.com/s/1WD2-gDU_96LiDuobsE2i-Q ；密码获取方式如下(不定期更换); 微信搜索【斗码小院】公众号并点击关注; 后台回复【电影数据集】获取密码
本数据集包括两个文件：
comment_trainset_2class.csv 45.6M
comment_testset_2class.csv 5.2M

50w中文闲聊语料 https://github.com/yangjianxin1/GPT2-chitchat
百度网盘【提取码:jk8d】https://pan.baidu.com/s/1mkP59GyF9CZ8_r1F864GEQ 
由作者GaoQ1(https://github.com/GaoQ1)提供的比较高质量的闲聊数据集，整理出了50w个多轮对话的语料

中文多项选择题数据集：
https://github.com/nlpdata/c3
收集的主要是形式自由的多项选择题，阅读材料来自汉语水平考试和民族汉语考试，包括试卷和练习。
一共有13369篇文章和19577个问题，其中的60%用是训练集，20%是开发集，20%是测试集。
数据集从类型上分为两个部分：正式书面文本(M)和口语化文本(D)

百度知道问答数据
https://github.com/liuhuanyong/MiningZhiDaoQACorpus
zhidao_qa.json.zip 1.54G
其中: 1, 问题个数583万个。
2, 问答对983万个。
3, 每个问题的答案个数1.7个。
4, 问题标签个数5824个。
网盘地址为链接:https://pan.baidu.com/s/1Eesx24tAbfJ3Mch-6OeGrA 密码:oin3

一些词库：
https://github.com/fighting41love/funNLP.git
IT词库    财经词库  成语词库  动物词库  繁简体转换词库  古诗词库      敏感词库            诗词短句词库  停用词                        医学词库  中文分词词库整理  中文谣言数据
NLP_BOOK  拆字词库  地名词库  法律词库  公司名字词库    历史名人词库  汽车品牌、零件词库  食物词库      同义词库、反义词库、否定词库  职业词库  中文缩写库        中英日文名字库

豆瓣电影与书籍详细信息
数据链接: https://pan.baidu.com/s/1cLdsAXLGH2akJqMIsGdoig 提取码: n97y

文本与wiki实体关系链接数据
linked-wikitext-2.142e2e52.zip
https://rloganiv.github.io/linked-wikitext-2/?spm=a2c4e.10696291.0.0.10b419a4otnaBv#/

中文实体链接数据集：
https://github.com/panchunguang/ccks_baidu_entity_link
比赛网址： https://biendata.com/competition/ccks_2019_el/ 
数据集：https://pan.baidu.com/s/1SShtugdAMVf0fdaBowtMiA 提取码:8r80

阅读理解数据集：
阅读理解模型，主要根据问题和答案选取出和问题相关的答案。
https://github.com/LIANGQINGYUAN/Q-A-TextClassification/blob/master/preprocess/train_data_sample.json

中文nlp数据集搜索：
NLP 数据集囊括了 NER、QA、情感分析、文本分类、文本分配、文本摘要、机器翻译、知识图谱、语料库以及阅读理解等 10 大类共 142 个数据集。
https://github.com/CLUEbenchmark/CLUEDatasetSearch

语音语料Librispeech
http://www.openslr.org/12
dev-clean.tar.gz [337M]   (development set, "clean" speech )   Mirrors: [China]   
dev-other.tar.gz [314M]   (development set, "other", more challenging, speech )   Mirrors: [China]   
test-clean.tar.gz [346M]   (test set, "clean" speech )   Mirrors: [China]   
test-other.tar.gz [328M]   (test set, "other" speech )   Mirrors: [China]   
train-clean-100.tar.gz [6.3G]   (training set of 100 hours "clean" speech )   Mirrors: [China]   
train-clean-360.tar.gz [23G]   (training set of 360 hours "clean" speech )   Mirrors: [China]   
train-other-500.tar.gz [30G]   (training set of 500 hours "other" speech )   Mirrors: [China]   
intro-disclaimers.tar.gz [695M]   (extracted LibriVox announcements for some of the speakers )   Mirrors: [China]   
original-mp3.tar.gz [87G]   (LibriVox mp3 files, from which corpus' audio was extracted )   Mirrors: [China]   
original-books.tar.gz [297M]   (Project Gutenberg texts, against which the audio in the corpus was aligned )   Mirrors: [China]   
raw-metadata.tar.gz [33M]   (Some extra meta-data produced during the creation of the corpus )   Mirrors: [China]   
md5sum.txt [600 bytes]   (MD5 checksums for the archive files )   Mirrors: [China]  

wget -c -t 0 http://www.openslr.org/resources/12/dev-clean.tar.gz

tar zxvf dev-clean.tar.gz
LibriSpeech/LICENSE.TXT
LibriSpeech/README.TXT
LibriSpeech/CHAPTERS.TXT
LibriSpeech/SPEAKERS.TXT
LibriSpeech/BOOKS.TXT
LibriSpeech/dev-clean/
LibriSpeech/dev-clean/2277/
LibriSpeech/dev-clean/2277/149896/
LibriSpeech/dev-clean/2277/149896/2277-149896-0026.flac
LibriSpeech/dev-clean/2277/149896/2277-149896-0005.flac
LibriSpeech/dev-clean/2277/149896/2277-149896-0033.flac
LibriSpeech/dev-clean/2277/149896/2277-149896-0006.flac
...

中文医疗问答数据集
<Andriatria_男科> 94596个问答对 <IM_内科> 220606个问答对 <OAGD_妇产科> 183751个问答对 <Oncology_肿瘤科> 75553个问答对 <Pediatric_儿科> 101602个问答对 <Surgical_外科> 115991个问答对 总计 792099个问答对
https://github.com/Toyhom/Chinese-medical-dialogue-data

中文医疗问答数据-好大夫.zip
中文医患对话数据集；
度盘下载地址: https://pan.baidu.com/s/1ZwzNgvAAMQk4klerTspsoA
提取码: lbo4

中文医药方面的问答数据集，超过10万条。
https://github.com/zhangsheng93/cMedQA2

医渡云标准化7K数据集
Yidu-N4K 数据集源自CHIP 2019 评测任务一，即“临床术语标准化任务”的数据集。 临床术语标准化任务是医学统计中不可或缺的一项任务。临床上，关于同一种诊断、手术、药品、检查、化验、症状等往往会有成百上千种不同的写法。标准化（归一）要解决的问题就是为临床上各种不同说法找到对应的标准说法。有了术语标准化的基础，研究人员才可对电子病历进行后续的统计分析。本质上，临床术语标准化任务也是语义相似度匹配任务的一种。但是由于原词表述方式过于多样，单一的匹配模型很难获得很好的效果。
http://openkg.cn/dataset/yidu-n7k
yidu-n7k.zip

医渡云结构化4K数据集
Yidu-S4K 数据集源自CCKS 2019 评测任务一，即“面向中文电子病历的命名实体识别”的数据集，包括两个子任务：
1）医疗命名实体识别：由于国内没有公开可获得的面向中文电子病历医疗实体识别数据集，本年度保留了医疗命名实体识别任务，对2017年度数据集做了修订，并随任务一同发布。本子任务的数据集包括训练集和测试集。
2）医疗实体及属性抽取（跨院迁移）：在医疗实体识别的基础上，对预定义实体属性进行抽取。本任务为迁移学习任务，即在只提供目标场景少量标注数据的情况下，通过其他场景的标注数据及非标注数据进行目标场景的识别任务。本子任务的数据集包括训练集（非目标场景和目标场景的标注数据、各个场景的非标注数据）和测试集（目标场景的标注数据）。
http://openkg.cn/dataset/yidu-s4k
yidu-s4k.zip

瑞金医院糖尿病数据集
度盘下载地址：https://pan.baidu.com/s/1CWKblBNBqR-vs2h0xiXSdQ
提取码：0c54

医疗行业专业词汇语料
说明	数量	文件
口腔科病历词汇	11,170	stomatology.txt
国际疾病分类ICD全库	54,304	ICD.csv
疾病诊断编码库ICD-10	12109	ICD-code-10.csv
医院固定资产词汇	471	properties.txt
药品名称词汇	37,308	medicine.txt
电子病历常见词汇	1985	emr.txt
链接：https://github.com/xtea/chinese_medical_words


音频数据，普通话朗读示范录音作品1-60_MP3.rar
下载地址：https://474b.com/file/20390886-447891588

中文标准女声音库（10000句)
数据内容：中文标准女声语音库数据
录音语料：综合语料样本量；音节音子的数量、类型、音调、音连以及韵律等进行覆盖。
有效时长：约12小时
平均字数：16字
语言类型：标准普通话
发 音 人：女；20-30岁；声音积极知性
采样格式：无压缩PCM WAV格式，采样率为48KHz、16bit
标注内容：音字校对、韵律标注、中文声韵母边界切分
标注格式：文本标注为.txt格式文档；音节音素边界切分文件为.interval格式
https://www.data-baker.com/#/data/index/source
https://online-of-baklong.oss-cn-huhehaote.aliyuncs.com/story_resource/BZNSYP.rar?Expires=1622816310&OSSAccessKeyId=LTAI3GkKBSJFDJsp&Signature=iVrcEIYdxpAHZODaxu%2F6i%2Fk0vkY%3D

LCSTS自动摘要数据集
微博的自动摘要数据集
原始数据：链接:https://pan.baidu.com/s/1gCOSPWDt3WLc5LE_-MoEbw 密码:d54p
第一部分，摘要和正文数据分别抽取出来。链接:https://pan.baidu.com/s/16UDPzlUMCP_hCjo8Huovcw 密码:8hkc

