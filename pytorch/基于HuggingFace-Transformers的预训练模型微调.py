#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from transformers import BertForSequenceClassification
from transformers import BertTokenizer
import torch
import os

USERNAME = os.getenv("USERNAME")

# æ¨¡å‹æ¥æºï¼šhttps://huggingface.co/IDEA-CCNL/Erlangshen-Roberta-110M-Sentiment
model_dir = rf"D:/Users/{USERNAME}/data/Erlangshen-Roberta-110M-Sentiment"

tokenizer=BertTokenizer.from_pretrained(model_dir)
model=BertForSequenceClassification.from_pretrained(model_dir)

text='ä»Šå¤©å¿ƒæƒ…ä¸å¥½'

output=model(torch.tensor([tokenizer.encode(text)]))
print(torch.nn.functional.softmax(output.logits,dim=-1))
# tensor([[0.9551, 0.0449]], grad_fn=<SoftmaxBackward0>)

############################################################################################################################
# ä½¿ç”¨ Trainer API åœ¨ PyTorch ä¸­è¿›è¡Œå¾®è°ƒ
# ç”±äº PyTorch ä¸æä¾›å°è£…å¥½çš„è®­ç»ƒå¾ªç¯ï¼ŒğŸ¤— Transformers åº“å†™äº†äº†ä¸€ä¸ªtransformers.Trainer APIï¼Œå®ƒæ˜¯ä¸€ä¸ªç®€å•ä½†åŠŸèƒ½å®Œæ•´çš„ PyTorch è®­ç»ƒå’Œè¯„ä¼°å¾ªç¯ï¼Œ
# é’ˆå¯¹ ğŸ¤— Transformers è¿›è¡Œäº†ä¼˜åŒ–ï¼Œæœ‰å¾ˆå¤šçš„è®­ç»ƒé€‰é¡¹å’Œå†…ç½®åŠŸèƒ½ï¼ŒåŒæ—¶ä¹Ÿæ”¯æŒå¤šGPU/TPUåˆ†å¸ƒå¼è®­ç»ƒå’Œæ··åˆç²¾åº¦ã€‚
# ä¹Ÿæ”¯æŒæ··åˆç²¾åº¦è®­ç»ƒï¼Œå¯ä»¥åœ¨è®­ç»ƒçš„é…ç½® TrainingArguments ä¸­ï¼Œè®¾ç½® fp16 = Trueã€‚
# å³Trainer APIæ˜¯ä¸€ä¸ªå°è£…å¥½çš„è®­ç»ƒå™¨ï¼ˆTransformersåº“å†…ç½®çš„å°æ¡†æ¶ï¼Œå¦‚æœæ˜¯Tensorflowï¼Œåˆ™æ˜¯TFTrainerï¼‰ã€‚
# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

from transformers import TrainingArguments
from datasets import load_metric
from transformers import Trainer
import numpy as np

from scipy.stats import pearsonr, spearmanr
from sklearn.metrics import f1_score, matthews_corrcoef, accuracy_score, precision_recall_fscore_support

def simple_accuracy(preds, labels):
    return float((preds == labels).mean())


def acc_and_f1(preds, labels):
    acc = simple_accuracy(preds, labels)
    f1 = float(f1_score(y_true=labels, y_pred=preds))
    return {
        "accuracy": acc,
        "f1": f1,
    }


def pearson_and_spearman(preds, labels):
    pearson_corr = float(pearsonr(preds, labels)[0])
    spearman_corr = float(spearmanr(preds, labels)[0])
    return {
        "pearson": pearson_corr,
        "spearmanr": spearman_corr,
    }

def compute_metrics(eval_preds):
    '''è®¡ç®—è¯„ä»·æŒ‡æ ‡'''
    logits, labels = eval_preds
    preds = np.argmax(logits, axis=-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')
    acc = accuracy_score(labels, preds)
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }

from datasets import load_dataset
raw_datasets = load_dataset('text', data_files={'train': rf"D:/Users/{USERNAME}/data/sentiment/sentiment.train.txt",
                                           'test': rf"D:/Users/{USERNAME}/data/sentiment/sentiment.test.txt",
                                           "valid": rf"D:/Users/{USERNAME}/data/sentiment/sentiment.valid.txt"})

def tokenize_function(example):
    text_list = []
    labels = []
    for raw_text in example['text']:
        if not raw_text or not raw_text.strip():
            continue
        text, label = raw_text.split('\t')
        text_list.append(text)
        labels.append(int(label))
    tokenized = tokenizer(text_list, padding='max_length', truncation=True, max_length=128)
    tokenized['labels'] = labels
    return tokenized

tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)

# from transformers import DataCollatorWithPadding
# data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
# data_collatorçš„ä½œç”¨æ˜¯è‡ªåŠ¨å°†åŒä¸€ä¸ªbatchçš„å¥å­paddingæˆåŒä¸€é•¿åº¦ï¼Œè€Œä¸æ˜¯ä¸€æ¬¡æ€§paddingæ•´ä¸ªæ•°æ®é›†ã€‚

# å®šä¹‰æ¨¡å‹è®­ç»ƒå‚æ•°ï¼Œè¿™é‡Œæœªä¿®æ”¹é¢„è®­ç»ƒæ¨¡å‹ç»“æ„ï¼Œä»…ä»…ä½¿ç”¨æä½å­¦ä¹ ç‡å¾®è°ƒæ¨¡å‹
training_args = TrainingArguments("test-trainer",
                                  evaluation_strategy="epoch", # æœ‰ä¸‰ä¸ªé€‰é¡¹: â€œnoâ€ï¼šè®­ç»ƒæ—¶ä¸åšä»»ä½•è¯„ä¼°; â€œstepâ€ï¼šæ¯ä¸ª eval_steps å®Œæˆï¼ˆå¹¶è®°å½•ï¼‰è¯„ä¼°; â€œepochâ€ï¼šåœ¨æ¯ä¸ª epoch ç»“æŸæ—¶è¿›è¡Œè¯„ä¼°ã€‚
                                  learning_rate=1e-5,
                                  load_best_model_at_end=True, # è®­ç»ƒç»“æŸæ—¶åŠ è½½åœ¨è®­ç»ƒæœŸé—´æ‰¾åˆ°çš„æœ€ä½³æ¨¡å‹
                                  save_strategy='epoch',  # åœ¨è®­ç»ƒæœŸé—´é‡‡ç”¨çš„æ£€æŸ¥ç‚¹ä¿å­˜ç­–ç•¥ã€‚å¯èƒ½çš„å€¼ä¸ºï¼šâ€œnoâ€ï¼šè®­ç»ƒæœŸé—´ä¸ä¿å­˜ï¼›â€œepochâ€ï¼šåœ¨æ¯ä¸ªepochç»“æŸæ—¶è¿›è¡Œä¿å­˜ï¼›â€œstepsâ€ï¼šæ¯ä¸ªstepä¿å­˜ä¸€æ¬¡ã€‚
                                  per_device_train_batch_size=32,
                                  num_train_epochs=5,  # è®­ç»ƒè½®æ•°
                                  )

trainer = Trainer(
    model,
    training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["valid"],
    # data_collator=data_collator,
    # tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)

trainer.train()

def main():
    pass


if __name__ == '__main__':
    main()
