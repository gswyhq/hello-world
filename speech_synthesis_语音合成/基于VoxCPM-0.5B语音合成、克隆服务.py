import gradio as gr
import soundfile as sf
from voxcpm import VoxCPM
import os
import tempfile
import logging
import time
import base64
from logging import StreamHandler
from logging.handlers import QueueHandler, QueueListener, TimedRotatingFileHandler
from multiprocessing import Queue
from fastapi import FastAPI, Depends, Request, UploadFile, File, HTTPException, status
from fastapi.responses import StreamingResponse
from fastapi.responses import FileResponse
from fastapi import Form
from pydantic import BaseModel, ConfigDict
from typing import Dict, Any, List, Optional
import logging
import traceback
import os


# åˆ›å»ºæ—¥å¿—è®°å½•å™¨
logger = logging.getLogger('my_logger')
logger.setLevel(logging.DEBUG)

true = True
false = False
null = None
CONNECT_TIMEOUT, READ_TIMEOUT = 600, 600

# åˆ›å»ºé˜Ÿåˆ—
log_queue = Queue()

# åˆ›å»ºå¤„ç†å™¨ï¼Œæ¯å¤©ç”Ÿæˆä¸€ä¸ªæ–°æ–‡ä»¶ï¼Œä¿ç•™7å¤©çš„æ—¥å¿—
file_handler = TimedRotatingFileHandler(
    'logs/app.log',  # æ—¥å¿—æ–‡ä»¶å
    when='D',    # æŒ‰å¤©åˆ†å‰²
    interval=1,  # æ¯éš”1å¤©åˆ†å‰²
    backupCount=3650,  # ä¿ç•™7å¤©çš„æ—¥å¿—
    encoding='utf-8' # è®¾ç½®æ–‡ä»¶ç¼–ç ä¸ºUTF-8
)

# åˆ›å»ºç»ˆç«¯å¤„ç†å™¨ï¼ˆæ–°å¢éƒ¨åˆ†ï¼‰
console_handler = StreamHandler()

# è®¾ç½®ç»Ÿä¸€çš„æ—¥å¿—æ ¼å¼
formatter = logging.Formatter('%(asctime)s - %(levelname)s - [Process: %(process)d] [Thread: %(thread)d] - [%(filename)s:%(lineno)d] %(message)s')
file_handler.setFormatter(formatter)
console_handler.setFormatter(formatter)  # ç»ˆç«¯ä½¿ç”¨ç›¸åŒæ ¼å¼

# åˆ›å»ºé˜Ÿåˆ—å¤„ç†å™¨
queue_handler = QueueHandler(log_queue)

# å°†é˜Ÿåˆ—å¤„ç†å™¨æ·»åŠ åˆ°è®°å½•å™¨
logger.addHandler(queue_handler)

# åˆ›å»ºé˜Ÿåˆ—ç›‘å¬å™¨
listener = QueueListener(log_queue, file_handler, console_handler)
listener.start()

# è®°å½•æ—¥å¿—
logger.info('This is a log message.')

# åŠ è½½ VoxCPM æ¨¡å‹(https://hf-mirror.com/openbmb/VoxCPM-0.5B)
model = VoxCPM.from_pretrained("/root/VoxCPM-0.5B", load_denoiser=False)

DEFAULT_VOICE_DICT = {
    "å¥³å£°": ["/root/VoxCPM-0.5B/examples/617d64a7a0e5.wav", """ä½ å¥½ï¼Œæˆ‘æ˜¯***çš„ï¼Œä½ è¿™è¾¹å‘¢æ˜¯æˆ‘ä»¬çš„è€å®¢æˆ·äº†ï¼Œç›¸ä¿¡ä½ ä¹Ÿæ˜¯å¬è¯´è¿‡æƒ æ°‘ä¿çš„å§ï¼Œé‚£è¿™ä¸ªå“ˆï¼Œå°±æ˜¯å’±ä»¬ç¦å»ºå½“åœ°æ¨å‡ºæ¥çš„è¡¥å……åŒ»ç–—ä¿éšœï¼Œæ˜¯æ”¿åºœåŠ›æ¨çš„å…¬ç›Šæ€§äº§å“ï¼Œå°±åªè¦129å…ƒå°±æœ‰350ä¸‡å…ƒçš„ä¿éšœï¼Œè€Œä¸”å…¨å®¶è€å°‘éƒ½å¯ä»¥ä¿ï¼Œå¸¦ç—…å‚ä¿ä¹Ÿæ˜¯å¯ä»¥èµ”çš„ï¼Œé‚£ç›®å‰è¿™è¾¹å‘¢ï¼Œå·²ç»æ˜¯æœ‰å‡ åä¸‡äººå‘¢éƒ½ä¿äº†ï¼Œä»Šå¤©å°±æ˜¯é€šçŸ¥ä½ å‚ä¿çš„ï¼Œæˆ‘å·²ç»æ˜¯ç»™ä½ å‘çŸ­ä¿¡äº†ï¼Œä½ ä¹Ÿèµ¶æ€¥ç‚¹å‡»äº†è§£ä¸€ä¸‹å§"""],
    "ç«¥å£°": ["/root/VoxCPM-0.5B/examples/tmpo6j6gkx3.wav", 'å±±è¡Œï¼Œå”ï¼Œæœç‰§ï¼Œè¿œä¸Šå¯’å±±çŸ³å¾„æ–œï¼Œç™½äº‘ç”Ÿå¤„æœ‰äººå®¶ï¼Œåœè½¦åçˆ±æ«æ—æ™šï¼Œéœœå¶çº¢äºäºŒæœˆèŠ±'],
    "alloy": ["/root/VoxCPM-0.5B/examples/tmpceuon29a.wav", 'ä½ å¥½ï¼Œè¿™é‡Œæ˜¯è¯­éŸ³æ’­æ”¾æµ‹è¯•'],
    "echo": ["/root/VoxCPM-0.5B/examples/tmpkagzsim4.wav", 'ä½ å¥½ï¼Œè¿™é‡Œæ˜¯è¯­éŸ³æ’­æ”¾æµ‹è¯•'],
    "fable": ["/root/VoxCPM-0.5B/examples/tmpxhchdxho.wav", 'ä½ å¥½ï¼Œè¿™é‡Œæ˜¯è¯­éŸ³æ’­æ”¾æµ‹è¯•'],
    "onyx": ["/root/VoxCPM-0.5B/examples/tmpfm9nrqc5.wav", 'ä½ å¥½ï¼Œè¿™é‡Œæ˜¯è¯­éŸ³æ’­æ”¾æµ‹è¯•'],
    "nova": ["/root/VoxCPM-0.5B/examples/tmp8d259tds.wav", 'ä½ å¥½ï¼Œè¿™é‡Œæ˜¯è¯­éŸ³æ’­æ”¾æµ‹è¯•'],
    "shimmer": ["/root/VoxCPM-0.5B/examples/tmp12w67ulu.wav", 'ä½ å¥½ï¼Œè¿™é‡Œæ˜¯è¯­éŸ³æ’­æ”¾æµ‹è¯•'],
}

# ç”Ÿæˆè¯­éŸ³çš„å‡½æ•°
def generate_audio(prompt_audio, prompt_text, target_text):
    logger.info("âœ… æ¥æ”¶åˆ°æ–°çš„è¯­éŸ³åˆæˆè¯·æ±‚")

    # åˆ¤æ–­æ˜¯å¦ä¸ºå…‹éš†æ¨¡å¼
    if prompt_audio:
        logger.info("ğŸ™ï¸ è¿›å…¥å£°éŸ³å…‹éš†æ¨¡å¼")
        with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmpfile:
            if isinstance(prompt_audio, str):
                prompt_wav_path = prompt_audio
            else:
                prompt_audio.save(tmpfile.name)
                prompt_wav_path = tmpfile.name
            logger.info(f"ğŸ“ å‚è€ƒéŸ³é¢‘å·²ä¿å­˜è‡³: {prompt_wav_path}")

        # ç”Ÿæˆè¯­éŸ³
        wav = model.generate(
            text=target_text,
            prompt_wav_path=prompt_wav_path,
            prompt_text=prompt_text,
            cfg_value=2.0,
            inference_timesteps=10,
            normalize=True,
            denoise=False,
            retry_badcase=True,
            retry_badcase_max_times=3,
            retry_badcase_ratio_threshold=6.0
        )
        logger.info("ğŸ”Š å¼€å§‹ä¿å­˜å…‹éš†è¯­éŸ³æ–‡ä»¶")
    else:
        logger.info("ğŸ™ï¸ è¿›å…¥æ™®é€šè¯­éŸ³åˆæˆæ¨¡å¼")
        # æ™®é€šè¯­éŸ³åˆæˆæ¨¡å¼
        wav = model.generate(
            text=target_text,
            prompt_wav_path=None,
            prompt_text=None,
            cfg_value=2.0,
            inference_timesteps=10,
            normalize=True,
            denoise=False,
            retry_badcase=True,
            retry_badcase_max_times=3,
            retry_badcase_ratio_threshold=6.0
        )
        logger.info("ğŸ”Š å¼€å§‹ä¿å­˜æ™®é€šè¯­éŸ³æ–‡ä»¶")

    # ä¿å­˜ç”Ÿæˆçš„è¯­éŸ³åˆ°ä¸´æ—¶æ–‡ä»¶
    with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmpfile:
        sf.write(tmpfile.name, wav, 16000)
        output_path = tmpfile.name
        logger.info(f"âœ… è¯­éŸ³åˆæˆå®Œæˆï¼Œæ–‡ä»¶ä¿å­˜è‡³: {output_path}")

    return output_path

# Gradio ç•Œé¢å®šä¹‰
def voice_cloning_interface(prompt_audio, prompt_text, target_text):
    if not target_text:
        return "âŒ è¯·è¾“å…¥éœ€è¦åˆæˆçš„æ–‡æœ¬ã€‚"

    logger.info("ğŸ“ å¼€å§‹å¤„ç†è¯­éŸ³åˆæˆè¯·æ±‚")
    output_path = generate_audio(prompt_audio, prompt_text, target_text)
    logger.info("ğŸ‰ è¯­éŸ³åˆæˆæµç¨‹ç»“æŸ")
    return output_path

# åˆ›å»º Gradio ç•Œé¢
demo = gr.Interface(
    fn=voice_cloning_interface,
    inputs=[
        gr.Audio(type="filepath", label="ä¸Šä¼ å‚è€ƒéŸ³é¢‘ï¼ˆ.wavï¼Œå¯é€‰ï¼‰", value=DEFAULT_VOICE_DICT['å¥³å£°'][0]),
        gr.Textbox(label="å‚è€ƒéŸ³é¢‘å¯¹åº”çš„æ–‡æœ¬ï¼ˆå¯é€‰ï¼‰", value=DEFAULT_VOICE_DICT['å¥³å£°'][1], lines=3),
        gr.Textbox(label="éœ€è¦åˆæˆçš„æ–‡æœ¬", lines=3, value='å±±è¡Œï¼Œå”ï¼Œæœç‰§ï¼Œè¿œä¸Šå¯’å±±çŸ³å¾„æ–œï¼Œç™½äº‘ç”Ÿå¤„æœ‰äººå®¶ï¼Œåœè½¦åçˆ±æ«æ—æ™šï¼Œéœœå¶çº¢äºäºŒæœˆèŠ±')
    ],
    outputs=gr.Audio(label="ç”Ÿæˆçš„è¯­éŸ³", type="filepath"),
    title="ğŸ™ï¸ VoxCPM è¯­éŸ³åˆæˆæœåŠ¡",
    description="è¾“å…¥æ–‡æœ¬ï¼Œå¯é€‰ä¸Šä¼ å‚è€ƒéŸ³é¢‘å’Œæ–‡æœ¬ï¼Œå³å¯ç”Ÿæˆè¯­éŸ³ã€‚"
)


# å¯ç”¨ Gradio çš„é˜Ÿåˆ—æœºåˆ¶ï¼ˆä¿ç•™ä½ çš„é…ç½®ï¼‰
demo.queue(
    status_update_rate=1,
    default_concurrency_limit=120,
    max_size=120
)

app = FastAPI(root_path="/ai")

# TTS Model
class TTSRequest(BaseModel):
    model: str
    input: str
    voice: str
    response_format: Optional[str] = "wav"
    speed: Optional[float] = 1.0

# Transcription Model
class TranscriptionRequest(BaseModel):
    model: str
    language: Optional[str] = None
    prompt: Optional[str] = None
    response_format: Optional[str] = "json"
    temperature: Optional[float] = 0.0
    timestamp_granularities: Optional[List[str]] = ["segment"]

# Constants
CHAT_OPENAI_API_KEY = "chat-9sF!2Lm@qW7xZ$423we23e"
MODEL_TOKEN_DICT = {
    "tts-1": "tts-erwweofwehFWEOFJfweff",
    "whisper-1": "whisper-dwhfweWEFWEWFWDWDDdfwf"
}

# API Key éªŒè¯ä¾èµ–
async def verify_api_key(request: Request):
    api_key = request.headers.get("Authorization")
    logger.info(f"api-key={api_key}")
    if api_key is None or not api_key.startswith("Bearer "):
        logger.error(f"è®¤è¯æ— æ•ˆï¼š{api_key}")
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Missing or invalid Authorization header"
        )
    token = api_key.split("Bearer ")[1].strip()
    try:
        item_data = await request.json()
        logger.info(f"item_data: {item_data}")
        model = item_data.get("model")
    except:
        try:
            form_data = await request.form()
            model = form_data.get("model")
        except Exception as e:
            logger.error(f"è§£æè¡¨å•å¤±è´¥: {e}")
            model = None

    logger.info(f"request.url.path: {request.url.path}")

    # å¦‚æœæ˜¯ /v1/models æ¥å£ï¼Œå…è®¸ä»»æ„ä¸€ä¸ªæœ‰æ•ˆçš„ API Key
    if request.url.path.startswith("/v1/models"):
        valid_keys = list(MODEL_TOKEN_DICT.values()) + [CHAT_OPENAI_API_KEY]
        if token not in valid_keys:
            logger.error(f"è®¤è¯æ— æ•ˆï¼š{api_key}")
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Invalid API Key"
            )
        return token

    expected_api_key = MODEL_TOKEN_DICT.get(model, CHAT_OPENAI_API_KEY)
    if token != expected_api_key:
        logger.error(f"è®¤è¯æ— æ•ˆï¼š{api_key}")
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid API Key"
        )
    return token

@app.get("/v1/models")
async def list_models(token: str = Depends(verify_api_key)):
    logger.info("è·å–æ¨¡å‹åˆ—è¡¨è¯·æ±‚")
    # æ¨¡æ‹Ÿæ¨¡å‹åˆ—è¡¨æ•°æ®
    models = [
        {
            "id": "tts-1",
            "object": "model",
            "owned_by": "organization-owner",
            "permission": [
                {
                    "id": "perm-1",
                    "object": "model_permission",
                    "created": int(time.time()),
                    "allow_create_engine": False,
                    "allow_sampling": True,
                    "allow_logprobs": True,
                    "allow_search_indices": False,
                    "allow_view": True,
                    "allow_fine_tuning": False,
                    "organization": "*",
                    "group": None,
                    "is_blocking": False
                }
            ]
        },
        {
            "id": "whisper-1",
            "object": "model",
            "owned_by": "organization-owner",
            "permission": [
                {
                    "id": "perm-2",
                    "object": "model_permission",
                    "created": int(time.time()),
                    "allow_create_engine": False,
                    "allow_sampling": True,
                    "allow_logprobs": True,
                    "allow_search_indices": False,
                    "allow_view": True,
                    "allow_fine_tuning": False,
                    "organization": "*",
                    "group": None,
                    "is_blocking": False
                }
            ]
        }
    ]
    return {
        "object": "list",
        "data": models
    }

@app.get("/v1/models/{model_id}")
async def get_model(model_id: str, token: str = Depends(verify_api_key)):
    logger.info(f"è·å–æ¨¡å‹è¯¦æƒ…è¯·æ±‚: {model_id}")
    # æ¨¡æ‹Ÿæ¨¡å‹è¯¦æƒ…æ•°æ®
    models = {
        "tts-1": {
            "id": "tts-1",
            "object": "model",
            "owned_by": "organization-owner",
            "permission": [
                {
                    "id": "perm-1",
                    "object": "model_permission",
                    "created": int(time.time()),
                    "allow_create_engine": False,
                    "allow_sampling": True,
                    "allow_logprobs": True,
                    "allow_search_indices": False,
                    "allow_view": True,
                    "allow_fine_tuning": False,
                    "organization": "*",
                    "group": None,
                    "is_blocking": False
                }
            ]
        },
        "whisper-1": {
            "id": "whisper-1",
            "object": "model",
            "owned_by": "organization-owner",
            "permission": [
                {
                    "id": "perm-2",
                    "object": "model_permission",
                    "created": int(time.time()),
                    "allow_create_engine": False,
                    "allow_sampling": True,
                    "allow_logprobs": True,
                    "allow_search_indices": False,
                    "allow_view": True,
                    "allow_fine_tuning": False,
                    "organization": "*",
                    "group": None,
                    "is_blocking": False
                }
            ]
        }
    }

    if model_id not in models:
        raise HTTPException(status_code=404, detail="Model not found")

    return models[model_id]

@app.post("/v1/audio/speech")
async def text_to_speech(tts_request: TTSRequest, request: Request, token: str = Depends(verify_api_key)):
    logger.info(f"TTS è¯·æ±‚: {tts_request.model_dump()}")
    # æ¨¡æ‹Ÿç”Ÿæˆè¯­éŸ³æ–‡ä»¶
    voice = tts_request.voice
    prompt_audio, prompt_text = None, None
    if voice in DEFAULT_VOICE_DICT.keys():
        prompt_audio, prompt_text = DEFAULT_VOICE_DICT[voice]
        if not os.path.isfile(prompt_audio):
            prompt_audio, prompt_text = None, None
    target_text = tts_request.input
    if prompt_text == target_text:
        output_path = prompt_audio
    else:
        output_path = generate_audio(prompt_audio, prompt_text, target_text)
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(output_path):
        raise HTTPException(status_code=500, detail="éŸ³é¢‘æ–‡ä»¶ç”Ÿæˆå¤±è´¥")

    return FileResponse(output_path, media_type="audio/wav")

########################################################################################################################
# åŠ è½½è¯­éŸ³è¯†åˆ«ã€è¯­éŸ³è½¬å½•æ¨¡å‹

import os
import gradio as gr
import torch
from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline
from datasets import load_dataset
USERNAME = os.getenv("USERNAME")

# æ¨¡å‹æ¥æºï¼šhttps://hf-mirror.com/openai/whisper-large-v3-turbo
# æ•°æ®é›†æ¥æºï¼šhttps://hf-mirror.com/datasets/distil-whisper/librispeech_long/tree/main

# ä½¿ç”¨putenvå‡½æ•°è®¾ç½®æ–°çš„PATHç¯å¢ƒå˜é‡
# https://www.gyan.dev/ffmpeg/builds/packages/ffmpeg-7.0.2-full_build.7z
#os.environ['PATH'] = f'D:\\Users\\{USERNAME}\\ffmpeg-7.1\\bin\\;' + os.getenv('PATH')
#print(os.getenv('PATH'))  # è¾“å‡ºæ–°çš„PATHç¯å¢ƒå˜é‡

device = "cuda:0" if torch.cuda.is_available() else "cpu"
torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32

# model_id = "openai/whisper-large-v3-turbo"
model_id = rf"/root/whisper-large-v3-turbo"

model = AutoModelForSpeechSeq2Seq.from_pretrained(
    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True
)
model.to(device)

processor = AutoProcessor.from_pretrained(model_id, language="zh")

pipe = pipeline(
    "automatic-speech-recognition",
    model=model,
    tokenizer=processor.tokenizer,
    feature_extractor=processor.feature_extractor,
    torch_dtype=torch_dtype,
    device=device,
)


@app.post("/v1/audio/transcriptions")
async def transcribe_audio(
    file: UploadFile = File(...),
    model: str = Form("whisper-1"),
    language: Optional[str] = Form(None),
    prompt: Optional[str] = Form(None),
    response_format: Optional[str] = Form("json"),
    temperature: Optional[float] = Form(0.0),
    timestamp_granularities: Optional[List[str]] = Form(None),
    request: Request = None,
    token: str = Depends(verify_api_key)
):
    logger.info(f"Transcription è¯·æ±‚: model={model}, language={language}, response_format={response_format}")
    logger.info(f"æ–‡ä»¶å: {file.filename}, æ–‡ä»¶ç±»å‹: {file.content_type}")

    # è¯»å–ä¸Šä¼ çš„æ–‡ä»¶å†…å®¹
    contents = await file.read()
    file_path = f"/tmp/{file.filename}"
    with open(file_path, "wb") as f:
        f.write(contents)

    # è°ƒç”¨ Whisper æ¨¡å‹è¿›è¡Œè½¬å½•
    result = pipe(file_path)
    logger.info(f"{file.filename} è½¬å½•çš„ç»“æœï¼š{result}")
    # æ¨¡æ‹Ÿè½¬å½•é€»è¾‘
    # å®é™…åº”è¯»å–æ–‡ä»¶å†…å®¹å¹¶è°ƒç”¨ Whisper æ¨¡å‹è¿›è¡Œè½¬å½•
    # return {       "text": "Imagine the wildest idea that you've ever had, and you're curious about how it might scale to something that's a 100, a 1,000 times bigger. This is a place where you can get to do that."}
    return result


# æŒ‚è½½ Gradio åº”ç”¨åˆ° FastAPI
app = gr.mount_gradio_app(app, demo, path="", mcp_server=True)

# å¯åŠ¨æœåŠ¡
if __name__ == "__main__":
    logger.info("ğŸš€ æ­£åœ¨å¯åŠ¨ Gradio æœåŠ¡...")
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=7860)
    logger.info("âœ… Gradio æœåŠ¡å·²å¯åŠ¨ï¼Œè®¿é—® http://localhost:7860")


'''
curl http://localhost:8723/v1/audio/speech \
  -H "Authorization: Bearer tts-erwweofwehFWEOFJfweff" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "tts-1",
    "input": "å±±è¡Œï¼Œå”ï¼Œæœç‰§ï¼Œè¿œä¸Šå¯’å±±çŸ³å¾„æ–œï¼Œç™½äº‘ç”Ÿå¤„æœ‰äººå®¶ï¼Œåœè½¦åçˆ±æ«æ—æ™šï¼Œéœœå¶çº¢äºäºŒæœˆèŠ±",
    "voice": "å¥³å£°"
  }' \
  --output speech.wav

curl -X POST http://localhost:8723/v1/audio/transcriptions \
  -H "Authorization: Bearer whisper-dwhfweWEFWEWFWDWDDdfwf" \
  -H "Content-Type: multipart/form-data" \
  -F "file=@tmpo6j6gkx3.wav" \
  -F "model=whisper-1" \
  -F "language=zh" \
  -F "response_format=json"
  
'''

